{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33098,
     "status": "ok",
     "timestamp": 1566466020661,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "EIP3ykajpZPk",
    "outputId": "bedf1f00-e9b6-4ddd-ada8-5a47cc2a8707"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd /content/gdrive/My\\ Drive/masters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3676,
     "status": "ok",
     "timestamp": 1566466057313,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "kICfj6YOpffj",
    "outputId": "cf73f62a-a2bc-49e7-cc50-0a3becd8b92b"
   },
   "outputs": [],
   "source": [
    "%ls ./data/processed\n",
    "\n",
    "# T1H-X1700M-Y1760M/  T24H-X850M-Y880M/  T3H-X850M-Y880M/\n",
    "# T12H-X850M-Y880M/  T24H-X425M-Y440M/   T24H-X85M-Y110M/   T6H-X850M-Y880M/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNulAac9o8gl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "from time import strftime\n",
    "from copy import deepcopy\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from utils.data_processing import *\n",
    "from logger.logger import setup_logging\n",
    "from utils.configs import BaseConf\n",
    "from utils.utils import write_json, Timer\n",
    "from models.kangkang_fnn_models import KangFeedForwardNetwork\n",
    "from dataloaders.flat_loader import FlatDataLoaders\n",
    "from datasets.flat_dataset import FlatDataGroup\n",
    "from utils.metrics import PRCurvePlotter, ROCCurvePlotter, LossPlotter\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "from models.model_result import ModelResult, ModelMetrics\n",
    "from utils.mock_data import mock_fnn_data_classification\n",
    "from utils.plots import im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6860,
     "status": "ok",
     "timestamp": 1566466592307,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "-TBN_1gYo8go",
    "outputId": "4d4f5037-e665-46e2-df61-928a6a22cb09",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-27T10:07:11 | root | INFO | =====================================BEGIN=====================================\n",
      "2019-10-27T10:07:11 | root | INFO | Device: cpu\n"
     ]
    }
   ],
   "source": [
    "data_dim_str = \"T24H-X850M-Y880M\"  #\"T1H-X1700M-Y1760M\"  # needs to exist\n",
    "data_dim_str = data_dim_str + \"_2013-01-01_2015-01-01\"\n",
    "model_name = \"FNN-CRIME-MODEL\"  # needs to be created\n",
    "\n",
    "data_path = f\"./data/processed/{data_dim_str}/\"\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    raise Exception(f\"Directory ({data_path}) needs to exist.\")\n",
    "\n",
    "model_path = data_path + f\"models/{model_name}/\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# logging config is set globally thus we only need to call this in this file\n",
    "# imported function logs will follow the configuration\n",
    "setup_logging(save_dir=model_path, log_config='./logger/standard_logger_config.json', default_level=log.INFO)\n",
    "log.info(\"=====================================BEGIN=====================================\")\n",
    "\n",
    "timer = Timer()\n",
    "# manually set the config\n",
    "# manually set the config\n",
    "conf_dict = {\n",
    "    \"seed\": 3,\n",
    "    \"use_cuda\": False,\n",
    "    \n",
    "    \"use_crime_types\": False,\n",
    "    \n",
    "    # data group/data set related\n",
    "    \"val_ratio\": 0.1,  # ratio of the total dataset\n",
    "    \"tst_ratio\": 0.2,# ratio of the total dataset\n",
    "    \"seq_len\": 1,\n",
    "    \"flatten_grid\": True,  # if the shaper should be used to squeeze the data\n",
    "    \n",
    "    # shaper related \n",
    "    \"shaper_top_k\": -1,  # if less then 0, top_k will not be applied\n",
    "    \"shaper_threshold\": 0,\n",
    "\n",
    "    \n",
    "    # data loader related\n",
    "    \"sub_sample_train_set\": True,\n",
    "    \"sub_sample_validation_set\": True,\n",
    "    \"sub_sample_test_set\": False,\n",
    "    \n",
    "    # training parameters\n",
    "    \"resume\": False,\n",
    "    \"early_stopping\": False,\n",
    "    \"tolerance\": 1e-8,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-8,\n",
    "    \"max_epochs\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout\": 0.2,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 6,\n",
    "    \n",
    "    # attached global variables - bad practice -find alternative\n",
    "    \"device\": None,  # pytorch device object [CPU|GPU]\n",
    "    \"timer\": Timer(),\n",
    "    \"model_name\": model_name,\n",
    "    \"model_path\": model_path,\n",
    "    \"checkpoint\": \"best\",\n",
    "    \n",
    "    \"use_seq_loss\": True,\n",
    "}\n",
    "conf = BaseConf(conf_dict=conf_dict)\n",
    "\n",
    "info = deepcopy(conf.__dict__)\n",
    "info[\"start_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# DATA LOADER SETUP\n",
    "np.random.seed(conf.seed)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed( conf.seed)\n",
    "else:\n",
    "    torch.manual_seed(conf.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "log.info(f\"Device: {device}\")\n",
    "info[\"device\"] = device.type\n",
    "conf.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.getLogger().setLevel(\"WARNING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KangFeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, spc_size=37, tmp_size=15, env_size=512, dropout_p=0.5, model_arch=None):\n",
    "        super(KangFeedForwardNetwork, self).__init__()\n",
    "\n",
    "        # drop out is not saved on the model state_dict - remember to turn off in evaluation\n",
    "        self.dropout_p = dropout_p\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "        if model_arch:\n",
    "            scp_net_h0 = model_arch.get(\"scp_net_h0\")\n",
    "            scp_net_h1 = model_arch.get(\"scp_net_h1\")\n",
    "            tmp_net_h0 = model_arch.get(\"tmp_net_h0\")\n",
    "            tmp_net_h1 = model_arch.get(\"tmp_net_h1\")\n",
    "            env_net_h0 = model_arch.get(\"env_net_h0\")\n",
    "            env_net_h1 = model_arch.get(\"env_net_h1\")\n",
    "            \n",
    "            final_net_h0 = scp_net_h1 + tmp_net_h1 + env_net_h1\n",
    "            final_net_h1 = model_arch.get(\"final_net_h1\")\n",
    "        else:  # default architechture\n",
    "            scp_net_h0 = 256\n",
    "            scp_net_h1 = 128\n",
    "            tmp_net_h0 = 256\n",
    "            tmp_net_h1 = 128\n",
    "            env_net_h0 = 256\n",
    "            env_net_h1 = 128\n",
    "\n",
    "            final_net_h0 = scp_net_h1 + tmp_net_h1 + env_net_h1\n",
    "            final_net_h1 = 1024\n",
    "\n",
    "        self.spcNet = nn.Sequential(nn.Linear(spc_size, scp_net_h0),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(scp_net_h0, scp_net_h0),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(scp_net_h0, scp_net_h1),\n",
    "                                    nn.ReLU())\n",
    "        self.tmpNet = nn.Sequential(nn.Linear(tmp_size, tmp_net_h0),\n",
    "                                    nn.ReLU(), nn.Linear(tmp_net_h0, tmp_net_h0),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(tmp_net_h0, tmp_net_h1),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        self.envNet = nn.Sequential(nn.Linear(env_size, env_net_h0),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(env_net_h0, env_net_h0),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(env_net_h0, env_net_h1),\n",
    "                                    nn.ReLU())\n",
    "\n",
    "        self.finalNet = nn.Sequential(nn.Linear(final_net_h0, final_net_h1),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(final_net_h1, final_net_h1),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(final_net_h1, 2))\n",
    "\n",
    "    def forward(self, spc_vec, tmp_vec, env_vec):\n",
    "        if self.dropout_p > 0:\n",
    "            spc_vec = self.dropout(spc_vec)\n",
    "            tmp_vec = self.dropout(tmp_vec)\n",
    "            env_vec = self.dropout(env_vec)\n",
    "\n",
    "        mid_vec = torch.cat([self.spcNet(spc_vec), self.tmpNet(tmp_vec), self.envNet(env_vec)], dim=-1)\n",
    "        out_vec = self.finalNet(mid_vec)\n",
    "\n",
    "        return out_vec  \n",
    "    \n",
    "class SmallKangFNN(nn.Module):  \n",
    "    def __init__(self, spc_size=37, tmp_size=15, env_size=512, dropout_p=0.5, model_arch=None):\n",
    "        super(SmallKangFNN, self).__init__()\n",
    "\n",
    "        self.dropout_p = dropout_p\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "        if model_arch:\n",
    "            h_size0 = model_arch.get(\"h_size0\")\n",
    "            h_size1 = model_arch.get(\"h_size1\")\n",
    "            h_size2 = model_arch.get(\"h_size2\")\n",
    "        else:\n",
    "            h_size0 = 50\n",
    "            h_size1 = 50\n",
    "            h_size2 = 50\n",
    "\n",
    "\n",
    "        self.spcNet = nn.Sequential(nn.Linear(spc_size, h_size0),\n",
    "                                    nn.ReLU(), \n",
    "                                    nn.Linear(h_size0, h_size1), \n",
    "                                    nn.ReLU())\n",
    "        self.tmpNet = nn.Sequential(nn.Linear(tmp_size, h_size0), \n",
    "                                    nn.Linear(h_size0, h_size1), \n",
    "                                    nn.ReLU())\n",
    "        \n",
    "        self.envNet = nn.Sequential(nn.Linear(env_size, h_size0), \n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(h_size0, h_size1), \n",
    "                                    nn.ReLU())\n",
    "        \n",
    "        self.finalNet = nn.Sequential(nn.Linear(3*h_size1, h_size2), \n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Linear(h_size2, 2))\n",
    "\n",
    "    def forward(self, spc_vec, tmp_vec, env_vec):\n",
    "        if self.dropout_p > 0:\n",
    "            spc_vec = self.dropout(spc_vec)\n",
    "            tmp_vec = self.dropout(tmp_vec)\n",
    "            env_vec = self.dropout(env_vec)\n",
    "\n",
    "        mid_vec = torch.cat([self.spcNet(spc_vec), self.tmpNet(tmp_vec), self.envNet(env_vec)], dim=-1)\n",
    "        out_vec = self.finalNet(mid_vec)\n",
    "\n",
    "        return out_vec      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import Shaper\n",
    "\n",
    "class MockLoaders:\n",
    "    def __init__(self, train_loader=None, validation_loader=None, test_loader=None):\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        self.test_loader = test_loader\n",
    "    \n",
    "    \n",
    "class MockLoader:\n",
    "    def __init__(self, vector_size, batch_size, n_samples, class_split):\n",
    "        n_feats = np.sum(vector_size)\n",
    "        X, y = mock_fnn_data_classification(n_samples=n_samples, n_feats=n_feats, class_split=class_split)\n",
    "      \n",
    "        y = np.expand_dims(y, axis=1)\n",
    "        y = np.expand_dims(y, axis=0)\n",
    "        \n",
    "        #(N,C,L) in the dataset but (seq_len, batch_size, n_features) in loader\n",
    "        self.targets = np.copy(y).swapaxes(0,1) # format is now in (N,C,L)\n",
    "        self.t_range = pd.date_range(start='2012',end='2016', periods=n_samples)\n",
    "        self.shaper = None\n",
    "        X = np.expand_dims(X, axis=0)\n",
    "        \n",
    "        self.indices = np.array([(i,0,0) for i in np.arange(n_samples)]) # (N,C,L) format\n",
    "        \n",
    "        vectors = []\n",
    "        i = 0\n",
    "        for j in vector_size:\n",
    "            vectors.append(X[:,:,i:i+j])\n",
    "            i += j\n",
    "        vectors.append(y)    \n",
    "        \n",
    "        self.n_samples = n_samples\n",
    "        self.n_feats = n_feats\n",
    "        self.vectors = vectors\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = int(np.ceil(self.n_samples / self.batch_size))\n",
    "        self.current_batch = 0\n",
    "        \n",
    "        self.max_index = n_samples\n",
    "        self.min_index = 0\n",
    "        self.dataset = self # just to act as an interface\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_batch = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch >= self.num_batches:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.current_batch += 1\n",
    "            start_index = (self.current_batch - 1) * self.batch_size\n",
    "            stop_index = self.current_batch * self.batch_size\n",
    "            if stop_index > len(self):\n",
    "                stop_index = len(self)\n",
    "            return self[start_index:stop_index]\n",
    "        \n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        r = tuple(map(lambda x: x[:,index], self.vectors))\n",
    "        return (self.indices[index], *r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic training loop\n",
    "def train_model(model, optimiser, loaders, train_epoch_fn, loss_fn, conf):\n",
    "    \"\"\"\n",
    "    Generic training loop that handles:\n",
    "    - early stopping\n",
    "    - timing\n",
    "    - logging\n",
    "    - model checkpoints\n",
    "    - saving epoch and batch losses\n",
    "    \n",
    "    :returns: best validation loss of all the epochs - used to tune the hyper-parameters of the models/optimiser\n",
    "    \"\"\"\n",
    "    stopped_early = False\n",
    "    trn_batch_losses = []\n",
    "    val_batch_losses = []\n",
    "\n",
    "    trn_epoch_losses = []\n",
    "    val_epoch_losses = []\n",
    "    val_epoch_losses_best = float(\"inf\")\n",
    "    \n",
    "    if conf.resume:\n",
    "        try:\n",
    "            # load losses\n",
    "            losses_zip = np.load(f\"{conf.model_path}losses_{conf.checkpoint}.npz\")\n",
    "\n",
    "            val_batch_losses = losses_zip[\"val_batch_losses\"].tolist()\n",
    "            trn_batch_losses = losses_zip[\"trn_batch_losses\"].tolist()\n",
    "\n",
    "            trn_epoch_losses = losses_zip[\"trn_epoch_losses\"].tolist()\n",
    "            val_epoch_losses = losses_zip[\"val_epoch_losses\"].tolist() \n",
    "            val_epoch_losses_best = float(losses_zip[\"val_epoch_losses_best\"])  \n",
    "        except Exception as e:\n",
    "            log.error(f\"Nothing to resume from, training from scratch \\n\\t-> {e}\")\n",
    "\n",
    "    log.info(f\"Start Training {conf.model_name}\")\n",
    "    log.info(f\"Using optimiser: \\n{optimiser}\\n\\n\")\n",
    "    \n",
    "    for epoch in range(conf.max_epochs):\n",
    "        log.info(f\"Epoch: {(1+epoch):04d}/{conf.max_epochs:04d}\")\n",
    "        conf.timer.reset()\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        epoch_loss = train_epoch_fn(model=model, \n",
    "                                    optimiser=optimiser, \n",
    "                                    batch_loader=loaders.train_loader, \n",
    "                                    loss_fn=loss_fn,\n",
    "                                    total_losses=trn_batch_losses, \n",
    "                                    conf=conf)\n",
    "\n",
    "        trn_epoch_losses.append(epoch_loss)\n",
    "        log.debug(f\"Epoch {epoch} -> Training Loop Duration: {conf.timer.check()}\")\n",
    "        conf.timer.reset()\n",
    "\n",
    "        # Validation loop\n",
    "        tmp_val_epoch_losses = []\n",
    "        with torch.set_grad_enabled(False):\n",
    "            model.eval()\n",
    "            epoch_loss = train_epoch_fn(model=model, \n",
    "                                        optimiser=optimiser, \n",
    "                                        batch_loader=loaders.validation_loader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        total_losses=val_batch_losses, \n",
    "                                        conf=conf)\n",
    "\n",
    "            val_epoch_losses.append(epoch_loss)\n",
    "            log.debug(f\"Epoch {epoch} -> Validation Loop Duration: {conf.timer.check()}\")\n",
    "\n",
    "        log.info(f\"\\tLoss (Trn): \\t{trn_epoch_losses[-1]:.5f}\")\n",
    "        log.info(f\"\\tLoss (Val): \\t{val_epoch_losses[-1]:.5f}\")\n",
    "        log.info(f\"\\tLoss (Dif): \\t{np.abs(val_epoch_losses[-1]-trn_epoch_losses[-1]):.5f}\\n\")        \n",
    "\n",
    "        # save best model\n",
    "        if val_epoch_losses[-1] < val_epoch_losses_best:\n",
    "            val_epoch_losses_best = val_epoch_losses[-1]\n",
    "            torch.save(model.state_dict(),  f\"{conf.model_path}model_best.pth\")\n",
    "            torch.save(optimiser.state_dict(), f\"{conf.model_path}optimiser_best.pth\")\n",
    "            np.savez_compressed(file=f\"{conf.model_path}losses_best.npz\",\n",
    "                                val_batch_losses=val_batch_losses,\n",
    "                                val_epoch_losses=val_epoch_losses,\n",
    "                                trn_epoch_losses=trn_epoch_losses,\n",
    "                                trn_batch_losses=trn_batch_losses,\n",
    "                                val_epoch_losses_best=val_epoch_losses_best)\n",
    "\n",
    "        # increasing moving average of val_epoch_losses\n",
    "        if conf.early_stopping and epoch > 5 and np.sum(np.diff(val_epoch_losses[-5:])) > 0:\n",
    "            log.warning(\"Early stopping: Over-fitting has taken place\")\n",
    "            stopped_early = True\n",
    "            break\n",
    "\n",
    "        if conf.early_stopping and epoch > 5 and np.abs(val_epoch_losses[-1]-val_epoch_losses[-2]) < conf.tolerance:\n",
    "            log.warning(f\"Converged: Difference between the past two\" \n",
    "                        + f\" validation losses is within tolerance of {conf.tolerance}\")\n",
    "            stopped_early = True\n",
    "            break\n",
    "\n",
    "        # checkpoint - save models and loss values\n",
    "        torch.save(model.state_dict(), f\"{conf.model_path}model_latest.pth\")\n",
    "        torch.save(optimiser.state_dict(), f\"{conf.model_path}optimiser_latest.pth\")\n",
    "        np.savez_compressed(file=f\"{conf.model_path}losses_latest.npz\",\n",
    "                            val_batch_losses=val_batch_losses,\n",
    "                            val_epoch_losses=val_epoch_losses,\n",
    "                            trn_epoch_losses=trn_epoch_losses,\n",
    "                            trn_batch_losses=trn_batch_losses,\n",
    "                            val_epoch_losses_best=val_epoch_losses_best)\n",
    "    \n",
    "    # Save training and validation plots - add flag to actually save or display\n",
    "    skip = 0\n",
    "    loss_plotter = LossPlotter(title=f\"Cross Entropy Loss ({conf.model_name})\")\n",
    "    loss_plotter.plot_losses(trn_epoch_losses, trn_batch_losses[skip:], val_epoch_losses, val_batch_losses[skip:])\n",
    "    loss_plotter.savefig(f\"{conf.model_path}plot_train_val_epoch_losses.png\")    \n",
    "    \n",
    "    return val_epoch_losses_best, stopped_early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.getLogger().setLevel(\"WARNING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_loop - wrapper with checkpoint saving and logging\n",
    "# tmp is only per epoch. all is for all epoch\n",
    "# train_loop - use conf to get device?\n",
    "def train_epoch_for_fnn(model, optimiser, batch_loader, loss_fn, total_losses, conf):\n",
    "    \"\"\"\n",
    "    Training the model for a single epoch\n",
    "    \"\"\"\n",
    "    epoch_losses = []\n",
    "    num_batches = batch_loader.num_batches\n",
    "    for indices, spc_feats, tmp_feats, env_feats, targets in batch_loader:\n",
    "        current_batch = batch_loader.current_batch\n",
    "\n",
    "        # Transfer to PyTorch Tensor and GPU\n",
    "        spc_feats = torch.Tensor(spc_feats[0]).to(conf.device) # only taking [0] for fnn\n",
    "        tmp_feats = torch.Tensor(tmp_feats[0]).to(conf.device) # only taking [0] for fnn\n",
    "        env_feats = torch.Tensor(env_feats[0]).to(conf.device) # only taking [0] for fnn\n",
    "        targets = torch.LongTensor(targets[0,:,0]).to(conf.device) # only taking [0] for fnn\n",
    "            \n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "        loss = loss_fn(input=out, target=targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "        total_losses.append(epoch_losses[-1])\n",
    "\n",
    "        if model.training:  # not used in validation loops\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            log.debug(f\"Batch: {current_batch:04d}/{num_batches:04d} \\t Loss: {epoch_losses[-1]:.4f}\")\n",
    "    mean_epoch_loss = np.mean(epoch_losses)\n",
    "    return mean_epoch_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-27T10:07:20 | root | INFO | Data shapes of files in generated_data.npz\n",
      "2019-10-27T10:07:20 | root | INFO | \tcrime_feature_indices shape (10,)\n",
      "2019-10-27T10:07:20 | root | INFO | \tcrime_types_grids shape (730, 10, 47, 33)\n",
      "2019-10-27T10:07:20 | root | INFO | \tcrime_grids shape (730, 1, 47, 33)\n",
      "2019-10-27T10:07:20 | root | INFO | \ttract_count_grids shape (730, 1, 47, 33)\n",
      "2019-10-27T10:07:20 | root | INFO | \tdemog_grid shape (1, 37, 47, 33)\n",
      "2019-10-27T10:07:20 | root | INFO | \tstreet_grid shape (1, 512, 47, 33)\n",
      "2019-10-27T10:07:20 | root | INFO | \ttime_vectors shape (731, 52)\n",
      "2019-10-27T10:07:20 | root | INFO | \tweather_vectors shape (365, 11)\n",
      "2019-10-27T10:07:20 | root | INFO | \tx_range shape (33,)\n",
      "2019-10-27T10:07:20 | root | INFO | \ty_range shape (47,)\n",
      "2019-10-27T10:07:20 | root | INFO | \tt_range shape (731,)\n",
      "(64, 3)\n",
      "(1, 64, 37)\n",
      "(10, 64, 57)\n",
      "(1, 64, 512)\n",
      "(10, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# CRIME DATA\n",
    "conf.seq_len = 10\n",
    "data_group = FlatDataGroup(data_path=data_path, conf=conf)\n",
    "loaders = FlatDataLoaders(data_group=data_group, conf=conf)\n",
    "for indices, spc_feats, tmp_feats, env_feats, targets in loaders.train_loader:\n",
    "    for i in [indices, spc_feats, tmp_feats, env_feats, targets]:\n",
    "        print(np.shape(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SET THE HYPER PARAMETERS\n",
    "conf.early_stopping = False\n",
    "conf.max_epochs = 4\n",
    "conf.dropout = 0 #1e-3\n",
    "conf.weight_decay = 0\n",
    "conf.resume = False\n",
    "conf.checkpoint = \"best\" # [\"best\"|\"latest\"]\n",
    "conf.lr = 1e-2\n",
    "conf.shaper_top_k = -1\n",
    "\n",
    "MOCK = False\n",
    "\n",
    "if MOCK:\n",
    "    # MOCK THE DATA\n",
    "    vector_size= [5,5,5]#[37,65,512]\n",
    "    batch_size = 100    \n",
    "    class_split=0.5\n",
    "    train_loader = MockLoader(vector_size, conf.batch_size, n_samples=1000, class_split=class_split)\n",
    "    validation_loader = MockLoader(vector_size, conf.batch_size, n_samples=200, class_split=class_split)\n",
    "    test_loader = MockLoader(vector_size, conf.batch_size, n_samples=300, class_split=class_split)\n",
    "    loaders = MockLoaders(train_loader,validation_loader,test_loader)\n",
    "else:\n",
    "    # CRIME DATA\n",
    "    data_group = FlatDataGroup(data_path=data_path, conf=conf)\n",
    "    loaders = FlatDataLoaders(data_group=data_group, conf=conf)\n",
    "\n",
    "\n",
    "# SET LOSS FUNCTION\n",
    "# size averaged - so more epochs or larger lr for smaller batches\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "\n",
    "# SETUP MODEL\n",
    "train_set = loaders.train_loader.dataset\n",
    "indices, spc_feats, tmp_feats, env_feats, target = train_set[train_set.min_index]\n",
    "spc_size, tmp_size, env_size = spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]\n",
    "\n",
    "\n",
    "model_arch = {\n",
    "    \"h_size0\": 100,\n",
    "    \"h_size1\": 100,\n",
    "    \"h_size2\": 100,\n",
    "}\n",
    "model = SmallKangFNN(spc_size=spc_size,\n",
    "                     tmp_size=tmp_size,\n",
    "                     env_size=env_size,\n",
    "                     dropout_p=conf.dropout,\n",
    "                     model_arch=model_arch)\n",
    "\n",
    "# model_arch = {\n",
    "#     \"scp_net_h0\": 64,\n",
    "#     \"scp_net_h1\": 32,\n",
    "#     \"tmp_net_h0\": 64,\n",
    "#     \"tmp_net_h1\": 32,\n",
    "#     \"env_net_h0\": 64,\n",
    "#     \"env_net_h1\": 32,\n",
    "#     \"final_net_h1\": 64,\n",
    "# }\n",
    "# model = KangFeedForwardNetwork(spc_size=spc_size,\n",
    "#                                  tmp_size=tmp_size,\n",
    "#                                  env_size=env_size,\n",
    "#                                  dropout_p=conf.dropout,\n",
    "#                                  model_arch=model_arch)\n",
    "\n",
    "model.to(conf.device)\n",
    "\n",
    "# SETUP OPTIMISER\n",
    "parameters = model.parameters()\n",
    "\n",
    "# important note: using weight decay (l2 penalty) can prohibit long term memory in LSTM networks\n",
    "# - use gradient clipping instead\n",
    "optimiser = optim.Adam(params=parameters, lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "##### RESUME LOGIC\n",
    "if conf.resume:  # todo check if the files actually exist\n",
    "    try:\n",
    "        # resume from previous check point or resume from best validaton score checkpoint\n",
    "        # load model state\n",
    "        model_state_dict = torch.load(f\"{conf.model_path}model_{conf.checkpoint}.pth\",\n",
    "                                      map_location=conf.device.type)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        \n",
    "        # load optimiser state\n",
    "        optimiser_state_dict = torch.load(f\"{conf.model_path}optimiser_{conf.checkpoint}.pth\",\n",
    "                                          map_location=conf.device.type)\n",
    "        optimiser.load_state_dict(optimiser_state_dict) \n",
    "\n",
    "        # new optimiser hyper-parameters\n",
    "        optimiser.param_groups[0]['lr'] = conf.lr\n",
    "        optimiser.param_groups[0]['weight_decay'] = conf.weight_decay\n",
    "\n",
    "        # new model hyper-parameters\n",
    "        model.dropout.p = conf.dropout # note that drop out is not part of the saved state dict\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Nothing to resume from, training from scratch \\n\\t-> {e}\")\n",
    "\n",
    "best_val_loss, stopped_early = train_model(model=model,\n",
    "                                            optimiser=optimiser,\n",
    "                                            loaders=loaders,\n",
    "                                            train_epoch_fn=train_epoch_for_fnn,\n",
    "                                            loss_fn=loss_function,\n",
    "                                            conf=conf)    \n",
    "\n",
    "print(f\"best_val_loss: {best_val_loss}, stopped_early: {stopped_early}\") # use the current epoch instead\n",
    "# if stopped_early -> continue with best_model - new hyper-parameters -> no n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.getLogger().setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import best_threshold\n",
    "\n",
    "# todo make wrapper that takes eval loop which returns probas_pred\n",
    "def eval_fn_fnn(model, batch_loader, conf):\n",
    "\n",
    "    # NOTE: important when using zeros_like, specify dtype - else float values are truncated to 0\n",
    "    probas_pred = np.zeros_like(batch_loader.dataset.target_shape,dtype=np.float)\n",
    "    \n",
    "    # todo not target shapes anymore because of year offset\n",
    "    y_true = np.zeros_like(batch_loader.dataset.targets,dtype=np.float)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        model.eval()\n",
    "            \n",
    "        num_batches = batch_loader.num_batches\n",
    "        for indices, spc_feats, tmp_feats, env_feats, targets in batch_loader:\n",
    "            current_batch = batch_loader.current_batch\n",
    "\n",
    "            # Transfer to PyTorch Tensor and GPU\n",
    "            spc_feats = torch.Tensor(spc_feats[0]).to(conf.device) # only taking [0] for fnn\n",
    "            tmp_feats = torch.Tensor(tmp_feats[0]).to(conf.device) # only taking [0] for fnn\n",
    "            env_feats = torch.Tensor(env_feats[0]).to(conf.device) # only taking [0] for fnn\n",
    "            targets = torch.LongTensor(targets[0,:,0]).to(conf.device) # only taking [0] for fnn\n",
    "            out = model(spc_feats, tmp_feats, env_feats)\n",
    "\n",
    "            batch_probas_pred = F.softmax(out,dim=-1)[:,1].numpy() # select class1 prediction\n",
    "            \n",
    "            for i, p, t in zip(indices,batch_probas_pred, targets.numpy()):\n",
    "                n,c,l = i\n",
    "                y_true[n,c,l] = t   \n",
    "                probas_pred[n,c,l] = p\n",
    "\n",
    "    y_pred = np.copy(probas_pred)\n",
    "    thresh = best_threshold(y_true, probas_pred)\n",
    "    y_pred[y_pred >= thresh] = 1\n",
    "    y_pred[y_pred < thresh] = 0\n",
    "    \n",
    "    return y_true, y_pred, probas_pred\n",
    "\n",
    "def evaluate_model(model, batch_loader, eval_fn, shaper, conf):\n",
    "    \"\"\"\n",
    "    Training the model for a single epoch\n",
    "    \"\"\"\n",
    "    y_true, y_pred, probas_pred = eval_fn(model, batch_loader, conf)\n",
    " \n",
    "    # save result\n",
    "    # only saves the restult of the metrics not the predicted values\n",
    "    model_metrics = ModelMetrics(model_name=conf.model_name,\n",
    "                                 y_true=y_true,\n",
    "                                 y_pred=y_pred,\n",
    "                                 probas_pred=probas_pred)\n",
    "    log.info(model_metrics)\n",
    "    \n",
    "    # saves the actaul target and predicted values to be visualised later on\n",
    "    model_result = ModelResult(model_name=conf.model_name,\n",
    "                                y_true=y_true,\n",
    "                                y_pred=y_pred,\n",
    "                                probas_pred=probas_pred,\n",
    "                                t_range=batch_loader.dataset.t_range,\n",
    "                                shaper=shaper)\n",
    "    log.info(model_result)\n",
    "    \n",
    "    # do result plotting and saving \n",
    "    pr_plotter = PRCurvePlotter()\n",
    "    pr_plotter.add_curve(y_true.flatten(), probas_pred.flatten(), label_name=conf.model_name)\n",
    "    pr_plotter.savefig(model_path + \"plot_pr_curve.png\")\n",
    "\n",
    "    roc_plotter = ROCCurvePlotter()\n",
    "    roc_plotter.add_curve(y_true.flatten(), probas_pred.flatten(), label_name=conf.model_name)\n",
    "    roc_plotter.savefig(model_path + \"plot_roc_curve.png\")\n",
    "    return y_true, y_pred, probas_pred\n",
    "\n",
    "y_true, y_pred, probas_pred = evaluate_model(model=model,\n",
    "                       batch_loader=loaders.test_loader,\n",
    "                       eval_fn=eval_fn_fnn,\n",
    "                       shaper=None,#data_group.shaper,\n",
    "                       conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im(data_group.shaper.unsqueeze(probas_pred).mean(0)[0])\n",
    "im(data_group.shaper.unsqueeze(data_group.targets).mean(0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_score_per_cell(y_true, probas_pred):\n",
    "    \"\"\"\n",
    "    y_true: shape (N,1,L)\n",
    "    probas_pred: (N,1,L)\n",
    "    \"\"\"\n",
    "    N,_,L = y_true.shape\n",
    "    result = np.zeros(L)\n",
    "    \n",
    "    for i in range(L):\n",
    "        result[i] = average_precision_score(y_true=y_true[:,:,i].flatten()\n",
    "                                            , y_score=probas_pred[:,:,i].flatten())\n",
    "        \n",
    "    return result\n",
    "\n",
    "def roc_auc_score_per_cell(y_true, probas_pred):\n",
    "    \"\"\"\n",
    "    y_true: shape (N,1,L)\n",
    "    probas_pred: (N,1,L)\n",
    "    \"\"\"\n",
    "    N,_,L = y_true.shape\n",
    "    result = np.zeros(L)\n",
    "    \n",
    "    for i in range(L):\n",
    "        result[i] = average_precision_score(y_true=y_true[:,:,i].flatten(),\n",
    "                                            y_score=probas_pred[:,:,i].flatten())\n",
    "        \n",
    "    return result\n",
    "    \n",
    "    \n",
    "def average_precision_score_per_time_slot(y_true, probas_pred):\n",
    "    \"\"\"\n",
    "    y_true: shape (N,1,L)\n",
    "    probas_pred: (N,1,L)\n",
    "    \"\"\"\n",
    "    N,_,L = y_true.shape\n",
    "    result = np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        result[i] = average_precision_score(y_true=y_true[:,:,i].flatten(),\n",
    "                                            y_score=probas_pred[:,:,i].flatten())\n",
    "        \n",
    "    return result\n",
    "\n",
    "def roc_auc_score_per_time_slot(y_true, probas_pred):\n",
    "    \"\"\"\n",
    "    y_true: shape (N,1,L)\n",
    "    probas_pred: (N,1,L)\n",
    "    \"\"\"\n",
    "    N,_,L = y_true.shape\n",
    "    result = np.zeros(N)\n",
    "    \n",
    "    for i in range(N):\n",
    "        result[i] = average_precision_score(y_true=y_true[i,:,:].flatten(),\n",
    "                                            y_score=probas_pred[i,:,:].flatten())\n",
    "        \n",
    "    return result    \n",
    "    \n",
    "\n",
    "# metric_per_time_step - get the auc or ap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im(data_group.shaper.unsqueeze(probas_pred)[40][0])\n",
    "im(data_group.shaper.unsqueeze(loaders.test_loader.dataset.targets)[40][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders.test_loader.dataset.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hX2VCaMOo8g1"
   },
   "source": [
    "# Todo Evaluation prediction formating into an actual map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = loaders.data_group.testing_set.targets\n",
    "\n",
    "# todo get better name\n",
    "probas_grid = np.zeros_like(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### testing ###### torch outputs to see if we're actually getting valuable things out\n",
    "out = torch.Tensor([[-3,2],[2,-1],[0.5,0.4]])\n",
    "print(out.shape)\n",
    "print(out)\n",
    "out_soft = F.softmax(out, dim=-1)\n",
    "print(out_soft)\n",
    "out_label = torch.argmax(out_soft, dim=-1)\n",
    "print(out_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79852,
     "status": "ok",
     "timestamp": 1566466668518,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "SfZh9USWo8g2",
    "outputId": "13490f4e-83ab-4b20-cd91-7edc68ee9ad8"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path + \"model_best.pth\"))\n",
    "conf = BaseConf(conf_dict=conf_dict)\n",
    "conf.batch_size = 4\n",
    "loaders = FlatDataLoaders(data_path=data_path, conf=conf)\n",
    "\n",
    "# EVALUATE MODEL\n",
    "with torch.set_grad_enabled(False):\n",
    "    # Transfer to GPU\n",
    "    testing_losses = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    probas_pred = []\n",
    "    \n",
    "    \n",
    "    # loop through is set does not fit in batch\n",
    "    for indices, spc_feats, tmp_feats, env_feats, targets in loaders.test_loader: \n",
    "        \"\"\"\n",
    "        IMPORTNANT NOTE: WHEN DOING LSTM - ONLY FEED THE TEMPORAL VECTORS IN THE LSTM\n",
    "        FEED THE REST INTO THE NORMAL LINEAR NETWORKS\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Transfer to GPU\n",
    "        spc_feats = torch.Tensor(spc_feats).to(conf.device)\n",
    "        tmp_feats = torch.Tensor(tmp_feats).to(conf.device)\n",
    "        env_feats = torch.Tensor(env_feats).to(conf.device)\n",
    "        targets = torch.LongTensor(targets).to(conf.device)\n",
    "        \n",
    "        y_true.extend(targets.tolist())\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "                        \n",
    "        out = F.softmax(out, dim=-1)\n",
    "        \n",
    "        log.info(f\"out: {out}\")\n",
    "        log.info(f\"indices: {indices}\")\n",
    "        log.info(f\"indices: {targets}\")\n",
    "    \n",
    "        out_label = torch.argmax(out, dim=-1)\n",
    "        log.info(f\"out_label: {out_label}\")\n",
    "        y_pred.extend(out_label.tolist())\n",
    "        out_proba = out[:, 1]  # likelihood of crime is more general form - when comparing to moving averages\n",
    "        probas_pred.extend(out_proba.tolist())\n",
    "        \n",
    "        break  # !! remove\n",
    "\n",
    "\n",
    "# todo change to be the shape (N,L) of the original prediction.        \n",
    "model_result = ModelResult(model_name=\"FNN (Kang and Kang)\",\n",
    "                            y_true=y_true,\n",
    "                            y_pred=y_pred,\n",
    "                            probas_pred=probas_pred,\n",
    "                            t_range=loaders.test_loader.dataset.t_range,\n",
    "                            shaper=loaders.data_group.shaper)\n",
    "                        \n",
    "# log.info(model_result)\n",
    "\n",
    "np.savez_compressed(model_path + \"evaluation_results.npz\", model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DRCri5po8g3"
   },
   "outputs": [],
   "source": [
    "pr_plotter = PRCurvePlotter()\n",
    "pr_plotter.add_curve(y_true, probas_pred, label_name=\"FNN (Kang and Kang)\")\n",
    "pr_plotter.savefig(model_path + \"plot_pr_curve.png\")\n",
    "\n",
    "roc_plotter = ROCCurvePlotter()\n",
    "roc_plotter.add_curve(y_true, probas_pred, label_name=\"FNN (Kang and Kang)\")\n",
    "roc_plotter.savefig(model_path + \"plot_roc_curve.png\")\n",
    "\n",
    "info[\"stop_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "write_json(info, model_path + \"info.json\")\n",
    "\n",
    "log.info(\"=====================================END=====================================\")targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K17LWEnyo8g5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hpqNyfVo8g7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_kang_fnn_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
