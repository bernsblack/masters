{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33098,
     "status": "ok",
     "timestamp": 1566466020661,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "EIP3ykajpZPk",
    "outputId": "bedf1f00-e9b6-4ddd-ada8-5a47cc2a8707"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd /content/gdrive/My\\ Drive/masters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3676,
     "status": "ok",
     "timestamp": 1566466057313,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "kICfj6YOpffj",
    "outputId": "cf73f62a-a2bc-49e7-cc50-0a3becd8b92b"
   },
   "outputs": [],
   "source": [
    "%ls ./data/processed\n",
    "\n",
    "# T1H-X1700M-Y1760M/  T24H-X850M-Y880M/  T3H-X850M-Y880M/\n",
    "# T12H-X850M-Y880M/  T24H-X425M-Y440M/   T24H-X85M-Y110M/   T6H-X850M-Y880M/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNulAac9o8gl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "from time import strftime\n",
    "from copy import deepcopy\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from utils.data_processing import *\n",
    "from logger.logger import setup_logging\n",
    "from utils.configs import BaseConf\n",
    "from utils.utils import write_json, Timer\n",
    "from models.kangkang_fnn_models import KangFeedForwardNetwork\n",
    "from dataloaders.flat_loader import FlatDataLoaders\n",
    "from datasets.flat_dataset import FlatDataGroup\n",
    "from utils.metrics import PRCurvePlotter, ROCCurvePlotter, LossPlotter\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "from models.model_result import ModelResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6860,
     "status": "ok",
     "timestamp": 1566466592307,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "-TBN_1gYo8go",
    "outputId": "4d4f5037-e665-46e2-df61-928a6a22cb09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-17T16:33:44 | root | INFO | =====================================BEGIN=====================================\n",
      "2019-10-17T16:33:44 | root | INFO | Device: cpu\n",
      "2019-10-17T16:33:44 | root | INFO | Data shapes of files in generated_data.npz\n",
      "2019-10-17T16:33:44 | root | INFO | \tcrime_feature_indices shape (11,)\n",
      "2019-10-17T16:33:44 | root | INFO | \tcrime_types_grids shape (365, 11, 47, 33)\n",
      "2019-10-17T16:33:44 | root | INFO | \tcrime_grids shape (365, 1, 47, 33)\n",
      "2019-10-17T16:33:44 | root | INFO | \tdemog_grid shape (1, 37, 47, 33)\n",
      "2019-10-17T16:33:44 | root | INFO | \tstreet_grid shape (1, 512, 47, 33)\n",
      "2019-10-17T16:33:44 | root | INFO | \ttime_vectors shape (366, 52)\n",
      "2019-10-17T16:33:44 | root | INFO | \tweather_vectors shape (365, 11)\n",
      "2019-10-17T16:33:44 | root | INFO | \tx_range shape (33,)\n",
      "2019-10-17T16:33:44 | root | INFO | \ty_range shape (47,)\n",
      "2019-10-17T16:33:44 | root | INFO | \tt_range shape (366,)\n",
      "shape crimes_train ->  (256, 1, 763)\n",
      "shape crimes ->  (364, 1, 763)\n"
     ]
    }
   ],
   "source": [
    "data_dim_str = \"T24H-X850M-Y880M\"  #\"T1H-X1700M-Y1760M\"  # needs to exist\n",
    "model_name = \"FNN-CRIME-MODEL\"  # needs to be created\n",
    "\n",
    "data_path = f\"./data/processed/{data_dim_str}/\"\n",
    "model_path = data_path + f\"models/{model_name}/\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# logging config is set globally thus we only need to call this in this file\n",
    "# imported function logs will follow the configuration\n",
    "setup_logging(save_dir=model_path, log_config='./logger/standard_logger_config.json', default_level=log.INFO)\n",
    "log.info(\"=====================================BEGIN=====================================\")\n",
    "\n",
    "timer = Timer()\n",
    "# manually set the config\n",
    "conf_dict = {\n",
    "    \"seed\": 3,\n",
    "    \"use_cuda\": False,\n",
    "    \n",
    "    \"use_crime_types\": False,\n",
    "    \n",
    "    # data related hyper-params\n",
    "    \"val_ratio\": 0.1,  # ratio of the total dataset\n",
    "    \"tst_ratio\": 0.2,# ratio of the total dataset\n",
    "    \"sub_sample_train_set\": True,\n",
    "    \"sub_sample_validation_set\": True,\n",
    "    \"sub_sample_test_set\": False,\n",
    "    \"flatten_grid\": True,  # if the shaper should be used to squeeze the data\n",
    "    \"shaper_top_k\": -1,  # if less then 0, top_k will not be applied\n",
    "    \"shaper_threshold\": 0,\n",
    "    \"seq_len\": 0,\n",
    "    \n",
    "    # training parameters\n",
    "    \"resume\": False,\n",
    "    \"early_stopping\": False,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-8,\n",
    "    \"max_epochs\": 1,\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout\": 0,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 6,\n",
    "}\n",
    "conf = BaseConf(conf_dict=conf_dict)\n",
    "\n",
    "info = deepcopy(conf.__dict__)\n",
    "info[\"start_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# DATA LOADER SETUP\n",
    "np.random.seed(conf.seed)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(conf.seed)\n",
    "else:\n",
    "    torch.manual_seed(conf.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "log.info(f\"Device: {device}\")\n",
    "info[\"device\"] = device.type\n",
    "\n",
    "# GET DATA\n",
    "loaders = FlatDataLoaders(data_path=data_path, conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6088,
     "status": "ok",
     "timestamp": 1566466592308,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "kkUGtvEpo8gq",
    "outputId": "037c007d-06c2-4733-cd02-22ed2491e867"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b640a8d512d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SET MODEL PARAMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mspc_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mspc_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspc_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/masters/datasets/flat_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mtmp_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweather_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrime_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# todo add more historical values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# todo teacher forcing - if we are using this then we need to return sequence of targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mtarget_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mt_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mstack_spc_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemog_vec\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# todo stacking the same grid might cause memory issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# SET MODEL PARAMS\n",
    "train_set = loaders.training_generator.dataset\n",
    "spc_feats, tmp_feats, env_feats, target = train_set[train_set.min_index]\n",
    "spc_size, tmp_size, env_size = spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]\n",
    "\n",
    "model = KangFeedForwardNetwork(spc_size=spc_size, tmp_size=tmp_size, env_size=env_size, dropout_p=conf.dropout)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "index -1 is out of bounds for array with size 195328",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-94cb5616a11a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_index\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Google Drive/masters/datasets/flat_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mt_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_index\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mt_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: index -1 is out of bounds for array with size 195328"
     ]
    }
   ],
   "source": [
    "train_set[train_set.min_index-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KoUBbjK2o8gt"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "trn_loss = []\n",
    "val_loss = []\n",
    "val_loss_best = float(\"inf\")\n",
    "\n",
    "all_trn_loss = []\n",
    "all_val_loss = []\n",
    "\n",
    "optimiser = optim.Adam(params=model.parameters(), lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "if conf.resume:\n",
    "    # load model and optimiser states\n",
    "    model_state_dict = torch.load(model_path + \"model_best.pth\", map_location=device.type)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    optimiser_state_dict = torch.load(model_path + \"optimiser_best.pth\", map_location=device.type)\n",
    "    optimiser.load_state_dict(optimiser_state_dict)\n",
    "    # load losses\n",
    "    losses_zip = np.load(model_path + \"losses.npz\")\n",
    "    all_val_loss = losses_zip[\"all_val_loss\"].tolist()\n",
    "    val_loss = losses_zip[\"val_loss\"].tolist()\n",
    "    trn_loss = losses_zip[\"trn_loss\"].tolist()\n",
    "    all_trn_loss = losses_zip[\"all_trn_loss\"].tolist()\n",
    "    val_loss_best = float(losses_zip[\"val_loss_best\"])\n",
    "    # todo only load loss since last best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5070,
     "status": "ok",
     "timestamp": 1566466592309,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "eCAWFDs_o8gu",
    "outputId": "782a432d-a4df-42f3-e689-e1e16d131525"
   },
   "outputs": [],
   "source": [
    "spc_feats, tmp_feats, env_feats, targets = next(iter(loaders.validation_generator))\n",
    "for i in [spc_feats, tmp_feats, env_feats, targets]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = loaders.validation_generator.dataset\n",
    "\n",
    "spc_feats, tmp_feats, env_feats, targets = d[d.min_index:d.min_index+10]\n",
    "for i in [spc_feats, tmp_feats, env_feats, targets]:\n",
    "    print(i.shape,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (j,(spc_feats, tmp_feats, env_feats, targets)) in enumerate(loaders.validation_generator):\n",
    "    for i in [spc_feats, tmp_feats, env_feats, targets]:\n",
    "        print(i.shape,end='')\n",
    "    print(j)  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 65268,
     "status": "ok",
     "timestamp": 1566466652939,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "P-RE42Sbo8gw",
    "outputId": "ff6727c6-310f-4eb4-a715-00c332d39286",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(conf.max_epochs):\n",
    "    log.info(f\"Epoch: {(1+epoch):04d}/{conf.max_epochs:04d}\")\n",
    "    timer.reset()\n",
    "    # Training loop\n",
    "    tmp_trn_loss = []\n",
    "    num_batches = loaders.training_generator.num_batches\n",
    "    for spc_feats, tmp_feats, env_feats, targets in loaders.training_generator:\n",
    "        current_batch = loaders.training_generator.current_batch\n",
    "        \n",
    "        # Transfer to PyTorch Tensor and GPU\n",
    "        spc_feats = torch.Tensor(spc_feats[0]).to(device) # only taking [0] for fnn\n",
    "        tmp_feats = torch.Tensor(tmp_feats[0]).to(device) # only taking [0] for fnn\n",
    "        env_feats = torch.Tensor(env_feats[0]).to(device) # only taking [0] for fnn\n",
    "        targets = torch.LongTensor(targets[0,:,0]).to(device) # only taking [0] for fnn\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "        loss = loss_function(input=out, target=targets)\n",
    "        tmp_trn_loss.append(loss.item())\n",
    "        all_trn_loss.append(tmp_trn_loss[-1])\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        log.debug(f\"Batch: {current_batch:04d}/{num_batches:04d} \\t Loss: {tmp_trn_loss[-1]:.4f}\")\n",
    "    trn_loss.append(np.mean(tmp_trn_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Training Loop Duration: {timer.check()}\")\n",
    "    timer.reset()\n",
    "\n",
    "    # Validation loop\n",
    "    tmp_val_loss = []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # Transfer to GPU\n",
    "        for spc_feats, tmp_feats, env_feats, targets in loaders.validation_generator:\n",
    "            # Transfer to GPU\n",
    "            spc_feats = torch.Tensor(spc_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            tmp_feats = torch.Tensor(tmp_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            env_feats = torch.Tensor(env_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            targets = torch.LongTensor(targets[0,:,0]).to(device)  # only taking [0] for fnn\n",
    "            out = model(spc_feats, tmp_feats, env_feats)\n",
    "\n",
    "            loss = loss_function(input=out, target=targets)\n",
    "            tmp_val_loss.append(loss.item())\n",
    "            all_val_loss.append(tmp_val_loss[-1])\n",
    "    val_loss.append(np.mean(tmp_val_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Validation Loop Duration: {timer.check()}\")\n",
    "\n",
    "    log.info(f\"\\tLoss (Trn): \\t{trn_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Val): \\t{val_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Dif): \\t{np.abs(val_loss[-1]-trn_loss[-1]):.5f}\\n\")        \n",
    "    \n",
    "\n",
    "\n",
    "    # save best model\n",
    "    if min(val_loss) < val_loss_best:\n",
    "        val_loss_best = min(val_loss)\n",
    "        torch.save(model.state_dict(), model_path + \"model_best.pth\")\n",
    "        torch.save(optimiser.state_dict(), model_path + \"optimiser_best.pth\")\n",
    "\n",
    "    # model has been over-fitting stop maybe? # average of val_loss has increase - starting to over-fit\n",
    "    if conf.early_stopping and epoch != 0 and val_loss[-1] > val_loss[-2]:\n",
    "        log.warning(\"Over-fitting has taken place - stopping early\")\n",
    "        break\n",
    "\n",
    "    # checkpoint - save models and loss values\n",
    "    torch.save(model.state_dict(), model_path + \"model.pth\")\n",
    "    torch.save(optimiser.state_dict(), model_path + \"optimiser.pth\")\n",
    "    np.savez_compressed(model_path + \"losses.npz\",\n",
    "                        all_val_loss=all_val_loss,\n",
    "                        val_loss=val_loss,\n",
    "                        trn_loss=trn_loss,\n",
    "                        all_trn_loss=all_trn_loss,\n",
    "                        val_loss_best=val_loss_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPy6J_Ppo8gy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67377,
     "status": "ok",
     "timestamp": 1566466655562,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "kIX5Ub0Yo8gz",
    "outputId": "694fefec-6f45-4d85-92df-3cd2be1edf87",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save training and validation plots\n",
    "skip = 0\n",
    "loss_plotter = LossPlotter(title=\"Cross Entropy Loss of Linear Regression Model\")\n",
    "loss_plotter.plot_losses(trn_loss, all_trn_loss[skip:], val_loss, all_val_loss[skip:])\n",
    "loss_plotter.savefig(model_path + \"plot_train_val_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hX2VCaMOo8g1"
   },
   "source": [
    "# todo evaluation prediction formating into an actual map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 79852,
     "status": "ok",
     "timestamp": 1566466668518,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDulTxFaVYZ27_n_Ds4qquFn-4Ri8HUURRyOqf4WUo=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "SfZh9USWo8g2",
    "outputId": "13490f4e-83ab-4b20-cd91-7edc68ee9ad8"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path + \"model_best.pth\"))\n",
    "conf = BaseConf(conf_dict=conf_dict)\n",
    "loaders = FlatDataLoaders(data_path=data_path, conf=conf)\n",
    "\n",
    "# EVALUATE MODEL\n",
    "with torch.set_grad_enabled(False):\n",
    "    # Transfer to GPU\n",
    "    testing_losses = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    probas_pred = []\n",
    "    \n",
    "    # loop through is set does not fit in batch\n",
    "    for spc_feats, tmp_feats, env_feats, targets in loaders.testing_generator: \n",
    "        \"\"\"\n",
    "        IMPORTNANT NOTE: WHEN DOING LSTM - ONLY FEED THE TEMPORAL VECTORS IN THE LSTM\n",
    "        FEED THE REST INTO THE NORMAL LINEAR NETWORKS\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Transfer to GPU\n",
    "        spc_feats = torch.Tensor(spc_feats).to(device)\n",
    "        tmp_feats = torch.Tensor(tmp_feats).to(device)\n",
    "        env_feats = torch.Tensor(env_feats).to(device)\n",
    "        targets = torch.LongTensor(targets).to(device)\n",
    "        \n",
    "        y_true.extend(targets.tolist())\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        out_label = torch.argmax(out, dim=1)\n",
    "        y_pred.extend(out_label.tolist())\n",
    "        out_proba = out[:, 1]  # likelihood of crime is more general form - when comparing to moving averages\n",
    "        probas_pred.extend(out_proba.tolist())\n",
    "\n",
    "        \n",
    "model_result = ModelResult(model_name=\"FNN (Kang and Kang)\",\n",
    "                            y_true=y_true,\n",
    "                            y_pred=y_pred,\n",
    "                            probas_pred=probas_pred,\n",
    "                            t_range=loaders.testing_generator.dataset.t_range,\n",
    "                            shape=None)  # todo change to be the shape (N,L) of the original prediction.\n",
    "                        \n",
    "log.info(model_result)\n",
    "\n",
    "np.savez_compressed(model_path + \"evaluation_results.npz\", model_result)\n",
    "\n",
    "pr_plotter = PRCurvePlotter()\n",
    "pr_plotter.add_curve(y_true, probas_pred, label_name=\"FNN (Kang and Kang)\")\n",
    "pr_plotter.savefig(model_path + \"plot_pr_curve.png\")\n",
    "\n",
    "roc_plotter = ROCCurvePlotter()\n",
    "roc_plotter.add_curve(y_true, probas_pred, label_name=\"FNN (Kang and Kang)\")\n",
    "roc_plotter.savefig(model_path + \"plot_roc_curve.png\")\n",
    "\n",
    "info[\"stop_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "write_json(info, model_path + \"info.json\")\n",
    "\n",
    "log.info(\"=====================================END=====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DRCri5po8g3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K17LWEnyo8g5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hpqNyfVo8g7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_kang_fnn_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
