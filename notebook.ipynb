{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All we need is to ensure that our model evaluation output of all loaders are the same so that we could easily compare the models. Currenlty we do not have any proper models, this is quite the problem you see. \n",
    "\n",
    "### Should we iterate over the indexes - yes. We can compare the RNN and FNN - with the FNN. So many different things we wanted to plug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import logging as log\n",
    "from utils.preprocessing import Shaper, MinMaxScaler, minmax_scale\n",
    "from utils.configs import BaseConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "# from datasets.flat_dataset import FlatDataset\n",
    "from utils.preprocessing import Shaper\n",
    "from utils.mock_data import mock_data\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tract2points.pkl',\n",
      " 'crimes.pkl',\n",
      " 'point2feats_res18.pkl',\n",
      " 'openweatherdata.json',\n",
      " 'weather_minmax_normed.pkl',\n",
      " 'tract2km2.pkl',\n",
      " 'point2tract.pkl',\n",
      " 'point2spatial_info.pkl',\n",
      " 'census.pkl',\n",
      " 'resnet18_state_dict.pkl',\n",
      " 'valid_tracts.npy',\n",
      " 'crimes_2012_to_2018.pkl',\n",
      " 'census_minmax_normed.pkl',\n",
      " 'valid_points.npy',\n",
      " 'street_view_coords.npy',\n",
      " 'tract_boundaries.pkl',\n",
      " 'invalid_tracts.npy']\n"
     ]
    }
   ],
   "source": [
    "pprint(os.listdir('./data/raw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes = pd.read_pickle('./data/raw/crimes_2012_to_2018.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Primary Type</th>\n",
       "      <th>Arrest</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>tract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145308</th>\n",
       "      <td>11552731</td>\n",
       "      <td>2018-12-31 23:55:00</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>False</td>\n",
       "      <td>41.751914</td>\n",
       "      <td>-87.647717</td>\n",
       "      <td>7108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145307</th>\n",
       "      <td>11552724</td>\n",
       "      <td>2018-12-31 23:56:00</td>\n",
       "      <td>BATTERY</td>\n",
       "      <td>True</td>\n",
       "      <td>41.857068</td>\n",
       "      <td>-87.657625</td>\n",
       "      <td>8429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145306</th>\n",
       "      <td>11552699</td>\n",
       "      <td>2018-12-31 23:57:00</td>\n",
       "      <td>CRIMINAL DAMAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>41.740521</td>\n",
       "      <td>-87.647391</td>\n",
       "      <td>7110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145305</th>\n",
       "      <td>11561837</td>\n",
       "      <td>2018-12-31 23:59:00</td>\n",
       "      <td>DECEPTIVE PRACTICE</td>\n",
       "      <td>False</td>\n",
       "      <td>41.763181</td>\n",
       "      <td>-87.657709</td>\n",
       "      <td>6719.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145304</th>\n",
       "      <td>11556487</td>\n",
       "      <td>2018-12-31 23:59:00</td>\n",
       "      <td>CRIMINAL DAMAGE</td>\n",
       "      <td>False</td>\n",
       "      <td>41.689079</td>\n",
       "      <td>-87.696064</td>\n",
       "      <td>7401.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                Date        Primary Type  Arrest   Latitude  \\\n",
       "145308  11552731 2018-12-31 23:55:00             BATTERY   False  41.751914   \n",
       "145307  11552724 2018-12-31 23:56:00             BATTERY    True  41.857068   \n",
       "145306  11552699 2018-12-31 23:57:00     CRIMINAL DAMAGE   False  41.740521   \n",
       "145305  11561837 2018-12-31 23:59:00  DECEPTIVE PRACTICE   False  41.763181   \n",
       "145304  11556487 2018-12-31 23:59:00     CRIMINAL DAMAGE   False  41.689079   \n",
       "\n",
       "        Longitude   tract  \n",
       "145308 -87.647717  7108.0  \n",
       "145307 -87.657625  8429.0  \n",
       "145306 -87.647391  7110.0  \n",
       "145305 -87.657709  6719.0  \n",
       "145304 -87.696064  7401.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crimes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "def size(a):\n",
    "    print(f\"{getsizeof(a)/1000} kB\")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64800.144 kB\n",
      "0.072 kB\n",
      "0.072 kB\n"
     ]
    }
   ],
   "source": [
    "npdata = np.random.binomial(1,0.1,(300,30,30,30))\n",
    "# npdata = np.random.randn(300,30,30,30)\n",
    "dense_data = torch.Tensor(npdata)\n",
    "spare_data = dense_data.to_sparse()\n",
    "size(npdata)\n",
    "size(dense_data)\n",
    "size(spare_data)\n",
    "\n",
    "\n",
    "np.savez_compressed(\"000_numpy_tensor.npz\", npdata)\n",
    "# torch.save(spare_data, \"000_spare_data.pth\")\n",
    "torch.save(dense_data, \"000_dense_data.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo `AUC` over time and if re-training is needed\n",
    "- add noise to tragets\n",
    "- get auc for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim_str = \"T24H-X850M-Y880M\"  #\"T1H-X1700M-Y1760M\"  # needs to exist\n",
    "data_dim_str = data_dim_str + \"_2013-01-01_2015-01-01\"\n",
    "model_name = \"RNN-CRIME-MODEL\"  # needs to be created\n",
    "\n",
    "data_path = f\"./data/processed/{data_dim_str}/\"\n",
    "model_path = f\"{data_path}models/{model_name}/\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'24H'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pprint import pformat\n",
    "with np.load(f\"{data_path}generated_data.npz\") as zip_file:\n",
    "    log.info(\"Data shapes of files in generated_data.npz\")\n",
    "    for k, v in zip_file.items():\n",
    "        try:\n",
    "            log.info(f\"\\t{k} shape {np.shape(v)}\")\n",
    "        except Exception as e:\n",
    "            log.info(f\"\\t{k} \")\n",
    "            \n",
    "      \n",
    "        \n",
    "t_range = pd.read_pickle(data_path + \"t_range.pkl\")\n",
    "log.info(f\"\\tt_range shape {np.shape(t_range)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from utils.plots import im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1, 45)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "data = np.random.binomial(1,0.1,(5,10,10))\n",
    "data = np.expand_dims(data,axis=1)\n",
    "\n",
    "\n",
    "shaper = Shaper(data)\n",
    "s = shaper.squeeze(data)\n",
    "\n",
    "with open(\"shaper.pkl\", \"wb\") as shaper_file:\n",
    "    pickle.dump(shaper, shaper_file)   \n",
    "\n",
    "\n",
    "with open(\"shaper.pkl\", \"rb\") as shaper_file:\n",
    "    new_shaper = pickle.load(shaper_file)  \n",
    "    \n",
    "print(s.shape)    \n",
    "unsqueezed_data = new_shaper.unsqueeze(s)\n",
    "\n",
    "print(np.equal(unsqueezed_data, data).all())\n",
    "print(id(unsqueezed_data) == id(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequenced RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_dim_strs = [\n",
    "#     \"T12H-X850M-Y880M/\",\n",
    "#     \"T1H-X1700M-Y1760M/\",\n",
    "#     \"T24H-X425M-Y440M/\",\n",
    "#     \"T24H-X850M-Y880M/\",\n",
    "#     \"T24H-X85M-Y110M/\",\n",
    "#     \"T3H-X850M-Y880M/\",\n",
    "#     \"T4H-X850M-Y880M/\",\n",
    "#     \"T6H-X850M-Y880M/\"\n",
    "# ]\n",
    "\n",
    "data_dim_strs = os.listdir(\"./data/processed\")[1:]\n",
    "pprint(data_dim_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data_dim_str in data_dim_strs:\n",
    "    print(data_dim_str)\n",
    "    data_path = f\"./data/processed/{data_dim_str}/\"\n",
    "    zip_file = np.load(data_path + \"generated_data.npz\")\n",
    "\n",
    "    # print info on the read data\n",
    "    log.info(\"Data shapes of files in generated_data.npz\")\n",
    "    for k in zip_file.keys():\n",
    "        try:\n",
    "            v = zip_file[k]\n",
    "            print(f\"\\t{k} shape {np.shape(v)}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"Issue loading {k} ->{e}\")\n",
    "            \n",
    "        \n",
    "    t_range = pd.read_pickle(data_path + \"t_range.pkl\")\n",
    "    print(f\"\\tt_range shape {np.shape(t_range)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in zip_file.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim_str = \"T24H-X850M-Y880M\"\n",
    "data_path = f\"./data/processed/{data_dim_str}/\"\n",
    "\n",
    "\n",
    "conf_dict = {\n",
    "    \"seed\": 3,\n",
    "    \"resume\": False,\n",
    "    \"early_stopping\": False,\n",
    "    \"use_cuda\": False,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"tst_ratio\": 0.2,\n",
    "    \"sub_sample_train_set\": True,\n",
    "    \"sub_sample_validation_set\": True,\n",
    "    \"sub_sample_test_set\": False,\n",
    "    \"flatten_grid\": True,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-8,\n",
    "    \"max_epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout\": 0,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 6,\n",
    "    \"seq_len\": 0,\n",
    "    \"top_k_cells\": 20,\n",
    "    \"use_top_k_cells\": False,\n",
    "}\n",
    "\n",
    "\n",
    "conf = BaseConf(conf_dict=conf_dict)\n",
    "\n",
    "data_group = FlatDataGroup(data_path=data_path, conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = data_group.testing_set\n",
    "# get evaluation of each time step - like evaluate sub-test sets per time and see if the actual accuracy dies off.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.targets.shape,test_set.crimes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try simple crime info only training loop\n",
    "### Check other training loops - already have good examples with checkpointing set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful plotting - when it comes to the per cell level predictions\n",
    "## TODO: add cell index sorting to the dataset/datagroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_crimes = data_group.crime_scaler.inverse_transform(data_group.crimes)\n",
    "og_crimes = np.exp2(og_crimes) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "last_n_steps = 150\n",
    "crimes = og_crimes[:,0]\n",
    "s = data_group.shaper.squeeze(crimes)\n",
    "indices = np.argsort(s.mean(0))\n",
    "sorted_s = s[:last_n_steps,indices[-top_k:]]\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(sorted_s,cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "im(crimes.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    data = sorted_s[:,-(i+1)]\n",
    "    ones = np.copy(data)\n",
    "    ones[ones == -1]\n",
    "    mean = np.ones_like(data)*np.mean(data)\n",
    "    cell_plotter = CellPlotter(\"test\")\n",
    "    cell_plotter.plot_predictions(y_true=[1,0,0,1,0,0,1,0,0,1],\n",
    "                                  y_pred=None,[0,1,0,0,1,0,1,0,0,1],\n",
    "                                  probas_pred=[.1,.01,.06,.6,0,.3,.8,0,0,.1])\n",
    "    cell_plotter.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
