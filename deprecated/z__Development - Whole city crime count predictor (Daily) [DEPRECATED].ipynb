{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whole City Count Prediction (Daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plots import plot, displot\n",
    "from seaborn import distplot\n",
    "import matplotlib.pyplot as plt\n",
    "from models.baseline_models import historic_average\n",
    "\n",
    "import os\n",
    "import logging as log\n",
    "from time import strftime\n",
    "from copy import deepcopy\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from utils.data_processing import *\n",
    "from logger.logger import setup_logging\n",
    "from utils.configs import BaseConf\n",
    "from utils.utils import write_json, Timer\n",
    "from models.kangkang_fnn_models import KangFeedForwardNetwork, SimpleKangFNN, evaluate_fnn\n",
    "from dataloaders.flat_loader import FlatDataLoaders, MockLoader, MockLoaders\n",
    "from datasets.flat_dataset import FlatDataGroup\n",
    "from utils.metrics import PRCurvePlotter, ROCCurvePlotter, LossPlotter, PerTimeStepPlotter\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "from models.model_result import ModelResult, ModelMetrics, save_results, save_metrics, \\\n",
    "    compare_all_models, get_models_metrics\n",
    "from utils.mock_data import mock_fnn_data_classification\n",
    "from utils.plots import im\n",
    "from utils.utils import pshape, get_data_sub_paths, by_ref\n",
    "from trainers.generic_trainer import train_model\n",
    "\n",
    "from utils.metrics import best_threshold, get_y_pred, get_y_pred_by_thresholds, best_thresholds\n",
    "from time import time\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from pprint import pprint\n",
    "import logging\n",
    "\n",
    "from matplotlib import rcParams\n",
    "from utils.forecasting import compare_time_series_metrics\n",
    "\n",
    "from utils.utils import load_total_counts, to_title, set_system_seed\n",
    "from utils.data_processing import normalize_df_mean_std, normalize_df_min_max\n",
    "from utils.plots import plot, plot_df, plot_time_signals, plot_autocorr, subplots_df\n",
    "\n",
    "from utils.forecasting import forecast_metrics\n",
    "\n",
    "from models.sequence_models import train_epoch_for_sequence_model, evaluate_sequence_model\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import pingouin as pg # used to allow partial correlation of dataframes\n",
    "\n",
    "from trainers.generic_trainer import train_model_final\n",
    "\n",
    "from utils.testing import assert_no_nan, assert_valid_datetime_index \n",
    "\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = str(os.cpu_count())\n",
    "rcParams[\"font.family\"] = \"STIXGeneral\"\n",
    "\n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "# [\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\", \"none\"]\n",
    "pio.templates.default = \"plotly_white\" # \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.sequence_dataset import SequenceDataLoaders\n",
    "from models.rnn_models import GRUFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub_paths = [i for i in get_data_sub_paths() if i.startswith('Totals')]\n",
    "data_sub_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_sequences = False # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub_path = data_sub_paths[0]\n",
    "save_folder = f\"./data/processed/{data_sub_path}/plots/\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "FREQ = data_sub_path.lstrip('Totals_T').split('_')[0]\n",
    "time_steps_per_day = 24/int(FREQ[:-1])\n",
    "\n",
    "freq_title = {\n",
    "    \"24H\":\"Daily\",\n",
    "    \"1H\": \"Hourly\",\n",
    "    \"168H\":\"Weekly\",\n",
    "}.get(FREQ, \"Hourly\")\n",
    "print(f\"Using: {freq_title} ({FREQ})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_total_counts(folder_name=data_sub_path)\n",
    "\n",
    "if FREQ == \"168H\": \n",
    "    # data is split into weeks starting on monays - we need to trim first\n",
    "    # and last weeks because they are technically half weeks because they don't start on mondays\n",
    "    df = df.iloc[1:-1] \n",
    "\n",
    "display(df)\n",
    "# plot_df(df).show()\n",
    "subplots_df(df,title='Crime Counts Over Time',xlabel='Date',ylabel='Count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.memory_summary())\n",
    "else:\n",
    "    raise Exception(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set\n",
    "conf = BaseConf()\n",
    "conf.seed = int(time()) # 3\n",
    "set_system_seed(conf.seed)\n",
    "conf.model_name = f\"{freq_title} City Count\" # \"SimpleKangFNN\" # \"KangFNN\"  # needs to be created\n",
    "conf.data_path = f\"./data/processed/{data_sub_path}/\"\n",
    "\n",
    "if not os.path.exists(conf.data_path):\n",
    "    raise Exception(f\"Directory ({conf.data_path}) needs to exist.\")\n",
    "\n",
    "conf.model_path = f\"{conf.data_path}models/{conf.model_name}/\"\n",
    "os.makedirs(conf.data_path, exist_ok=True)\n",
    "os.makedirs(conf.model_path, exist_ok=True)\n",
    "\n",
    "# logging config is set globally thus we only need to call this in this file\n",
    "# imported function logs will follow the configuration\n",
    "setup_logging(save_dir=conf.model_path, log_config='./logger/standard_logger_config.json', default_level=log.INFO)\n",
    "log.info(\"=====================================BEGIN=====================================\")\n",
    "\n",
    "info = deepcopy(conf.__dict__)\n",
    "info[\"start_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# DATA LOADER SETUP\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "log.info(f\"Device: {device}\")\n",
    "info[\"device\"] = device.type\n",
    "conf.device = device\n",
    "\n",
    "# SET THE hyperparameterS\n",
    "conf.shaper_top_k = -1\n",
    "conf.use_classification = False\n",
    "conf.train_set_first = True\n",
    "conf.use_crime_types = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine the total crime counts in the next time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.mutual_information_plots import subplot_mi_curves, plot_mi_curves\n",
    "from utils.data_processing import to_percentile\n",
    "from utils.rolling import rolling_norm, flag_anomalies, periodic_rolling_mean\n",
    "\n",
    "plot_anomalies = True\n",
    "if plot_anomalies:\n",
    "    window = {\n",
    "        \"1H\":501,\n",
    "        \"24H\":51,\n",
    "        \"168H\":29,\n",
    "    }.get(FREQ)\n",
    "    \n",
    "    \n",
    "    period = 1\n",
    "    \n",
    "    thresh = 3\n",
    "    \n",
    "    \n",
    "    logging.warning(\"Plot outliers are done with symmetric windowing and are \" + \n",
    "                    \"only used to flag outliers not predict them\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        a = df[col].values\n",
    "\n",
    "        anoms = flag_anomalies(\n",
    "            data=a, thresh=thresh, window=window, period=period, center=True, mode='reflect')\n",
    "\n",
    "        ma = periodic_rolling_mean(data=a, window=window, period=period, center=True)\n",
    "        normed = rolling_norm(data=a, window=window, period=period, center=True)\n",
    "\n",
    "        fig = go.Figure(\n",
    "            data=[\n",
    "                go.Scatter(x=df.index,y=a,opacity=.5,name=f'Counts'),\n",
    "                go.Scatter(x=df.index[anoms],y=a[anoms],mode='markers',opacity=.5, name='Outliers'),\n",
    "                go.Scatter(x=df.index,y=ma,opacity=.5,name=f'MA'),\n",
    "            ],\n",
    "            layout=dict(\n",
    "                title_text=col,\n",
    "                title_x=0.5,\n",
    "                font=dict(family=\"STIXGeneral\"),\n",
    "                yaxis_title=\"Counts\",\n",
    "                xaxis_title=\"Date Time\",\n",
    "            ),\n",
    "        )\n",
    "        fig.write_image(f\"{save_folder}{FREQ}_outliers_{col}.png\".replace(' ','_'))\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from utils.plots import subplots_df, plot_df\n",
    "# from utils.mutual_information_plots import plot_mi_curves\n",
    "plot_mi_curves = True\n",
    "if plot_mi_curves:\n",
    "    temporal_variables = {\n",
    "        \"1H\":[\"Hour\", \"Day of Week\", \"Time of Month\", \"Time of Year\"],\n",
    "        \"1H\":[\"Hour\", \"Day of Week\"],\n",
    "        \"24H\":[\"Day of Week\", \"Time of Month\", \"Time of Year\"],\n",
    "        \"168H\":[\"Time of Month\", \"Time of Year\"],\n",
    "    }.get(FREQ, [\"Time of Month\", \"Time of Year\"])\n",
    "\n",
    "\n",
    "    max_offset = {\n",
    "        \"1H\":168*2,\n",
    "        \"24H\":365,\n",
    "        \"168H\":54,\n",
    "    }.get(FREQ)\n",
    "\n",
    "\n",
    "    for i, name in enumerate(df.columns):\n",
    "        a = df[name].values\n",
    "        \n",
    "        mutual_info_bins = 16 # 16\n",
    "#         print(f\"optimal bins: {get_optimal_bins(a)}\")\n",
    "#         a = to_percentile(a)\n",
    "#         a = np.round(np.log(1+a)) # whatch out for values between 1024 2048\n",
    "#         a = cut(np.log(1+a)) # whatch out for values between 1024 2048\n",
    "        \n",
    "        fig = subplot_mi_curves(\n",
    "            a=a,\n",
    "            t_range=df.index,\n",
    "            max_offset=max_offset,\n",
    "            norm=True,\n",
    "            log_norm=False,\n",
    "            bins=mutual_info_bins,\n",
    "            month_divisions=4,\n",
    "            year_divisions=12,\n",
    "            temporal_variables=temporal_variables,\n",
    "            title=f'{freq_title} {name} Mutual and Conditional Mutual Information',\n",
    "            a_title=f'{freq_title} {name} City Wide Counts',\n",
    "        )\n",
    "        fig.write_image(f\"{save_folder}{FREQ}_mi_plots_{name}.png\".replace(' ','_'))\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peridic Rolling Normilasation\n",
    "By applying rolling normilation we can control for the period nature of our signals. We subtract the signal by a periodic rolling mean and divide it by a period rolling standard deviation. We can also perform this on various periods e.g. 7 days and 365 days, which caters for weekly and yearly periodic cycles. By controlling for the signals periodic nature we essentially have a new signal show casing how many standard deviations a current time step is outside of the expected value for that day of the week and time of the year. Our potential models then just need to predict the residuals of our crime signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# try and normalise rolling data to mitigate the trends\n",
    "# from utils.rolling import rolling_norm\n",
    "\n",
    "# TODO: make use of RollingNormScaler instead?\n",
    "norm_offset = 0\n",
    "normalize_periodically = True\n",
    "if not normalize_periodically:\n",
    "    normed_df = df.copy()\n",
    "else:\n",
    "    logging.warning(\"Using rolling norm means values at the \" + \n",
    "                    \"start within the window will be set to NaN and dropped\")\n",
    "\n",
    "    max_offset = {\n",
    "        \"1H\":168*2 + 24,\n",
    "        \"24H\":365,\n",
    "        \"168H\":54,\n",
    "    }.get(FREQ)\n",
    "    \n",
    "    fig = plot_autocorr(**df,\n",
    "                  title=\"Autocorrelation by Crime Type before any Rolling Normalisation\",\n",
    "                  partial=False,\n",
    "                  max_offset=max_offset)\n",
    "    fig.write_image(f\"{save_folder}{FREQ}_auto_corr_normed_none.png\".replace(' ','_'))\n",
    "    fig.show()\n",
    "    \n",
    "    window, period, period_string = {\n",
    "        \"24H\": (52,7, \"Weekly\"),  # jumps in weeks\n",
    "        \"1H\":  (365,24, \"Daily\"),  # jumps in days\n",
    "#         \"24H\": (10,7, \"Weekly\"),  # jumps in weeks\n",
    "#         \"1H\":  (10,24, \"Daily\"),  # jumps in days\n",
    "        \"168H\": (7,52, \"Yearly\"),  # jumps in weeks\n",
    "    }.get(FREQ, (10,1))\n",
    "    logging.info(f\"Using rolling norm window: {window} and period: {period}\")\n",
    "    \n",
    "#     normed_df = rolling_norm(data=df,window=window,period=period).dropna()\n",
    "    normed_df = rolling_norm(data=df,window=window,period=period, offset=norm_offset)\n",
    "    # ensure only rows where all values are nan are trimmed - replace other nan values with 0\n",
    "    valid_rows = ~normed_df.isna().all(1)\n",
    "    normed_df[valid_rows] = normed_df[valid_rows].fillna(0)\n",
    "    normed_df = normed_df.dropna()\n",
    "    assert_no_nan(normed_df)\n",
    "    assert_valid_datetime_index(normed_df)\n",
    "    \n",
    "    \n",
    "    fig = plot_autocorr(**normed_df,\n",
    "              title=f\"Autocorrelation by Crime Type after {period_string} Rolling Normalisation\",\n",
    "              max_offset=max_offset)\n",
    "    fig.write_image(f\"{save_folder}{FREQ}_auto_corr_normed_{period_string.lower()}.png\".replace(' ','_'))\n",
    "    fig.show()\n",
    "    \n",
    "    fig = plot_df(pd.DataFrame(df[\"Total\"]),xlabel=\"Date\",ylabel=\"Count\",\n",
    "            title=f\"Total Crimes before any Rolling Normalisation\")\n",
    "    fig.write_image(f\"{save_folder}{FREQ}_total_crimes_normed_none.png\".replace(' ','_'))\n",
    "    fig.show()\n",
    "    \n",
    "    fig = plot_df(pd.DataFrame(normed_df[\"Total\"]),xlabel=\"Date\",ylabel=\"Scaled Count\",\n",
    "            title=f\"Total Crimes after {period_string} Rolling Normalisation\")\n",
    "    fig.write_image(f\"{save_folder}{FREQ}_total_crimes_normed_{period_string.lower()}.png\".replace(' ','_'))\n",
    "    fig.show()\n",
    "    \n",
    "    double_rolling_norm = True\n",
    "    if double_rolling_norm:\n",
    "        window2, period2, period_string2 = {\n",
    "            \"24H\": (10,365, \"Yearly\"),  # jumps in years\n",
    "            \"1H\":  (10,168, \"Weekly\"),  # jumps in weeks\n",
    "            \"168H\": (None,None,None),  # no years\n",
    "        }.get(FREQ, (None,None,None))\n",
    "        logging.info(f\"Using second rolling norm window: {window2} and period: {period2}\")\n",
    "\n",
    "        if period_string2:\n",
    "            # double norming takes out other periodic signals as well\n",
    "#             normed_df = rolling_norm(data=normed_df,window=window2,period=period2).dropna()\n",
    "            normed_df = rolling_norm(data=normed_df,window=window2,period=period2,offset=norm_offset)\n",
    "            # ensure only rows where all values are nan are trimmed - replace other nan values with 0\n",
    "            valid_rows = ~normed_df.isna().all(1)\n",
    "            normed_df[valid_rows] = normed_df[valid_rows].fillna(0)\n",
    "            normed_df = normed_df.dropna()\n",
    "            assert_no_nan(normed_df)\n",
    "            assert_valid_datetime_index(normed_df)\n",
    "\n",
    "            fig = plot_autocorr(**normed_df,\n",
    "                  title=f\"Autocorrelation by Crime Type after {period_string} and\" + \n",
    "                                f\" {period_string2} Rolling Normalisation\",\n",
    "                  max_offset=max_offset)\n",
    "            fig.write_image(f\"{save_folder}{FREQ}_auto_corr_normed_{period_string.lower()}_\" + \n",
    "                            f\"{period_string2.lower()}.png\".replace(' ','_'))\n",
    "            fig.show()\n",
    "\n",
    "            fig = plot_df(pd.DataFrame(normed_df[\"Total\"]),xlabel=\"Date\",ylabel=\"Scaled Count\",\n",
    "                    title=f\"Total Crimes after {period_string} and\" + \n",
    "                                        f\" {period_string2} Rolling Normalisation\")\n",
    "            fig.write_image(f\"{save_folder}{FREQ}_total_crimes_normed_{period_string.lower()}_\" + \n",
    "                            f\"{period_string2.lower()}.png\".replace(' ','_'))\n",
    "            fig.show()\n",
    "    assert len(np.unique(np.diff(normed_df.index.astype(int)))), \\\n",
    "\"Normed values are not contiguous, dropna method might have dropped values\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Min Max Scale vectors\n",
    "# from utils.data_processing import DataFrameMinMaxScaler\n",
    "# scaler = DataFrameMinMaxScaler(minimum=0, maximum=1, axis=0) # allows us to rescale the results afterwards\n",
    "# # total_df = scaler.fit_transform(df)\n",
    "# total_df = scaler.fit_transform(normed_df)\n",
    "\n",
    "# display(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_df = normalize_df_min_max(normed_df) # rescale between 0 and 1 again...is this realy needed?\n",
    "# total_df = normalize_df_mean_std(normed_df) # rescale between 0 and 1 again...is this realy needed?\n",
    "total_df = normed_df\n",
    "# total_df = df\n",
    "# total_df = normalize_df_min_max(df)\n",
    "# total_df = normalize_df_mean_std(df)\n",
    "total_df.plot(kind='kde',alpha=0.4,title='Normalised Crime Counts Distributions')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(9,6)\n",
    "plt.grid()\n",
    "plt.savefig(f\"{save_folder}{FREQ}_crime_counts_distribution_normed.png\")\n",
    "plt.show()\n",
    "display(total_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils.plots import plot_corr\n",
    "\n",
    "for temp, _normed, period_string in zip([df, total_df],['', '_normed'],['',' after Normalisation']):\n",
    "    corr = temp.loc[:, temp.columns != 'Total'].corr()\n",
    "    plot_corr(corr, title=f'Pearson Correlation Coefficients Between {freq_title} Crime Types{period_string}\\n')\n",
    "    plt.savefig(f\"{save_folder}{FREQ}_corr_matrix_pearson{_normed}.png\")\n",
    "\n",
    "    corr = temp.loc[:, temp.columns != 'Total'].pcorr()\n",
    "    plot_corr(corr, title=f'Partial Pearson Correlation Coefficients '+\n",
    "              f'Between {freq_title} Crime Types{period_string}\\n')\n",
    "    plt.savefig(f\"{save_folder}{FREQ}_corr_matrix_pearson_partial{_normed}.png\")\n",
    "\n",
    "    corr = temp.loc[:, temp.columns != 'Total'].corr(method='spearman')\n",
    "    plot_corr(corr, title=f'Spearman Correlation Coefficients Between {freq_title} Crime Types{period_string}\\n')\n",
    "    plt.savefig(f\"{save_folder}{FREQ}_corr_matrix_spearman{_normed}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_range = total_df.index[1:] # should be equal to the target times\n",
    "total_crime_types = total_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_vectors = encode_time_vectors(t_range)\n",
    "\n",
    "plot_time_vectors = True\n",
    "if plot_time_vectors:  \n",
    "    k = int(time_steps_per_day*365)\n",
    "    tv, tr = time_vectors[:k], t_range[:k]\n",
    "\n",
    "    t_vec_names = {\n",
    "        \"1H\":['$H_{sin}$', '$H_{cos}$', '$DoW_{sin}$', '$DoW_{cos}$',\n",
    "              '$ToM_{sin}$', '$ToM_{cos}$', '$ToY_{sin}$', '$ToY_{cos}$', '$Wkd$'],\n",
    "        \"24H\": ['$DoW_{sin}$','$DoW_{cos}$', '$ToM_{sin}$',\n",
    "               '$ToM_{cos}$', '$ToY_{sin}$','$ToY_{cos}$', '$Wkd$'],\n",
    "        \"168H\":['$ToM_{sin}$','$ToM_{cos}$', '$ToY_{sin}$','$ToY_{cos}$'],\n",
    "    }.get(FREQ)\n",
    "\n",
    "\n",
    "    pio.templates.default = \"plotly\"\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=tv.T,\n",
    "            y=t_vec_names,\n",
    "            x=tr,\n",
    "        ),\n",
    "        layout=dict(\n",
    "            title=f\"Encoded Time Vectors on {freq_title} Level\",\n",
    "            title_x=0.5,\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Encoded Vector Values\",\n",
    "            font=dict(family=\"STIXGeneral\"),\n",
    "        ),\n",
    "    )\n",
    "    fig.write_image(f\"{save_folder}{FREQ}_time_vector_encoding.png\")\n",
    "    fig.show()\n",
    "    pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoRegression Test on the data and how time vectors influence auto-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def auto_split(data, max_offset):\n",
    "#     \"\"\"\n",
    "#     takes time series and stacks time delayed versions of the series on axis 1\n",
    "#     returns then the first column of the stack and the last columns separated like y, X\n",
    "#     \"\"\"\n",
    "#     cols = []\n",
    "#     for i in range(1,max_offset):\n",
    "#         cols.append(data[max_offset-i:-i])\n",
    "#     stack = np.concatenate(cols,axis=1)\n",
    "#     return stack[:,0:1], stack[:,1:]\n",
    "\n",
    "\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# # total_crime_types = total_df.values\n",
    "# # input_data = np.concatenate([input_data,time_vectors],axis=1)\n",
    "\n",
    "# time_df = pd.DataFrame(data=time_vectors,\n",
    "#              index=t_range,\n",
    "#              columns=t_vec_names)\n",
    "# crime_time_df = pd.concat((total_df, time_df),axis=1).dropna()\n",
    "# crime_time_df['Wd'] = crime_time_df.index.day_name()\n",
    "# y = crime_time_df.Total # current crime \n",
    "\n",
    "\n",
    "# X = crime_time_df[[\n",
    "#     '$DoW_{sin}$',\n",
    "#     '$DoW_{cos}$',\n",
    "# #     '$ToM_{sin}$',\n",
    "# #     '$ToM_{cos}$',\n",
    "#     '$ToY_{sin}$',\n",
    "#     '$ToY_{cos}$',\n",
    "# #     '$Wkd$',\n",
    "# ]] # current time vec\n",
    "\n",
    "\n",
    "# #option 1\n",
    "# # X = np.concatenate((X[1:].values,crime_time_df[['Total']][:-1].values), axis=1)\n",
    "# # option 2\n",
    "# # X = crime_time_df[['Total']][:-1].values\n",
    "# #option 1 and 2\n",
    "# # y = y.values[1:]\n",
    "\n",
    "# # option 3\n",
    "# y, X = auto_split(crime_time_df[['Total']].values,380)\n",
    "\n",
    "# tst_size = int(.2*len(X))\n",
    "# y_tst, X_tst = y[-tst_size:], X[-tst_size:]\n",
    "# y_trn, X_trn = y[:-tst_size], X[:-tst_size]\n",
    "\n",
    "\n",
    "# lr = LinearRegression()\n",
    "# lr.fit(X=X_trn,y=y_trn)\n",
    "\n",
    "# y_hat = lr.predict(X=X_trn)\n",
    "# plot(y_hat=y_hat.flatten(),y_trn=y_trn.flatten()).show()\n",
    "# display(forecast_metrics(y_true=y_trn.flatten(), y_score=y_hat))\n",
    "\n",
    "# y_hat = lr.predict(X=X_tst)\n",
    "# plot(y_hat=y_hat.flatten(),y_tst=y_tst.flatten()).show()\n",
    "# display(forecast_metrics(y_true=y_tst.flatten(), y_score=y_hat))\n",
    "\n",
    "\n",
    "# resid = y_tst - y_hat \n",
    "# plot_autocorr(resid=resid,y=y, max_offset=800,partial=False,title='ACF').show()\n",
    "# # plot_autocorr(resid=resid,y=y, max_offset=800,partial=True,title='PACF').show()\n",
    "\n",
    "# coef = pd.Series(lr.coef_.flatten())\n",
    "\n",
    "# plot(coef=coef, abs_coef=np.abs(coef)).show()\n",
    "\n",
    "# # determine how much MAE improvemet there is if we make the AR order larger\n",
    "\n",
    "# # ---------------------------------------------------------\n",
    "# # from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# # mae_list = []\n",
    "# # offsets = list(range(3,20))\n",
    "# # for i in offsets:\n",
    "# #     y, X = auto_split(crime_time_df[['Total']].values,i)\n",
    "# #     lr = LinearRegression()\n",
    "# #     lr.fit(X=X,y=y)\n",
    "# #     y_hat = lr.predict(X=X)\n",
    "# #     mae = mean_absolute_error(y,y_hat)\n",
    "# #     mae_list.append(mae)\n",
    "    \n",
    "# # mae_per_offsets = pd.DataFrame(index=offsets,data=mae_list,columns=['MAE'])\n",
    "# # plot_df(df=mae_per_offsets,title='MAE per AR Order', xlabel='AR Order', ylabel='MAE').show()\n",
    "\n",
    "# # --------------------------------------------------------\n",
    "\n",
    "# # coef_pct = pd.Series(coef).rank(pct=True)\n",
    "# # offset_coef = coef[coef_pct > .1]\n",
    "# # # offset_coef = coef[np.abs(coef) > .01]\n",
    "# # offset_coef.index = offset_coef.index + 1\n",
    "# # class AutoRegressor:\n",
    "# #     def __init__(self, offset_weights):\n",
    "# #         self.offsets = np.array(list(offset_weights.keys()))\n",
    "# #         self.weights = np.array(list(offset_weights.values()))\n",
    "# #         display(offset_weights)\n",
    "# #         display(f\"offsets: {self.offsets}\")\n",
    "# #         display(f\"weights: {self.weights}\")\n",
    "        \n",
    "# #         self.max_offset = np.max(self.offsets)\n",
    "        \n",
    "# #     def predict(self,x):\n",
    "# #         x = x.flatten()\n",
    "# #         r = np.empty(x.shape)\n",
    "# #         r.fill(np.NaN)\n",
    "        \n",
    "# #         for i in range(self.max_offset,len(x)):\n",
    "# #             r[i] = np.dot(x[i-self.offsets],self.weights)\n",
    "            \n",
    "# #         return r\n",
    "        \n",
    "# # y_hat_ar = AutoRegressor(dict(offset_coef)).predict(y)\n",
    "\n",
    "# # plot(y_hat_ar=y_hat_ar,y_hat=y_hat.flatten(),y=y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Crime Time Correlations maps how each crime type is correlated to \n",
    "# certain time variables like time of day or day of week\n",
    "\n",
    "plot_crime_time_correlations = True\n",
    "if plot_crime_time_correlations:  \n",
    "    time_df = pd.DataFrame(data=time_vectors,\n",
    "                 index=t_range,\n",
    "                 columns=list(map(lambda s: s.replace('$','').replace('{','').replace('}',''), t_vec_names)))\n",
    "\n",
    "    time_df['Weekend'] = time_df['Wkd']\n",
    "    del time_df['Wkd']\n",
    "    time_df['Day of Week'] = time_df.index.dayofweek\n",
    "    time_df['Month'] = time_df.index.month\n",
    "    time_df['Day of Month'] = time_df.index.day\n",
    "    time_df['Day of Year'] = time_df.index.dayofyear\n",
    "    time_df['Week of Year'] = time_df.index.weekofyear\n",
    "\n",
    "    t_size = time_df.shape[1]\n",
    "    \n",
    "    crime_time_df = pd.concat((time_df,df.loc[:, total_df.columns != 'Total']),axis=1).dropna()\n",
    "    corr = crime_time_df.corr().iloc[t_size:, :t_size]\n",
    "    plot_corr(corr, title=f'Pearson Correlation Coefficients Between {freq_title} Crime' +\n",
    "              ' Types and Time Variables\\n')\n",
    "    plt.savefig(f\"{save_folder}{FREQ}_corr_matrix_crime_time.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    corr = crime_time_df.pcorr().iloc[t_size:, :t_size]\n",
    "    plot_corr(corr, title=f'Partial Pearson Correlation Coefficients Between {freq_title}' +\n",
    "              ' Crime Types and Time Variables\\n')\n",
    "    plt.savefig(f\"{save_folder}{FREQ}_corr_matrix_crime_time_partial.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    crime_time_df = pd.concat((time_df,total_df.loc[:, total_df.columns != 'Total']),axis=1).dropna()\n",
    "    corr = crime_time_df.corr().iloc[t_size:, :t_size]\n",
    "    plot_corr(corr, title=f'Pearson Correlation Coefficients Between Normalised {freq_title} Crime ' + \n",
    "              'Types and Time Variables\\n')\n",
    "    plt.savefig(f\"{save_folder}{FREQ}_corr_matrix_crime_time_normed.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    corr = crime_time_df.pcorr().iloc[t_size:, :t_size]\n",
    "    plot_corr(corr, title=f'Partial Pearson Correlation Coefficients Between Normalised {freq_title}' +\n",
    "              ' Crime Types and Time Variables\\n')\n",
    "    plt.savefig(f\"{save_folder}{FREQ}_corr_matrix_crime_time_normed_partial.png\")\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conf.seq_len = 60 # int(14*time_steps_per_day) + 1\n",
    "conf.seq_len = {\n",
    "    \"24H\": 90,\n",
    "    \"1H\": 168,\n",
    "    \"168H\": 52,\n",
    "}.get(FREQ, 60)\n",
    "\n",
    "conf.batch_size = 128\n",
    "\n",
    "logging.info(f\"Using sequence length: {conf.seq_len}\")\n",
    "\n",
    "input_data= total_crime_types[:-1] \n",
    "\n",
    "predict_only_total = True\n",
    "if predict_only_total:\n",
    "    target_data = total_crime_types[1:, 0:1] \n",
    "else:\n",
    "    target_data = total_crime_types[1:]\n",
    "\n",
    "assert len(input_data) == len(time_vectors), \\\n",
    "    f\"len(input_data) != len(time_vectors), {len(input_data)},{len(time_vectors)}\"\n",
    "\n",
    "use_time_vectors = True\n",
    "if use_time_vectors:\n",
    "    logging.info(\"using time vectors in input\")\n",
    "    input_data = np.concatenate([input_data,time_vectors],axis=1)\n",
    "else:\n",
    "    logging.info(\"NOT using time vectors in input\")\n",
    "\n",
    "\n",
    "input_size = input_data.shape[-1]\n",
    "output_size = target_data.shape[-1]\n",
    "\n",
    "assert len(input_data) == len(t_range)\n",
    "\n",
    "test_size = {\n",
    "    \"24H\":365,\n",
    "    \"H\": 8760,\n",
    "    \"1H\":8760,\n",
    "    \"168H\":52*2,\n",
    "}.get(FREQ)\n",
    "\n",
    "tst_ratio = test_size/len(input_data)\n",
    "\n",
    "loaders = SequenceDataLoaders( # setup data loader 1\n",
    "    input_data=input_data,\n",
    "    target_data=target_data,\n",
    "    t_range=t_range,\n",
    "    batch_size=conf.batch_size,\n",
    "    seq_len=conf.seq_len,\n",
    "    shuffle=conf.shuffle,\n",
    "    num_workers=0,\n",
    "    val_ratio=0.2, #0.5,\n",
    "    tst_ratio=tst_ratio,\n",
    "    overlap_sequences=overlap_sequences,\n",
    ")\n",
    "\n",
    "input_data.shape, target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter Optimisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "\n",
    "def train_evaluate_hyper(hyper_parameters):\n",
    "    logging.info(f\"Running HyperOpt Trial with: {pformat(hyper_parameters)}\")\n",
    "    \n",
    "    # hyper param setup\n",
    "    hidden_size = int(hyper_parameters.get('hidden_size',50))\n",
    "    num_layers = int(hyper_parameters.get('num_layers',5))\n",
    "    conf.weight_decay = hyper_parameters.get('weight_decay', 1e-4)\n",
    "    conf.lr = hyper_parameters.get('lr', 1e-3)\n",
    "\n",
    "    default_seq_len = {\n",
    "        \"24H\": 90,\n",
    "        \"1H\": 168,\n",
    "        \"168H\": 52,\n",
    "    }.get(FREQ, 60)\n",
    "    conf.seq_len = int(hyper_parameters.get('seq_len', default_seq_len))\n",
    "\n",
    "    conf.early_stopping = True\n",
    "    conf.patience = 30\n",
    "    conf.min_epochs = 1\n",
    "    conf.max_epochs = 10_000\n",
    "    \n",
    "    conf.seed = np.random.randint(10_000)*(int(time()) % hidden_size + num_layers)\n",
    "    \n",
    "    set_system_seed(conf.seed) # should be reset with each model instatiation\n",
    "    \n",
    "    \n",
    "    loaders = SequenceDataLoaders( # setup data loader 2: hyper opt\n",
    "        input_data=input_data,\n",
    "        target_data=target_data,\n",
    "        t_range=t_range,\n",
    "        batch_size=conf.batch_size,\n",
    "        seq_len=conf.seq_len,\n",
    "        shuffle=conf.shuffle,\n",
    "        num_workers=0,\n",
    "        val_ratio=0.5,\n",
    "        tst_ratio=tst_ratio,\n",
    "        overlap_sequences=overlap_sequences,\n",
    "    )    \n",
    "    \n",
    "    # model setup\n",
    "    model = GRUFNN(\n",
    "        input_size=input_size,\n",
    "        hidden_size0=hidden_size, \n",
    "        hidden_size1=hidden_size//2, \n",
    "        output_size=output_size,\n",
    "        num_layers=num_layers,\n",
    "    ).to(conf.device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = torch.optim.AdamW(params=model.parameters(),lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "    # training\n",
    "    trn_epoch_losses, val_epoch_losses, stopped_early = train_model(\n",
    "        model=model,\n",
    "        optimiser=optimiser,\n",
    "        loaders=loaders,\n",
    "        train_epoch_fn=train_epoch_for_sequence_model,\n",
    "        loss_fn=criterion,\n",
    "        conf=conf,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    # load the saved best validation model\n",
    "    # Load latest or best validation model\n",
    "    conf.checkpoint = \"best_val\"\n",
    "    log.info(f\"Loading model from checkpoint ({conf.checkpoint}) for evaluation\")\n",
    "    log.info(f\"Loading model from {conf.model_path}\")\n",
    "    model_state_dict = torch.load(f\"{conf.model_path}model_{conf.checkpoint}.pth\",\n",
    "                                  map_location=conf.device.type)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    # evaluation\n",
    "    val_y_true, val_y_score = evaluate_sequence_model(model, loaders.validation_loader, conf)\n",
    "    \n",
    "    return forecast_metrics(y_true=val_y_true, y_score=val_y_score)['MASE']\n",
    "\n",
    "\n",
    "total_hyper_opt_trials = 20 # 20\n",
    "optimize_hyper_params = True\n",
    "if optimize_hyper_params:\n",
    "    # Hyper Opt Training\n",
    "\n",
    "    from ax.plot.contour import plot_contour\n",
    "    from ax.plot.trace import optimization_trace_single_method\n",
    "    from ax.service.managed_loop import optimize\n",
    "    from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "    from ax.utils.tutorials.cnn_utils import load_mnist, train, evaluate, CNN\n",
    "\n",
    "    # PARAM_CLASSES = [\"range\", \"choice\", \"fixed\"]\n",
    "    # PARAM_TYPES = {\"int\": int, \"float\": float, \"bool\": bool, \"str\": str}\n",
    "    # potential parameters\n",
    "    hyper_params = [\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-4, 1e-2], \"log_scale\": True},\n",
    "        {\"name\": \"weight_decay\", \"type\": \"range\", \"bounds\": [1e-8, 1e-2], \"log_scale\": True},    \n",
    "#         {\"name\": \"hidden_size\", \"type\": \"choice\", \"values\": [8,16,32,64,128]},\n",
    "#         {\"name\": \"num_layers\", \"type\": \"choice\", \"values\": [1,2,4,8]},\n",
    "#         {\"name\": \"seq_len\", \"type\": \"choice\", \"values\": [7,24,64,128]},\n",
    "        {\"name\": \"hidden_size\", \"type\": \"range\", \"bounds\": [8,128]},\n",
    "        {\"name\": \"num_layers\", \"type\": \"range\", \"bounds\": [1,8]},\n",
    "        {\"name\": \"seq_len\", \"type\": \"range\", \"bounds\": [8,128]},\n",
    "    ]\n",
    "\n",
    "    best_parameters, values, experiment, hyper_model = optimize(\n",
    "        parameters=hyper_params,\n",
    "        evaluation_function=train_evaluate_hyper,\n",
    "        objective_name='MASE',\n",
    "        minimize=True, # change when the objective is a loss\n",
    "        total_trials=total_hyper_opt_trials,\n",
    "    #     random_seed=1, # optional\n",
    "    )\n",
    "\n",
    "    experiment_data = experiment.fetch_data()\n",
    "    display(experiment_data.df)\n",
    "    display(pd.DataFrame([best_parameters]))\n",
    "    \n",
    "#     render(plot_contour(model=hyper_model, param_x='lr', param_y='weight_decay', metric_name='MASE'))\n",
    "else:\n",
    "    # default values to set when not using hyperparameter optimization\n",
    "    best_parameters = {\n",
    "        'lr': 1e-3,\n",
    "        'weight_decay': 1e-6,\n",
    "        'hidden_size': 8,\n",
    "        'num_layers': 1,\n",
    "        'seq_len': 28,\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(best_parameters, f\"{save_folder}{FREQ}_best_hyper_params.json\".replace(' ','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame([best_parameters]).iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-run evaluation with various seeds\n",
    "Used to show that the hyperparameters/model is insensitive the seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials(num_trials=10):\n",
    "    \"\"\"\n",
    "    Run a full experiment on GRUFNN model multiple times with different seeds\n",
    "    Data and hyperparameters must be set before hand.\n",
    "    This function acts as a closure, i.e. some variables are created outside the scope of the function.\n",
    "    \n",
    "    Used to:\n",
    "        1. Setup data loader with predefined input and target data\n",
    "        2. Setup model\n",
    "        3. Train model with a validation set once to determine the num_epochs\n",
    "        4. Run trials loops:\n",
    "            4.a. Reset seed\n",
    "            4.b. Resetup model\n",
    "            4.c. Train with train_val set for predetermined num_epochs\n",
    "            4.d. Run evaluations on the model\n",
    "        5. Return a dataframe with seed and forecast metrics for each trial run\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    trial_metrics_list = []\n",
    "    \n",
    "    conf.early_stopping = True\n",
    "    conf.patience = 30\n",
    "    conf.min_epochs = 1\n",
    "    conf.max_epochs = 10_000\n",
    "\n",
    "    conf.lr = best_parameters['lr']\n",
    "    conf.weight_decay = best_parameters['weight_decay']\n",
    "    hidden_size = best_parameters['hidden_size'] \n",
    "    num_layers = best_parameters['num_layers']\n",
    "    conf.seq_len = best_parameters['seq_len']\n",
    "    \n",
    "    #====================================== can be put into loop as well\n",
    "    # model setup\n",
    "    conf.seed = int(time())  # unique seed for each run\n",
    "    set_system_seed(conf.seed) # should be reset with each model instatiation\n",
    "    \n",
    "    loaders = SequenceDataLoaders( # setup data loader 3: run trial\n",
    "        input_data=input_data,\n",
    "        target_data=target_data,\n",
    "        t_range=t_range,\n",
    "        batch_size=conf.batch_size,\n",
    "        seq_len=conf.seq_len,\n",
    "        shuffle=conf.shuffle,\n",
    "        num_workers=0,\n",
    "        val_ratio=0.5,\n",
    "        tst_ratio=tst_ratio,\n",
    "        overlap_sequences=overlap_sequences,\n",
    "    )   \n",
    "    \n",
    "    model = GRUFNN(\n",
    "        input_size=input_size,\n",
    "        hidden_size0=hidden_size, \n",
    "        hidden_size1=hidden_size//2, \n",
    "        output_size=output_size,\n",
    "        num_layers=num_layers,\n",
    "    ).to(conf.device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimiser = torch.optim.AdamW(params=model.parameters(),lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "    trn_epoch_losses, val_epoch_losses, stopped_early = train_model(\n",
    "        model=model,\n",
    "        optimiser=optimiser,\n",
    "        loaders=loaders,\n",
    "        train_epoch_fn=train_epoch_for_sequence_model,\n",
    "        loss_fn=criterion,\n",
    "        conf=conf,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    logging.info(f\"best validation: {np.min(val_epoch_losses):.6f} @ epoch: {np.argmin(val_epoch_losses) + 1}\")\n",
    "\n",
    "    # full train-val dataset training \n",
    "    conf.max_epochs = np.argmin(val_epoch_losses) + 1 # because of index starting at zero\n",
    "    #====================================== can be put into loop as well\n",
    "\n",
    "    for i in range(num_trials):\n",
    "        logging.info(f\"Starting trial {i+1} of {num_trials}.\")\n",
    "\n",
    "        conf.seed = int(time() + 10*i)  # unique seed for each run\n",
    "        set_system_seed(conf.seed) # should be reset with each model instatiation\n",
    "        model = GRUFNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size0=hidden_size, \n",
    "            hidden_size1=hidden_size//2, \n",
    "            output_size=output_size,\n",
    "            num_layers=num_layers,\n",
    "        ).to(conf.device)\n",
    "\n",
    "        optimiser = torch.optim.AdamW(params=model.parameters(),lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "        trn_val_epoch_losses = train_model_final(\n",
    "            model=model,\n",
    "            optimiser=optimiser,\n",
    "            loaders=loaders,\n",
    "            train_epoch_fn=train_epoch_for_sequence_model,\n",
    "            loss_fn=criterion,\n",
    "            conf=conf,\n",
    "        )\n",
    "\n",
    "        tst_y_true, tst_y_score = evaluate_sequence_model(model, loaders.test_loader, conf)\n",
    "        trial_metrics = forecast_metrics(y_true=tst_y_true, y_score=tst_y_score)\n",
    "        trial_metrics['Seed'] = conf.seed\n",
    "        trial_metrics_list.append(trial_metrics)\n",
    "        \n",
    "        logging.info(f\"Completed trial {i+1} of {num_trials}.\")\n",
    "    \n",
    "    return trial_metrics_list\n",
    "\n",
    "num_trials=10\n",
    "trial_results = run_trials(num_trials=num_trials)\n",
    "\n",
    "display(pd.DataFrame(trial_results))\n",
    "\n",
    "pd.DataFrame(trial_results)[['MASE']].plot(kind='box',\n",
    "                                           title=f\"{freq_title} Crime GRUFNN MASE Score for {num_trials} Seeds\\n\")\n",
    "plt.savefig(f\"{save_folder}{FREQ}_result_seed_consistency.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper params-> set seed -> set model -> set optimiser\n",
    "conf.seed = int(time())  # 1607355910\n",
    "\n",
    "conf.lr = best_parameters['lr']\n",
    "conf.weight_decay = best_parameters['weight_decay']\n",
    "hidden_size = best_parameters['hidden_size']\n",
    "num_layers = best_parameters['num_layers']\n",
    "conf.seq_len = best_parameters['seq_len']\n",
    "\n",
    "set_system_seed(conf.seed) # should be reset with each model instatiation\n",
    "model = GRUFNN(\n",
    "    input_size=input_size,\n",
    "    hidden_size0=hidden_size, \n",
    "    hidden_size1=hidden_size//2, \n",
    "    output_size=output_size,\n",
    "    num_layers=num_layers,\n",
    ").to(conf.device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimiser = torch.optim.AdamW(params=model.parameters(),lr=conf.lr, weight_decay=conf.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model with early stopping -> get max epochs for train validation set\n",
    "\n",
    "conf.resume = False\n",
    "conf.checkpoint = \"final\" #\"latest\" # [\"best_val\", \"best_trn\", \"best_trn_val\"]\n",
    "conf.early_stopping = True\n",
    "conf.patience = 30\n",
    "conf.min_epochs = 1\n",
    "conf.max_epochs = 10_000\n",
    "\n",
    "trn_epoch_losses, val_epoch_losses, stopped_early = train_model(\n",
    "    model=model,\n",
    "    optimiser=optimiser,\n",
    "    loaders=loaders,\n",
    "    train_epoch_fn=train_epoch_for_sequence_model,\n",
    "    loss_fn=criterion,\n",
    "    conf=conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"best validation: {np.min(val_epoch_losses):.6f} @ epoch: {np.argmin(val_epoch_losses) + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation sets\n",
    "\n",
    "from utils.interactive import plot_interactive_epoch_losses\n",
    "fig = plot_interactive_epoch_losses(trn_epoch_losses, val_epoch_losses)\n",
    "fig.write_image(f\"{save_folder}{FREQ}_loss_val_plot.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set max epochs -> reset to same seed as before -> recreate model -> recreate optimiser -> train final model\n",
    "\n",
    "conf.max_epochs = np.argmin(val_epoch_losses) + 1 # because of index starting at zero\n",
    "\n",
    "set_system_seed(conf.seed) # should be reset with each model instatiation\n",
    "model = GRUFNN(\n",
    "    input_size=input_size,\n",
    "    hidden_size0=hidden_size, \n",
    "    hidden_size1=hidden_size//2, \n",
    "    output_size=output_size,\n",
    "    num_layers=num_layers,\n",
    ").to(conf.device)\n",
    "\n",
    "optimiser = torch.optim.AdamW(params=model.parameters(),lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "trn_val_epoch_losses = train_model_final(\n",
    "    model=model,\n",
    "    optimiser=optimiser,\n",
    "    loaders=loaders,\n",
    "    train_epoch_fn=train_epoch_for_sequence_model,\n",
    "    loss_fn=criterion,\n",
    "    conf=conf,\n",
    ")\n",
    "\n",
    "plot(trn_val_epoch_losses=trn_val_epoch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest or best validation or final model\n",
    "# conf.checkpoint = \"latest\"\n",
    "# conf.checkpoint = \"best_val\"\n",
    "# conf.checkpoint = \"best_trn\"\n",
    "conf.checkpoint = \"final\" # train and validation set trained model\n",
    "\n",
    "log.info(f\"Loading model from checkpoint ({conf.checkpoint}) for evaluation\")\n",
    "\n",
    "# resume from previous check point or resume from best validation score checkpoint\n",
    "# load model state\n",
    "log.info(f\"loading model from {conf.model_path}\")\n",
    "model_state_dict = torch.load(f\"{conf.model_path}model_{conf.checkpoint}.pth\", map_location=conf.device.type)\n",
    "model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FREQ == '1H':\n",
    "    step=24\n",
    "    max_steps=14\n",
    "elif FREQ == '24H':\n",
    "    step=7\n",
    "    max_steps=6\n",
    "elif FREQ == '168H':\n",
    "    step=4#52\n",
    "    max_steps=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "class EWM:\n",
    "    def __init__(self, series=None):\n",
    "        self.alpha = .9\n",
    "        self.options = np.arange(0.001,0.999,0.001)\n",
    "        \n",
    "        if series is not None:\n",
    "            self.fit(series)\n",
    "            \n",
    "    def __call__(self, series):\n",
    "        return self.predict(series)\n",
    "        \n",
    "    def fit(self, series):\n",
    "        losses = []\n",
    "        for alpha in self.options:\n",
    "            self.alpha = alpha\n",
    "            pred = self.predict(series)\n",
    "            loss = mean_absolute_error(series, pred)\n",
    "            losses.append(loss)\n",
    "            \n",
    "        self.alpha = self.options[np.argmin(losses)]\n",
    "#         print(f\"alpha => {self.alpha}\")\n",
    "    \n",
    "    def predict(self, series):\n",
    "        series = np.pad(series, (1,0), 'edge')\n",
    "        \n",
    "        pred = pd.Series(series).ewm(alpha=self.alpha).mean().values[:-1]\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "def compare_time_series_metrics(\n",
    "    y_true, \n",
    "    y_score, \n",
    "    t_range,\n",
    "    feature_names, \n",
    "    is_training_set,\n",
    "    step=24, \n",
    "    max_steps=29,\n",
    "    alpha=0.5, \n",
    "    rangeslider_visible=True,\n",
    "):\n",
    "    kwargs = dict()\n",
    "\n",
    "    offset = step * max_steps\n",
    "\n",
    "    assert len(y_true.shape) == 2\n",
    "    \n",
    "    \n",
    "    model_name_dict = {}\n",
    "    feature_results = {}\n",
    "    \n",
    "    if is_training_set:\n",
    "        training_str = \"(Training Data)\"\n",
    "    else:\n",
    "        training_str = \"(Test Data)\"\n",
    "    \n",
    "     \n",
    "    for i, feat_name in enumerate(feature_names):\n",
    "        kwargs[f\"{feat_name}_y_score\"] = y_score[offset:, i]\n",
    "        kwargs[f\"{feat_name}_y_true\"] = y_true[offset:, i]\n",
    "        kwargs[f\"{feat_name}_y_ha\"] = historic_average(y_true[:, i], step=step,\n",
    "                                                       max_steps=max_steps)[offset - 1:-1]    \n",
    "        \n",
    "        ewm = EWM(y_true[:,i])\n",
    "        kwargs[f\"{feat_name}_y_ewm\"] = ewm(y_true[:,i])[offset:]\n",
    "        \n",
    "        feature_results[feat_name] = {\n",
    "            \"Ground Truth\": y_true[offset:, i],\n",
    "            \"GRUFNN\": y_score[offset:, i],\n",
    "            f\"HA({step},{max_steps})\": historic_average(y_true[:, i],\n",
    "                                                  step=step,max_steps=max_steps)[offset - 1:-1],\n",
    "            f\"EWM({ewm.alpha:.3f})\": ewm(y_true[:,i])[offset:],\n",
    "        }\n",
    "\n",
    "        model_name_dict[feat_name] = {\n",
    "            \"y_score\": \"GRUFNN\",\n",
    "            \"y_true\": \"Ground Truth\",\n",
    "            \"y_ha\": f\"HA({step},{max_steps})\",\n",
    "            \"y_ewm\": f\"EWM({ewm.alpha:.3f})\",\n",
    "        }\n",
    "        \n",
    "        fig = plot_time_signals(\n",
    "            t_range=t_range[offset:], \n",
    "            alpha=alpha,\n",
    "            title=f'{freq_title} {feat_name.title()} Predicted Normalised City Counts {training_str}',\n",
    "            ylabel='Normalised Counts [0,1]',\n",
    "            xlabel='Date',\n",
    "            rangeslider_visible=rangeslider_visible,\n",
    "            **feature_results[feat_name]\n",
    "        )\n",
    "\n",
    "        fig.write_image(f\"{save_folder}{FREQ}_predictions_{feat_name}_\"+\n",
    "                        f\"{training_str.lower().replace(' ','_').replace(')','_').replace('(','')}.png\")\n",
    "        fig.show()\n",
    "\n",
    "    ll = [] # {}\n",
    "    for k, v in kwargs.items():\n",
    "        i = k.find(\"_\")\n",
    "        feat_name = k[:i]\n",
    "        model_type = k[i+1:]\n",
    "        if model_type == 'y_true':\n",
    "            continue\n",
    "        row = {}\n",
    "        crime_type_name = feat_name.title()\n",
    "        model_name = model_name_dict[feat_name][model_type]\n",
    "        row['Crime Type'] = crime_type_name\n",
    "        row['Model'] = model_name\n",
    "        row_ = forecast_metrics(y_true=kwargs[f\"{feat_name}_y_true\"], y_score=kwargs[k])\n",
    "        row = {**row, **row_}\n",
    "        \n",
    "        ll.append(row)\n",
    "\n",
    "#     plot_time_signals(t_range=t_range[offset:], alpha=alpha,\n",
    "#                       title=f'{freq_title} Predicted Normalised City Counts',\n",
    "#                       yaxis_title='Normalised Counts [0,1]',**kwargs).show()\n",
    "\n",
    "    metrics = DataFrame(ll)\n",
    "    metrics.sort_values(['Crime Type','MASE'],inplace=True)\n",
    "    metrics.reset_index(inplace=True, drop='index')\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import write_txt\n",
    "metrics_folder = f\"./data/processed/{data_sub_path}/metrics/\"\n",
    "os.makedirs(metrics_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_y_true, trn_y_score = evaluate_sequence_model(model, loaders.train_loader, conf)\n",
    "\n",
    "# trn_y_score = scaler.inverse_transform(trn_y_score)\n",
    "# trn_y_true = scaler.inverse_transform(trn_y_true)\n",
    "\n",
    "trn_metrics = compare_time_series_metrics(\n",
    "    y_true=trn_y_true,\n",
    "    y_score=trn_y_score,\n",
    "    t_range=loaders.train_loader.dataset.t_range[-len(trn_y_true):],\n",
    "    feature_names=list(total_df.columns[:trn_y_true.shape[1]]),#feature_names[:trn_y_true.shape[1]],\n",
    "    step=step,\n",
    "    max_steps=max_steps,\n",
    "    is_training_set=True,\n",
    "    rangeslider_visible=False,\n",
    ")\n",
    "display(trn_metrics)\n",
    "\n",
    "trn_metrics_latex = trn_metrics.round(decimals=3).to_latex(\n",
    "    index=False,\n",
    "    caption=f\"{freq_title} Crime Count Forecasting Metrics (Training Data)\",\n",
    "    label=\"tab:daily-crime-count-metrics\",\n",
    ")\n",
    "\n",
    "pd.to_pickle(trn_metrics, f\"{metrics_folder}{FREQ}_metrics_train.pkl\")\n",
    "write_txt(trn_metrics_latex, f\"{metrics_folder}{FREQ}_metrics_train_latex.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_y_true, tst_y_score = evaluate_sequence_model(model, loaders.test_loader, conf)\n",
    "# tst_y_true, tst_y_score = tst_y_true[:,0], tst_y_score[:,0] \n",
    "\n",
    "tst_metrics = compare_time_series_metrics(\n",
    "    y_true=tst_y_true, \n",
    "    y_score=tst_y_score,\n",
    "    t_range=loaders.test_loader.dataset.t_range[-len(tst_y_true):],\n",
    "    feature_names=list(total_df.columns[:tst_y_true.shape[1]]),\n",
    "    is_training_set=False,\n",
    "    step=step,\n",
    "    max_steps=max_steps,\n",
    "    rangeslider_visible=False,\n",
    ")\n",
    "\n",
    "display(tst_metrics)\n",
    "\n",
    "tst_metrics_latex = tst_metrics.round(decimals=3).to_latex(\n",
    "    index=False,\n",
    "    caption=f\"{freq_title} Crime Count Forecasting Metrics (Test Data)\",\n",
    "    label=\"tab:daily-crime-count-metrics\",\n",
    ")\n",
    "\n",
    "pd.to_pickle(tst_metrics, f\"{metrics_folder}{FREQ}_metrics_test.pkl\")\n",
    "write_txt(tst_metrics_latex, f\"{metrics_folder}{FREQ}_metrics_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"STOP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Test Results (1H)  `(best 0.700818)`\n",
    "```python\n",
    "hidden_size = 20\n",
    "epochs = 4\n",
    "seq_len = 168\n",
    "batch_size = 128\n",
    "lr = 1-e2\n",
    "seed = 1606470692\n",
    "num_layer=1\n",
    "\n",
    "\n",
    "GRUFNN(\n",
    "  (gru): GRU(18, 20, batch_first=True)\n",
    "  (lin1): Linear(in_features=20, out_features=10, bias=True)\n",
    "  (lin2): Linear(in_features=10, out_features=1, bias=True)\n",
    "  (activation): RReLU(lower=0.01, upper=0.1)\n",
    ")\n",
    "\n",
    "model = GRUFNN(\n",
    "    input_size=input_size,\n",
    "    hidden_size0=hidden_size, \n",
    "    hidden_size1=hidden_size//2, \n",
    "    output_size=1, \n",
    "    num_layers=2,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Best Test Results (24H)  `(best 0.814835)`\n",
    "```python\n",
    "val_ratio = 0.25\n",
    "tst_ratio = 0.25\n",
    "hidden_size = 100\n",
    "epochs = 15\n",
    "seq_len = 90\n",
    "batch_size = 128\n",
    "lr = 1-e2\n",
    "seed = 1607355910\n",
    "num_layer=2\n",
    "\n",
    "GRUFNN(\n",
    "  (gru): GRU(16, 100, num_layers=2, batch_first=True)\n",
    "  (lin1): Linear(in_features=100, out_features=50, bias=True)\n",
    "  (lin2): Linear(in_features=50, out_features=1, bias=True)\n",
    "  (activation): RReLU(lower=0.01, upper=0.1)\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk-forward Evalations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plots import plot_df\n",
    "\n",
    "joint_df = pd.DataFrame({\n",
    "    **{f\"x_{i}\": input_data[:,i] for i in range(input_data.shape[1])},\n",
    "    **{f\"y_{i}\": target_data[:,i] for i in range(target_data.shape[1])},\n",
    "}, index=t_range)\n",
    "\n",
    "input_cols = [col for col in joint_df.columns if col.startswith('x')]\n",
    "target_cols = [col for col in joint_df.columns if col.startswith('y')]\n",
    "\n",
    "plot_df(joint_df[target_cols]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.indexers import WalkForwardIndexer\n",
    "\n",
    "sub_tst_set_size = 90\n",
    "sub_set_size = 2*365 + sub_tst_set_size\n",
    "\n",
    "\n",
    "indexer = WalkForwardIndexer(total_set_size=len(joint_df),\n",
    "                             test_set_size=sub_tst_set_size,\n",
    "                             sub_set_size=sub_set_size)\n",
    "\n",
    "# vaify that the index breaks sets into correct amounts\n",
    "plot_list = []\n",
    "set_no = 0\n",
    "for i, j in indexer:\n",
    "\n",
    "    set_no += 1\n",
    "    subset = joint_df[target_cols].iloc[i:j]\n",
    "    plot_list.append(\n",
    "        go.Scatter(name=f\"set_{set_no}\", y=subset.to_numpy().ravel()+3*set_no,x=subset.index,opacity=.5))\n",
    "    \n",
    "    sub_df = joint_df.iloc[i:j]\n",
    "    sub_loaders = SequenceDataLoaders( # setup data loader 2: hyper opt\n",
    "        input_data=sub_df[input_cols].values,\n",
    "        target_data=sub_df[target_cols].values,\n",
    "        t_range=sub_df.index,\n",
    "        batch_size=conf.batch_size,\n",
    "        seq_len=conf.seq_len,\n",
    "        shuffle=conf.shuffle,\n",
    "        num_workers=0,\n",
    "        val_ratio=0.2,\n",
    "        tst_size=30*3,\n",
    "        overlap_sequences=overlap_sequences,\n",
    "    )\n",
    "    \n",
    "go.Figure(data=plot_list).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_forward_train_evaluate():\n",
    "    loaders = None\n",
    "    logging.info(f\"Walk-forward training and evalualtion with: {pformat(best_parameters)}\")\n",
    "    \n",
    "    conf.seed = int(time())\n",
    "    set_system_seed(conf.seed) # should be reset with each model instatiation\n",
    "    \n",
    "    # TODO single walk-forward hyper opt to set better best_parameters\n",
    "\n",
    "    conf.lr = best_parameters['lr']\n",
    "    conf.weight_decay = best_parameters['weight_decay']\n",
    "    hidden_size = best_parameters['hidden_size']\n",
    "    num_layers = best_parameters['num_layers']\n",
    "    conf.seq_len = best_parameters['seq_len']\n",
    "\n",
    "    model = GRUFNN(\n",
    "        input_size=input_size,\n",
    "        hidden_size0=hidden_size, \n",
    "        hidden_size1=hidden_size//2, \n",
    "        output_size=output_size,\n",
    "        num_layers=num_layers,\n",
    "    ).to(conf.device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimiser = torch.optim.AdamW(params=model.parameters(),lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "    conf.early_stopping = True\n",
    "    conf.patience = 30\n",
    "    conf.min_epochs = 1\n",
    "    conf.max_epochs = 10_000\n",
    "    \n",
    "    set_system_seed(int(time())) # should be reset with each model instatiation   \n",
    "\n",
    "    set_no = 0\n",
    "    set_metrics = list()\n",
    "    for i, j in indexer:  \n",
    "#         if set_no > 10: \n",
    "#             logging.warning(\"stopping walk forward eearly in dev mode\")\n",
    "#             break\n",
    "        \n",
    "        \n",
    "        subset = joint_df[target_cols].iloc[i:j]\n",
    "        plot_list.append(\n",
    "            go.Scatter(name=f\"set_{set_no}\", y=subset.to_numpy().ravel()+3*set_no,x=subset.index,opacity=.5))\n",
    "\n",
    "        sub_df = joint_df.iloc[i:j]\n",
    "        sub_loaders = SequenceDataLoaders( # setup data loader 2: hyper opt\n",
    "            input_data=sub_df[input_cols].values,\n",
    "            target_data=sub_df[target_cols].values,\n",
    "            t_range=sub_df.index,\n",
    "            batch_size=conf.batch_size,\n",
    "            seq_len=conf.seq_len,\n",
    "            shuffle=conf.shuffle,\n",
    "            num_workers=0,\n",
    "            val_ratio=0.2,\n",
    "            tst_size=30*3,\n",
    "            overlap_sequences=overlap_sequences,\n",
    "        )\n",
    "\n",
    "        # training\n",
    "        trn_epoch_losses, val_epoch_losses, stopped_early = train_model(\n",
    "            model=model,\n",
    "            optimiser=optimiser,\n",
    "            loaders=sub_loaders,\n",
    "            train_epoch_fn=train_epoch_for_sequence_model,\n",
    "            loss_fn=criterion,\n",
    "            conf=conf,\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        # load the saved best validation model\n",
    "        # Load latest or best validation model\n",
    "        conf.checkpoint = \"best_val\"\n",
    "        log.info(f\"Loading model from checkpoint ({conf.checkpoint}) for evaluation\")\n",
    "        log.info(f\"Loading model from {conf.model_path}\")\n",
    "        model_state_dict = torch.load(f\"{conf.model_path}model_{conf.checkpoint}.pth\",\n",
    "                                      map_location=conf.device.type)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "\n",
    "        # evaluation\n",
    "        tst_y_true, tst_y_score = evaluate_sequence_model(model, sub_loaders.test_loader, conf)\n",
    "\n",
    "        # Todo save evaluation predictions and stitch together later on\n",
    "        \n",
    "        set_metrics.append({\n",
    "            \"set_no\": set_no,\n",
    "            \"start\": sub_loaders.test_loader.dataset.t_range[0+conf.seq_len],\n",
    "            \"stop\": sub_loaders.test_loader.dataset.t_range[-1],\n",
    "            **forecast_metrics(y_true=tst_y_true, y_score=tst_y_score),\n",
    "        })\n",
    "        \n",
    "        set_no += 1\n",
    "        \n",
    "        \n",
    "    return set_metrics\n",
    "\n",
    "walk_forward_metrics = pd.DataFrame(walk_forward_train_evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(walk_forward_metrics)\n",
    "walk_forward_metrics[['MASE']].boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_forward_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(walk_forward_metrics.MASE,alpa=0.1)\n",
    "plt.show()\n",
    "\n",
    "walk_forward_metrics.MASE.plot(kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io import clipboard\n",
    "clipboard.copy(f'''\n",
    "<html>\n",
    "<center> <h4>(seed: {conf.seed}, epochs: {conf.max_epochs}, num_layers: {num_layers}, hidden_size: {hidden_size}, lr: {conf.lr}, wd: {conf.weight_decay})</h4> </center>   \n",
    "{tst_metrics.to_html()}\n",
    "</html>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<html>\n",
    "<center> <h4>(seed: 1610315186, epochs: 15, num_layers: 4, hidden_size: 128, lr: 0.0010328916706433232, wd: 9.688212788780198e-07)</h4> </center>   \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.841351</td>\n",
    "      <td>0.111381</td>\n",
    "      <td>0.019770</td>\n",
    "      <td>0.140605</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.326)</td>\n",
    "      <td>0.849066</td>\n",
    "      <td>0.112589</td>\n",
    "      <td>0.020810</td>\n",
    "      <td>0.144255</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.037868</td>\n",
    "      <td>0.137467</td>\n",
    "      <td>0.028475</td>\n",
    "      <td>0.168745</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<center> <h4>(seed: 1609928999, epochs: 27, num_layers: 1, hidden_size: 64, lr: 0.0014512428545394168, wd: 1.1569041471744064e-05)</h4> </center>   \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.852752</td>\n",
    "      <td>0.119475</td>\n",
    "      <td>0.022752</td>\n",
    "      <td>0.150838</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.332)</td>\n",
    "      <td>0.865785</td>\n",
    "      <td>0.121262</td>\n",
    "      <td>0.023223</td>\n",
    "      <td>0.152391</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.058839</td>\n",
    "      <td>0.148276</td>\n",
    "      <td>0.033438</td>\n",
    "      <td>0.182861</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n",
    "\n",
    "\n",
    "<html>\n",
    "<center> <h4>(seed: 1609928653, epochs: 15, num_layers: 1, hidden_size: 64, lr: 0.0014512428545394168, wd: 1.1569041471744064e-05)</h4> </center>   \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.839292</td>\n",
    "      <td>0.117585</td>\n",
    "      <td>0.021612</td>\n",
    "      <td>0.147011</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.332)</td>\n",
    "      <td>0.865785</td>\n",
    "      <td>0.121262</td>\n",
    "      <td>0.023223</td>\n",
    "      <td>0.152391</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.058839</td>\n",
    "      <td>0.148276</td>\n",
    "      <td>0.033438</td>\n",
    "      <td>0.182861</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100 nodes @ 2 layers\n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 232323)</h4> </center>   \n",
    "    \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>1.036609</td>\n",
    "      <td>0.032579</td>\n",
    "      <td>0.001614</td>\n",
    "      <td>0.040180</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "</html>\n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 1607355910, epochs: 15)</h4> </center>   \n",
    "   \n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.892761</td>\n",
    "      <td>0.028061</td>\n",
    "      <td>0.001242</td>\n",
    "      <td>0.035237</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "    \n",
    "</html>\n",
    "    \n",
    "    \n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 1609862642, epochs: 25)</h4> </center>   \n",
    " \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.996154</td>\n",
    "      <td>0.031305</td>\n",
    "      <td>0.001518</td>\n",
    "      <td>0.038966</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 nodes @ 2 layers\n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 1609863699, epochs: 60)</h4> </center>   \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.930785</td>\n",
    "      <td>0.029265</td>\n",
    "      <td>0.001306</td>\n",
    "      <td>0.036135</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n",
    "\n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 1607355910, epochs: 44)</h4> </center>   \n",
    " \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.868942</td>\n",
    "      <td>0.027309</td>\n",
    "      <td>0.001180</td>\n",
    "      <td>0.034354</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n",
    "\n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 1609863699, epochs: 30)</h4> </center>   \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.918288</td>\n",
    "      <td>0.028870</td>\n",
    "      <td>0.001323</td>\n",
    "      <td>0.036370</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 nodes @ 1 layer\n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 1607355910, epochs: 106)</h4> </center>   \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.916288</td>\n",
    "      <td>0.028808</td>\n",
    "      <td>0.001348</td>\n",
    "      <td>0.036717</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n",
    "\n",
    "\n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 1607355910, epochs: 17)</h4> </center>   \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.952805</td>\n",
    "      <td>0.029954</td>\n",
    "      <td>0.001456</td>\n",
    "      <td>0.038160</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n",
    "\n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 1609864074, epochs: 50)</h4> </center>   \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.917947</td>\n",
    "      <td>0.028868</td>\n",
    "      <td>0.001355</td>\n",
    "      <td>0.036804</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n",
    "\n",
    "\n",
    "### 16 nodes @ 2 layer\n",
    "\n",
    "\n",
    "<html>\n",
    "<center> <h4>Seed Test Metrics (seed: 1607355910, epochs: 110)</h4> </center>   \n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.880144</td>\n",
    "      <td>0.027685</td>\n",
    "      <td>0.001247</td>\n",
    "      <td>0.035309</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Crime Type</th>\n",
    "      <th>Model</th>\n",
    "      <th>MASE</th>\n",
    "      <th>MAE</th>\n",
    "      <th>MSE</th>\n",
    "      <th>RMSE</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>Total</td>\n",
    "      <td>EWM(0.341)</td>\n",
    "      <td>0.911698</td>\n",
    "      <td>0.028654</td>\n",
    "      <td>0.001316</td>\n",
    "      <td>0.036275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>Total</td>\n",
    "      <td>GRUFNN</td>\n",
    "      <td>0.996154</td>\n",
    "      <td>0.031305</td>\n",
    "      <td>0.001518</td>\n",
    "      <td>0.038966</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>Total</td>\n",
    "      <td>HA(7,6)</td>\n",
    "      <td>1.041704</td>\n",
    "      <td>0.032737</td>\n",
    "      <td>0.001807</td>\n",
    "      <td>0.042505</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.memory_summary())\n",
    "else:\n",
    "    raise Exception(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Sequence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series normalisation \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
