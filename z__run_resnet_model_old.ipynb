{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Loader\n",
    "H = rows\n",
    "W = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo split into dataset and dataloader and batch_loader\n",
    "\n",
    "# batch_loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE DATASET AND THE DATALOADER\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Note: when rewriting this code - keep everything in numpy array - only convert to torch or transfer to torch gpu \n",
    "in the actually training loop, only batches should be moved to GPU to save memory usage.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# TODO Convert this into a dataset and let the pytorch loader to the magic\n",
    "# TODO INHERIT FROM DATASET INSTEAD AND DEFINE LEN AND GET ITEM FUNCTIONS\n",
    "class STResNetDataLoader:  # add test and train data and validation set\n",
    "    def __init__(self, S, E, lc=3, lp=3, lq=3, c=1, p=24, q=168, shuffle=False, trn_tst_split=0.8, trn_val_split=0.8,\n",
    "                 overlap_in_out=True, norm='none'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            S: Total Sequence size -> (N,rows,cols)\n",
    "            E: External Info -> (N, n_features)\n",
    "            lc,lp,lq: number of closeness, period and trend frames respectively\n",
    "            c,p,q: time step of closeness, period and trend frames respectively\n",
    "\n",
    "            shuffle: if true shuffles the training batch after each epoch\n",
    "            trn_tst_split: ratio between train and test data split\n",
    "            trn_val_splt: ratio between train and val data split\n",
    "            overlap_in_out: if false, make sure that none of the train/validation/test sets' input and output data overlaps\n",
    "            norm: [minmax | meanstd | none]\n",
    "\n",
    "        \"\"\"\n",
    "        # DATA\n",
    "        self.S = S\n",
    "        self.E = E\n",
    "\n",
    "        # DATA FORMAT PARAMETERS\n",
    "        self.lc = lc\n",
    "        self.lp = lp\n",
    "        self.lq = lq\n",
    "\n",
    "        self.c = c\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "\n",
    "        # TRAIN/VAL/TEST DATA INDICES\n",
    "        self.max_t = len(self.S)\n",
    "        self.min_t = self.lq * self.q  # might not be the min in case p >> q\n",
    "\n",
    "        self.max_t_val = int(\n",
    "            np.floor((len(self.S) - self.min_t) * trn_tst_split) + self.min_t)  # max values for validation set\n",
    "        self.max_t_trn = int(\n",
    "            np.floor((self.max_t_val - self.min_t) * trn_val_split) + self.min_t)  # max values for train set\n",
    "\n",
    "        self.trn_times = np.arange(self.min_t, self.max_t_trn, dtype=int)\n",
    "        self.trn_val_times = np.arange(self.min_t, self.max_t_val, dtype=int)  # train and validation together\n",
    "\n",
    "        if overlap_in_out:\n",
    "            gap = 0\n",
    "        else:\n",
    "            gap = self.min_t\n",
    "\n",
    "        self.val_times = np.arange(self.max_t_trn + gap, self.max_t_val, dtype=int)\n",
    "        self.tst_times = np.arange(self.max_t_val + gap, self.max_t, dtype=int)\n",
    "\n",
    "        # TRAIN MEAN,STD,MIN,MAX FOR SCALING\n",
    "        trn_data = S[self.trn_times]\n",
    "        self.trn_mean = trn_data.mean()\n",
    "        self.trn_std = trn_data.std()\n",
    "        self.trn_min = trn_data.min()\n",
    "        self.trn_max = trn_data.max()\n",
    "\n",
    "        # NORMALIZE THE DATA\n",
    "        if norm == 'minmax':\n",
    "            self.S = self.minmax_norm(S)\n",
    "\n",
    "        if norm == 'meanstd':\n",
    "            self.S = self.mean_std_norm(S)\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.trn_times)\n",
    "\n",
    "        self.current_t = 0\n",
    "\n",
    "        # TODO: Add t_range so that we get a concept of time\n",
    "        print('Total data: ', len(S))\n",
    "        print('Usable data: ', len(self.tst_times) + len(self.trn_times) + len(self.val_times))\n",
    "        print('Train data: ', len(self.trn_times))\n",
    "        print('Validation data: ', len(self.val_times))\n",
    "        print('Test data: ', len(self.tst_times))\n",
    "        print()\n",
    "\n",
    "    def get_train_data_length(self):\n",
    "        return len(self.trn_times)\n",
    "\n",
    "    def __call__(self, batch_size):\n",
    "        \"\"\"\n",
    "        Returns a random batch from the training data\n",
    "        return Dbc,Dbp,Dbq,Dbt,times\n",
    "        \"\"\"\n",
    "        return self.get_train_batch(batch_size)\n",
    "\n",
    "    def get_data(self, times):\n",
    "        \"\"\"\n",
    "        returns data sequences given a set of Xt times\n",
    "        return Dbc,Dbp,Dbq,Dbe,Dbt,times\n",
    "        \"\"\"\n",
    "        Dbc = []  # batch of Xc (closeness)\n",
    "        Dbp = []  # batch of Xp (period)\n",
    "        Dbq = []  # batch of Xq (trend)\n",
    "        Dbt = []  # batch of Xt (current time slot)\n",
    "        Dbe = []  # batch of Et (external factors of current time slot)\n",
    "\n",
    "        for t in times:\n",
    "            Sc = self.S[t - self.c * self.lc:t:self.c]\n",
    "            Sp = self.S[t - self.p * self.lp:t:self.p]\n",
    "            Sq = self.S[t - self.q * self.lq:t:self.q]\n",
    "            Dbc.append(Sc)\n",
    "            Dbp.append(Sp)\n",
    "            Dbq.append(Sq)\n",
    "\n",
    "            Xt = self.S[t:t + 1]\n",
    "            Dbt.append(Xt)\n",
    "            # if self.E is not None:\n",
    "            Et = self.E[t:t + 1]\n",
    "            Dbe.append(Et)\n",
    "\n",
    "        Dbc = np.stack(Dbc)\n",
    "        Dbp = np.stack(Dbp)\n",
    "        Dbq = np.stack(Dbq)\n",
    "        Dbe = np.stack(Dbe)\n",
    "        Dbt = np.stack(Dbt)\n",
    "\n",
    "        return Dbc, Dbp, Dbq, Dbe, Dbt, times\n",
    "\n",
    "        # if self.E != None:\n",
    "        #     Dbe = np.stack(Dbe)\n",
    "        #     return Dbc,Dbp,Dbq,Dbe,Dbt\n",
    "        # else:\n",
    "        #     return Dbc,Dbp,Dbq,Dbt\n",
    "\n",
    "    def get_train_batch(self, batch_size):  # TODO fix offset\n",
    "        \"\"\"\n",
    "        Returns a random batch from the train set\n",
    "        format: Dbc,Dbp,Dbq,Dbe,Dbt,times\n",
    "\n",
    "        \"\"\"\n",
    "        # times = np.random.choice(range(self.min_t,self.max_t_trn),size=batch_size,replace=False)\n",
    "        times = self.trn_times[self.current_t:self.current_t + batch_size]\n",
    "        if len(times) < 1:\n",
    "            if self.shuffle:\n",
    "                np.random.shuffle(self.trn_times)  # Shuffle after every epoch\n",
    "            self.current_t = 0\n",
    "            times = self.trn_times[self.current_t:self.current_t + batch_size]\n",
    "        else:\n",
    "            self.current_t = self.current_t + batch_size\n",
    "\n",
    "        return self.get_data(times)\n",
    "\n",
    "    def get_test_set(self):\n",
    "        \"\"\"\n",
    "        returns all data in test set\n",
    "        format: Dbc,Dbp,Dbq,Dbe,Dbt,times\n",
    "        \"\"\"\n",
    "        #         times = np.arange(self.max_t_val,self.max_t)\n",
    "        times = self.tst_times\n",
    "\n",
    "        return self.get_data(times)\n",
    "\n",
    "    def get_train_set(self):\n",
    "        \"\"\"\n",
    "        returns all data in train set\n",
    "        format: Dbc,Dbp,Dbq,Dbe,Dbt,times\n",
    "        \"\"\"\n",
    "        #         times = np.arange(self.min_t,self.max_t_trn)\n",
    "        times = self.trn_times\n",
    "\n",
    "        return self.get_data(times)\n",
    "\n",
    "    # TODO: Implement cross validation\n",
    "    def get_validation_set(self):\n",
    "        \"\"\"\n",
    "        returns all data in train set\n",
    "        format: Dbc,Dbp,Dbq,Dbe,Dbt and times\n",
    "        \"\"\"\n",
    "        #         times = np.arange(self.max_t_trn,self.max_t_val)\n",
    "        times = self.val_times\n",
    "\n",
    "        return self.get_data(times)\n",
    "\n",
    "    def reset_current_t(self):\n",
    "        self.current_t = 0\n",
    "\n",
    "    def minmax_norm(self, data):\n",
    "        #       return (data-self.trn_min)/(self.trn_max-self.trn_min)\n",
    "        return (data - self.trn_min) / (self.trn_max - self.trn_min)\n",
    "\n",
    "    def mean_std_norm(self, data):\n",
    "        return (data - self.trn_mean) / self.trn_std\n",
    "\n",
    "    def minmax_norm_r(self, data):\n",
    "        \"\"\"\n",
    "        reverse minmax_norm transformation\n",
    "        \"\"\"\n",
    "        #       return data*(self.trn_max-self.trn_min) + self.trn_min\n",
    "        return data * (self.trn_max - self.trn_min) + self.trn_min\n",
    "\n",
    "    def mean_std_norm_r(self, data):\n",
    "        \"\"\"\n",
    "        reverse meanstd_norm transformation\n",
    "        \"\"\"\n",
    "        return data * self.trn_std + self.trn_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop weather fo now and just check what you can get\n",
    "# or cap everything to the length of the weather\n",
    "# or cap\n",
    "from datasets.grid_dataset import GridDataGroup\n",
    "from datasets.flat_dataset import FlatDataGroup\n",
    "from utils.plots import im\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from utils.configs import BaseConf\n",
    "import matplotlib.pyplot as plt\n",
    "from models.baseline_models import ExponentialMovingAverage, UniformMovingAverage,\\\n",
    "                                   TriangularMovingAverage, HistoricAverage\n",
    "from utils.metrics import CellPlotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dim_str = \"T24H-X850M-Y880M\"\n",
    "data_dim_str = \"T1H-X1700M-Y1760M\"\n",
    "\n",
    "data_path = f\"./data/processed/{data_dim_str}/\"\n",
    "\n",
    "\n",
    "conf_dict = {\n",
    "    \"seed\": 3,\n",
    "    \"resume\": False,\n",
    "    \"early_stopping\": False,\n",
    "    \"use_cuda\": False,\n",
    "    \"val_ratio\": 0.1,\n",
    "    \"tst_ratio\": 0.2,\n",
    "    \"sub_sample_train_set\": 1,\n",
    "    \"sub_sample_validation_set\": 1,\n",
    "    \"sub_sample_validation_set\": 0,\n",
    "    \"flatten_grid\": True,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-8,\n",
    "    \"max_epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout\": 0,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 6,\n",
    "    \"seq_len\": 0\n",
    "}\n",
    "\n",
    "\n",
    "conf = BaseConf(conf_dict=conf_dict)\n",
    "\n",
    "data_group = GridDataGroup(data_path=data_path, conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = data_group.testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mock_data import generate_mock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = test_set.crimes[:,0]\n",
    "E = test_set.time_vectors\n",
    "\n",
    "# S: Total Sequence size -> (N,rows,cols)\n",
    "# E: External Info -> (N, n_features)\n",
    "n_feats = 10\n",
    "N,H,W = 3000, 20, 20\n",
    "S = np.ones((N,H,W))*np.arange(N).reshape((N,1,1))\n",
    "E = np.ones((N,n_feats))*np.arange(N).reshape((N,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data:  3000\n",
      "Usable data:  2496\n",
      "Train data:  1596\n",
      "Validation data:  400\n",
      "Test data:  500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loader = STResNetDataLoader(S, E, lc=3, lp=3, lq=3, c=1, p=24, q=168, shuffle=False, trn_tst_split=0.8, trn_val_split=0.8,\n",
    "                 overlap_in_out=True, norm='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\"\"\"\n",
    "General notes on module:\n",
    "========================\n",
    "### STRes-Net (add link to paper)\n",
    "* **Input Data Format:** (N,C,H,W) where C a.k.a the channels is the previous time steps leading up to t\n",
    "* **Input Data Type:** Continuous value (number of crimes per cell)\n",
    "* **Output Data Format:** (N,C,H,W) \n",
    "* **Output Data Type:** Continuous value (number of crimes per cell)\n",
    "* **Loss Function:** RMSE\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ResUnit(nn.Module):\n",
    "    def __init__(self, n_channels=1):\n",
    "        super(ResUnit, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=n_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(n_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(n_channels)\n",
    "\n",
    "    # using pytorch default weight inits (Xavier Init) we should probably use nn.init.kaiming_normal_\n",
    "    #         self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.conv2.bias.data.fill_(0)\n",
    "        self.conv1.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.conv2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.bn1(x)\n",
    "        o = self.relu(o)\n",
    "        o = self.conv1(o)\n",
    "        o = self.bn2(o)\n",
    "        o = self.relu(o)\n",
    "        o = self.conv2(o)\n",
    "        o = x + o\n",
    "\n",
    "        return o\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, n_layers=1, in_channels=1, n_channels=1):\n",
    "        \"\"\"\n",
    "        n_layers: number of ResUnits\n",
    "        in_channels: number of channels at conv1 input \n",
    "        n_channels: number of channels at conv1 output and res-units inputs\n",
    "        conv2 take n_channels and outputs 1 channel\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=n_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=n_channels, out_channels=1, kernel_size=3, stride=1, padding=1)\n",
    "        self.resUnits = nn.Sequential()\n",
    "        for i in range(n_layers):\n",
    "            self.resUnits.add_module(name='ResUnit' + str(i), module=ResUnit(n_channels))\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.conv2.bias.data.fill_(0)\n",
    "        self.conv1.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.conv2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o = self.conv1(x)\n",
    "        o = self.resUnits(o)\n",
    "        o = self.conv2(o)\n",
    "\n",
    "        return o\n",
    "\n",
    "\n",
    "class ExternalNet(nn.Module):  # need to add \n",
    "    def __init__(self, in_features, y_size, x_size):\n",
    "        super(ExternalNet, self).__init__()\n",
    "\n",
    "        self.y_size = y_size\n",
    "        self.x_size = x_size\n",
    "        self.out_features = y_size * x_size\n",
    "        self.fc1 = nn.Linear(in_features, self.out_features)\n",
    "        self.fc2 = nn.Linear(self.out_features, self.out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.fc1.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc1.bias.data.fill_(0)\n",
    "        self.fc2.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, a):\n",
    "        Xext = self.fc2(self.relu(self.fc1(a)))\n",
    "        return Xext.view(-1, 1, self.y_size, self.x_size)\n",
    "\n",
    "\n",
    "class Fuse(nn.Module):  # fuse the 3 matrices with parametric matrices\n",
    "    def __init__(self, y_size, x_size):\n",
    "        super(Fuse, self).__init__()\n",
    "\n",
    "        self.Wc = nn.parameter.Parameter(torch.zeros(y_size, x_size))\n",
    "        self.Wp = nn.parameter.Parameter(torch.zeros(y_size, x_size))\n",
    "        self.Wq = nn.parameter.Parameter(torch.zeros(y_size, x_size))\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.Wc.data.uniform_(-0.5, 0.5)\n",
    "        self.Wp.data.uniform_(-0.5, 0.5)\n",
    "        self.Wq.data.uniform_(-0.5, 0.5)\n",
    "\n",
    "    def forward(self, Xc, Xp, Xq):\n",
    "        Xres = self.Wc * Xc + self.Wp * Xp + self.Wq * Xq\n",
    "        return Xres\n",
    "\n",
    "\n",
    "class STResNet(nn.Module):\n",
    "    def __init__(self, n_layers, y_size, x_size, lc=1, lp=1, lq=1, n_channels=1, n_ext_features=10):\n",
    "        \"\"\"\n",
    "        n_layers: number of layers\n",
    "        y_size: grids.shape[-2]\n",
    "        x_size: grids.shape[-1]\n",
    "        ext_features: number of external features, dimensions of E\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: check if pytorch has parallel modules like sequential\n",
    "        # TODO: See if we can set parallel networks by a param: not just lc,lp,lq, but even more\n",
    "        # TODO: Add option with no external data\n",
    "\n",
    "        super(STResNet, self).__init__()\n",
    "        self.resNetc = ResNet(n_layers, in_channels=lc, n_channels=n_channels)\n",
    "        self.resNetp = ResNet(n_layers, in_channels=lp, n_channels=n_channels)\n",
    "        self.resNetq = ResNet(n_layers, in_channels=lq, n_channels=n_channels)\n",
    "        self.extNet = ExternalNet(in_features=n_ext_features, y_size=y_size, x_size=x_size)\n",
    "        self.fuse = Fuse(y_size=y_size, x_size=x_size)\n",
    "\n",
    "    def forward(self, Sc, Sp, Sq, Et=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        =======\n",
    "        Sc, Sp , Sq: Sequence of grids - each grid as a channel\n",
    "        Et: External features at time t\n",
    "\n",
    "        Outputs:\n",
    "        ========\n",
    "        Xt_hat: Estimated crime grid at time t\n",
    "        \"\"\"\n",
    "        # l indicates the output of the lth ResUnit\n",
    "        Xc = self.resNetc(Sc)\n",
    "        Xp = self.resNetp(Sp)\n",
    "        Xq = self.resNetq(Sq)\n",
    "        Xres = self.fuse(Xc, Xp, Xq)\n",
    "        \n",
    "       \n",
    "\n",
    "        # sigmoid squeezes values between 0 and 1 that's, that's why the cum-sum wasn't working\n",
    "        #         Last layer is sigmoid not tanh\n",
    "        #         our values are all positive no real reason to use tan like the used in deep-st\n",
    "        if Et is None:\n",
    "            Xt_hat = torch.sigmoid(Xres)\n",
    "        else: \n",
    "            Xext = self.extNet(Et)        \n",
    "            Xt_hat = torch.sigmoid(Xres + Xext)\n",
    "\n",
    "        return Xt_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExternalNet(nn.Module):  # need to add \n",
    "    def __init__(self, in_features, y_size, x_size):\n",
    "        super(ExternalNet, self).__init__()\n",
    "\n",
    "        self.y_size = y_size\n",
    "        self.x_size = x_size\n",
    "        self.out_features = y_size * x_size\n",
    "        self.fc1 = nn.Linear(in_features, self.out_features)\n",
    "        self.fc2 = nn.Linear(self.out_features, self.out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.fc1.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc1.bias.data.fill_(0)\n",
    "        self.fc2.weight.data.uniform_(-0.5, 0.5)\n",
    "        self.fc2.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, a):\n",
    "        Xext = self.fc2(self.relu(self.fc1(a)))\n",
    "        return Xext.view(-1, 1, self.y_size, self.x_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([61, 1, 13, 11])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 61\n",
    "shape = in_features, y_size, x_size = 23, 13, 11\n",
    "model = ExternalNet(*shape)\n",
    "input_data = torch.Tensor(np.ones((n,in_features)))\n",
    "output_data = model(input_data)\n",
    "output_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo create batch loader that will load until it's run through all possibilities\n",
    "# get data group separator - test, val, train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "Start Time: 2019-09-25 21:52:08.800555\n",
      "========================================\n",
      "Total data:  3000\n",
      "Usable data:  2496\n",
      "Train data:  2021\n",
      "Validation data:  225\n",
      "Test data:  250\n",
      "\n",
      "Steps per epoch 63\n",
      "\n",
      "Epoch: 0\n",
      "Iteration: 0\n",
      "Iteration: 50\n",
      "Last Train Loss of Epoch: 0.5688894391059875\n",
      "Last Validation Loss of Epoch: 0.7182467579841614\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "\n",
      "Epoch: 1\n",
      "Iteration: 100\n",
      "Last Train Loss of Epoch: 0.5889267325401306\n",
      "Last Validation Loss of Epoch: 0.71470046043396\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "\n",
      "Epoch: 2\n",
      "Iteration: 150\n",
      "Last Train Loss of Epoch: 0.584270715713501\n",
      "Last Validation Loss of Epoch: 0.71470046043396\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "\n",
      "Epoch: 3\n",
      "Iteration: 200\n",
      "Iteration: 250\n",
      "Last Train Loss of Epoch: 0.5695790648460388\n",
      "Last Validation Loss of Epoch: 0.7129207253456116\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "\n",
      "Epoch: 4\n",
      "Iteration: 300\n",
      "Last Train Loss of Epoch: 0.5808013677597046\n",
      "Last Validation Loss of Epoch: 0.7129207253456116\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "\n",
      "Epoch: 5\n",
      "Iteration: 350\n",
      "Last Train Loss of Epoch: 0.5815930366516113\n",
      "Last Validation Loss of Epoch: 0.7129207253456116\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "\n",
      "Epoch: 6\n",
      "Iteration: 400\n",
      "Last Train Loss of Epoch: 0.5789939761161804\n",
      "Last Validation Loss of Epoch: 0.7129207253456116\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "\n",
      "Epoch: 7\n",
      "Iteration: 450\n",
      "Iteration: 500\n",
      "Last Train Loss of Epoch: 0.5798574090003967\n",
      "Last Validation Loss of Epoch: 0.7129207253456116\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "\n",
      "Epoch: 8\n",
      "Iteration: 550\n",
      "Last Train Loss of Epoch: 0.5860311985015869\n",
      "Last Validation Loss of Epoch: 0.7129207253456116\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "\n",
      "Epoch: 9\n",
      "Iteration: 600\n",
      "Last Train Loss of Epoch: 0.5830740332603455\n",
      "Last Validation Loss of Epoch: 0.7129207253456116\n",
      "Current Best Validation Loss: 0.6890348792076111\n",
      "========================================\n",
      "BEST VAL LOSS @ iteration: 0\n",
      "Stop Time: 2019-09-25 21:53:00.535713\n",
      "========================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tst_collection = []\n",
    "trn_collection = []\n",
    "val_collection = []\n",
    "best_model_val_collection = []\n",
    "best_model_trn_collection = []\n",
    "\n",
    "options = [0]\n",
    "for option in options:\n",
    "    print(\"========================================\")\n",
    "    print('Start Time:', pd.datetime.now())\n",
    "    print(\"========================================\")\n",
    "\n",
    "#     datalimit = 5000\n",
    "#     cap_grids = grids.copy()\n",
    "#     cap_grids[cap_grids > cap] = cap\n",
    "\n",
    "#     S = get_S(cap_grids[:datalimit], do_superres, do_cumsum).to(device)\n",
    "#     E = get_E(t_range[:datalimit]).to(device)\n",
    "    \n",
    "    n_feats = 10\n",
    "    N,H,W = 3000, 20, 20\n",
    "    lc, lp, lq= 3, 3, 3\n",
    "    c, p, q = 1, 24, 168\n",
    "    n_layers = 2\n",
    "    n_channels = 3\n",
    "    \n",
    "    S = np.ones((N,H,W))*np.arange(N).reshape((N,1,1))\n",
    "    E = np.ones((N,n_feats))*np.arange(N).reshape((N,1))\n",
    "\n",
    "    loader = STResNetDataLoader(S, E, lc=lc, lp=lp, lq=lq, c=c, p=p, q=q, shuffle=True, trn_tst_split=0.9, trn_val_split=0.9,\n",
    "                        overlap_in_out=True, norm='minmax')\n",
    "    loader.reset_current_t()\n",
    "\n",
    "    torch.manual_seed(901)  # manual seed to test other params of the model and keep init weights constant\n",
    "\n",
    "    lr = 0.005  # 1e-3 # make learn rate smaller if tends to average and epochs more\n",
    "    model = STResNet(n_layers=n_layers, y_size=S.shape[-2], x_size=S.shape[-1], lc=lc, lp=lp, lq=lq,\n",
    "                     n_channels=n_channels, n_ext_features=E.shape[-1])\n",
    "    #   model.load_state_dict(best_model_val) # if you want to continue wherer you left off\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr, weight_decay=0.000)  # Adam is used in literature \n",
    "\n",
    "    criterion = nn.MSELoss()  # for RMSE -> loss = torch.sqrt(criterion(*))\n",
    "\n",
    "    n_epochs = 10\n",
    "    batch_size = 32  # bigger the batch less noise / bad luck batches in the SGD \n",
    "\n",
    "    n_steps = loader.get_train_data_length() // batch_size\n",
    "    print(\"Steps per epoch\", n_steps)\n",
    "\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    min_val_loss = np.inf  # used for early stopping\n",
    "\n",
    "    # TODO: Fix validation loss calculation\n",
    "    # val batch size sub because of memory space on GPU cannot be 300 or we need to do val_loss in a loop\n",
    "    valSc, valSp, valSq, valEt, valXt, valTimes = loader.get_validation_set()  # TODO: Implement cross validation\n",
    "    valSc, valSp, valSq, valEt, valXt, valTimes = valSc[:batch_size], valSp[:batch_size], valSq[:batch_size], valEt[\n",
    "                                                                                                              :batch_size], valXt[\n",
    "                                                                                                                            :batch_size], valTimes[\n",
    "                                                                                                                                          :batch_size]\n",
    "    \n",
    "    valSc, valSp, valSq, valEt, valXt = torch.Tensor(valSc), torch.Tensor(valSp), torch.Tensor(valSq), torch.Tensor(valEt), torch.Tensor(valXt)\n",
    "    \n",
    "    tstSc, tstSp, tstSq, tstEt, tstXt, tstTimes = loader.get_test_set()\n",
    "    tstSc, tstSp, tstSq, tstEt, tstXt, tstTimes = tstSc[:batch_size], tstSp[:batch_size], tstSq[:batch_size], tstEt[\n",
    "                                                                                                              :batch_size], tstXt[\n",
    "                                                                                                                            :batch_size], tstTimes[\n",
    "                                                                                                                                         :batch_size]\n",
    "    \n",
    "    tstSc, tstSp, tstSq, tstEt, tstXt = torch.Tensor(tstSc), torch.Tensor(tstSp), torch.Tensor(tstSq), torch.Tensor(tstEt), torch.Tensor(tstXt)\n",
    "    \n",
    "    itr = 0  # iteration\n",
    "    val_loss_increased = False\n",
    "    val_loss_step_limit = 3000  # used to check if validation loss is not increasing\n",
    "    best_val_at_itr = 0\n",
    "\n",
    "    for epoch in range(n_epochs):  # epoch is one iteration over the entire input data\n",
    "        print(\"\\nEpoch:\", epoch)\n",
    "        for step in range(n_steps):\n",
    "            # Train Evaluation and Optimization Step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            Sc, Sp, Sq, Et, Xt, times = loader.get_train_batch(batch_size)\n",
    "            \n",
    "            Sc, Sp, Sq, Et, Xt = torch.Tensor(Sc), torch.Tensor(Sp), torch.Tensor(Sq), torch.Tensor(Et), torch.Tensor(Xt)\n",
    "\n",
    "            Xt_hat = model.forward(Sc, Sp, Sq, Et)\n",
    "\n",
    "            trnLoss = torch.sqrt(criterion(Xt_hat, Xt))\n",
    "\n",
    "            trnLoss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            train_losses.append(trnLoss.cpu().item())\n",
    "\n",
    "            # Validation Evaluatiofn\n",
    "            model.eval()\n",
    "            valXt_hat = model.forward(valSc, valSp, valSq, valEt)\n",
    "\n",
    "            valLoss = torch.sqrt(criterion(valXt_hat, valXt))\n",
    "\n",
    "            validation_losses.append(valLoss.cpu().item())\n",
    "\n",
    "            if validation_losses[-1] < min_val_loss:\n",
    "                min_val_loss = validation_losses[-1]\n",
    "                best_model_val = model.cpu().state_dict().copy()\n",
    "                best_val_at_itr = itr\n",
    "                model.to(device)\n",
    "\n",
    "            # Test Evaluation  \n",
    "            tstXt_hat = model.forward(tstSc, tstSp, tstSq, tstEt)\n",
    "\n",
    "            tstLoss = torch.sqrt(criterion(tstXt_hat, tstXt))\n",
    "            test_losses.append(tstLoss.cpu().item())\n",
    "\n",
    "            if itr % 50 == 0:\n",
    "                print(\"Iteration:\", itr)\n",
    "\n",
    "            # check if validation loss is increasing\n",
    "            if (itr - best_val_at_itr) > val_loss_step_limit:\n",
    "                print(\"========================================================\")\n",
    "                print(\"VAL LOSS NOT DECREASED IN LAST\", str(itr - best_val_at_itr), \"ITERATIONS\")\n",
    "                print(\"========================================================\")\n",
    "                val_loss_increased = True\n",
    "                break\n",
    "\n",
    "                # learnrate scheduler\n",
    "            #       if itr == 100:\n",
    "            #         optimizer.param_groups[0]['lr'] /= 2\n",
    "\n",
    "            itr += 1\n",
    "\n",
    "        print(\"Last Train Loss of Epoch:\", train_losses[-1])\n",
    "        print(\"Last Validation Loss of Epoch:\", validation_losses[-1])\n",
    "        print(\"Current Best Validation Loss:\", min_val_loss)\n",
    "\n",
    "        if val_loss_increased == True:\n",
    "            break\n",
    "\n",
    "    best_model_trn = model.cpu().state_dict().copy()\n",
    "    model.to(device)\n",
    "    tst_collection.append(np.array(test_losses))\n",
    "    trn_collection.append(np.array(train_losses))\n",
    "    val_collection.append(np.array(validation_losses))\n",
    "    best_model_trn_collection.append(best_model_trn)\n",
    "    best_model_val_collection.append(best_model_val)\n",
    "\n",
    "    print(\"========================================\")\n",
    "    print('BEST VAL LOSS @ iteration:', str(best_val_at_itr))\n",
    "    print('Stop Time:', pd.datetime.now())\n",
    "    print(\"========================================\\n\\n\")\n",
    "\n",
    "plt.figure(figsize=(20, 5))\n",
    "plt.title(\"RMSE Loss of Model over Itterations\")\n",
    "for i, _ in enumerate(trn_collection):\n",
    "    plt.plot(trn_collection[i], label='Training ' + str(options[i]), alpha=0.5)\n",
    "    plt.plot(val_collection[i], label='Validation ' + str(options[i]), alpha=0.5)\n",
    "    plt.plot(tst_collection[i], label='Test ' + str(options[i]), alpha=0.5)\n",
    "\n",
    "    plt.scatter(np.argmin(val_collection[i]), np.min(val_collection[i]))\n",
    "plt.xticks(np.arange(0, itr, n_steps))\n",
    "plt.xlabel(\"Iteraion\")\n",
    "plt.ylabel(\"RMSE Loss\")\n",
    "plt.legend()\n",
    "plt.ylim(0, .1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
