{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# major problem what we have is that the models try to generalise to all cells and do not focus on local temporal features\n",
    "\n",
    "in this notebook we are going to simplify our problem by only looking at the individual cells. This allows us to ocmpare different techniques like the moving averages and actual hawkes processes (point processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd /content/gdrive/My\\ Drive/masters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ./data/processed\n",
    "\n",
    "# T1H-X1700M-Y1760M/  T24H-X850M-Y880M/  T3H-X850M-Y880M/\n",
    "# T12H-X850M-Y880M/  T24H-X425M-Y440M/   T24H-X85M-Y110M/   T6H-X850M-Y880M/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "from time import strftime\n",
    "from copy import deepcopy\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from utils.data_processing import *\n",
    "from logger.logger import setup_logging\n",
    "from utils.configs import BaseConf\n",
    "from utils.utils import write_json, Timer, get_data_sub_paths, pshape\n",
    "from models.kangkang_fnn_models import KangFeedForwardNetwork\n",
    "from dataloaders.flat_loader import FlatDataLoaders\n",
    "from datasets.flat_dataset import FlatDataGroup\n",
    "from utils.metrics import PRCurvePlotter, ROCCurvePlotter, LossPlotter\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "from models.model_result import ModelResult\n",
    "from utils.mock_data import mock_rnn_data_classification, mock_rnn_data_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.plots import im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-25T21:55:42 | root | INFO | =====================================BEGIN=====================================\n",
      "2019-10-25T21:55:42 | root | INFO | Device: cpu\n"
     ]
    }
   ],
   "source": [
    "data_dim_str = \"T24H-X850M-Y880M\"  #\"T1H-X1700M-Y1760M\"  # needs to exist\n",
    "data_dim_str = data_dim_str + \"_2013-01-01_2015-01-01\"\n",
    "conf = BaseConf()\n",
    "\n",
    "conf.model_name = \"GRU\"  # needs to be created\n",
    "data_path = f\"./data/processed/{data_dim_str}/\"\n",
    "conf.model_path = f\"{data_path}models/{model_name}/\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "# logging config is set globally thus we only need to call this in this file\n",
    "# imported function logs will follow the configuration\n",
    "setup_logging(save_dir=model_path, log_config='./logger/standard_logger_config.json', default_level=log.INFO)\n",
    "log.info(\"=====================================BEGIN=====================================\")\n",
    "\n",
    "\n",
    "info = deepcopy(conf.__dict__)\n",
    "info[\"start_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# DATA LOADER SETUP\n",
    "np.random.seed(conf.seed)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(conf.seed)\n",
    "else:\n",
    "    torch.manual_seed(conf.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "log.info(f\"Device: {device}\")\n",
    "info[\"device\"] = device.type\n",
    "conf.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-25T21:55:42 | root | INFO | Data shapes of files in generated_data.npz\n",
      "2019-10-25T21:55:42 | root | INFO | \tcrime_feature_indices shape (10,)\n",
      "2019-10-25T21:55:43 | root | INFO | \tcrime_types_grids shape (730, 10, 47, 33)\n",
      "2019-10-25T21:55:43 | root | INFO | \tcrime_grids shape (730, 1, 47, 33)\n",
      "2019-10-25T21:55:43 | root | INFO | \ttract_count_grids shape (730, 1, 47, 33)\n",
      "2019-10-25T21:55:43 | root | INFO | \tdemog_grid shape (1, 37, 47, 33)\n",
      "2019-10-25T21:55:43 | root | INFO | \tstreet_grid shape (1, 512, 47, 33)\n",
      "2019-10-25T21:55:43 | root | INFO | \ttime_vectors shape (731, 52)\n",
      "2019-10-25T21:55:43 | root | INFO | \tweather_vectors shape (365, 11)\n",
      "2019-10-25T21:55:43 | root | INFO | \tx_range shape (33,)\n",
      "2019-10-25T21:55:43 | root | INFO | \ty_range shape (47,)\n",
      "2019-10-25T21:55:43 | root | INFO | \tt_range shape (731,)\n"
     ]
    }
   ],
   "source": [
    "# GET DATA\n",
    "data_group = FlatDataGroup(data_path=data_path, conf=conf)\n",
    "loaders = FlatDataLoaders(data_group=data_group, conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "model = RNN_crime(vocab_size, embed_size, hidden_size, num_layers)\n",
    "# model.cuda()\n",
    "\n",
    "# # Loss and Optimizer\n",
    "# if is_weighted_loss:\n",
    "#     criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# else:\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Truncated Backpropagation \n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] \n",
    "\n",
    "print(\"Model Training Starting...\")\n",
    "\n",
    "# Training and validation\n",
    "for epoch in range(num_epochs): # TODO: Change to epoch length \n",
    "    for start in range(1): #TODO: change 1 to seq length # this is done to ensure more available training data\n",
    "\n",
    "        for i in range(start, train.size(1) - seq_length, seq_length):\n",
    "            #Train model\n",
    "            model.train()\n",
    "            \n",
    "            # Get batch inputs and targets\n",
    "            # inputs = Variable(train[:, i:i+seq_length]).cuda()\n",
    "            # targets = Variable(train[:, (i+1):(i+1)+seq_length].contiguous()).cuda()\n",
    "            inputs = Variable(train[:, i:i+seq_length])\n",
    "            targets = Variable(train[:, (i+1):(i+1)+seq_length].contiguous())\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            model.zero_grad()\n",
    "\n",
    "            # hidden states not reinitialised - same h_state is propogated forward\n",
    "            # hidden states must be detached from history because the weights have been updated\n",
    "            states = detach(states) \n",
    "\n",
    "            outputs, states = model(inputs, states) \n",
    "            loss = criterion(outputs, targets.view(-1))\n",
    "            loss.backward() #no retain graph - work with set sequence lenght\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            print('{{\"metric\": \"train loss\", \"value\": {}}}'.format(loss.data[0]))\n",
    "\n",
    "            step = (i // seq_length) + 1\n",
    "            if step % 1 == 0:\n",
    "                print ('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n",
    "                       (epoch+1, num_epochs, step, num_batches, loss.data[0], np.exp(loss.data[0])))\n",
    "\n",
    "            # Validate model\n",
    "            # Calculate the evaluation metrics\n",
    "            model.eval()\n",
    "\n",
    "            val_outputs, _ = model(val_inputs, val_states)\n",
    "            val_loss = criterion(val_outputs, val_targets.view(-1))\n",
    "\n",
    "            # Save Ideal Model on Validation Set (Early Stopping)\n",
    "            if (val_loss.data[0] < min_val_los):\n",
    "                min_val_los = val_loss.data[0] \n",
    "                ideal_model_state = model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating mock data for the models\n",
    "### Cumsum example from tutorial [5 Examples of Simple Sequence Prediction Problems for LSTMs](https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MockLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-4f76c1db443e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mclass_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMockLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0mvalidation_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMockLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMockLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MockLoader' is not defined"
     ]
    }
   ],
   "source": [
    "class MockLoaders:\n",
    "    def __init__(self, train_loader=None, validation_loader=None, test_loader=None):\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        self.test_loader = test_loader\n",
    "    \n",
    "\n",
    "class MockRNNLoader:\n",
    "    def __init__(self, seq_len, batch_size, n_samples):\n",
    "        \n",
    "        X,t = mock_rnn_data(seq_len=seq_len, batch_size=n_samples) #(seq_len, batch_size, n_features)                \n",
    "        \n",
    "        indices = np.expand_dims(np.arange(n_samples), axis=0)\n",
    "        indices = np.expand_dims(indices, axis=2)\n",
    "        \n",
    "        vectors = [indices]\n",
    "        i = 0\n",
    "        for j in vector_size:\n",
    "            vectors.append(X[:,:,i:i+j])\n",
    "            i += j\n",
    "        vectors.append(y)    \n",
    "        \n",
    "        self.n_samples = n_samples\n",
    "        self.n_feats = n_feats\n",
    "        self.vectors = vectors\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = int(np.ceil(self.n_samples / self.batch_size))\n",
    "        self.current_batch = 0\n",
    "        \n",
    "        \n",
    "        self.max_index = n_samples\n",
    "        self.min_index = 0\n",
    "        self.dataset = self # just to act as an interface\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_batch = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch >= self.num_batches:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.current_batch += 1\n",
    "            start_index = (self.current_batch - 1) * self.batch_size\n",
    "            stop_index = self.current_batch * self.batch_size\n",
    "            if stop_index > len(self):\n",
    "                stop_index = len(self)\n",
    "            return self[start_index:stop_index]\n",
    "        \n",
    "            \n",
    "    def __getitem__(self, index):        \n",
    "        return tuple(map(lambda x: x[:,index], self.vectors))\n",
    "    \n",
    "vector_size= [3,6,13]#[37,65,512]\n",
    "batch_size = 100    \n",
    "class_split=0.5\n",
    "train_loader = MockLoader(vector_size, batch_size, n_samples=7000, class_split=class_split)\n",
    "validation_loader = MockLoader(vector_size, batch_size, n_samples=1000, class_split=class_split)\n",
    "test_loader = MockLoader(vector_size, batch_size, n_samples=2000, class_split=class_split)\n",
    "loaders = MockLoaders(train_loader,validation_loader,test_loader)\n",
    "\n",
    "X,t = mock_rnn_data() #(seq_len, n_features, batch_size)\n",
    "X,t  = np.swapaxes(X,1,2),np.swapaxes(t,1,2) # (seq_len, batch, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUMLPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU then a MLP (flattens data output to easier use in NLLLoss)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRUMLPClassifier, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)  # note batch first\n",
    "        self.lin1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        if h0 is not None:\n",
    "            out, hn = self.gru(x, h0)\n",
    "        else:\n",
    "            out, hn = self.gru(x)  # hidden state start is zero\n",
    "        out = self.lin1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        # Reshape output to (batch_size*seq_len, hidden_size)\n",
    "#         out = out.contiguous().view(out.size(0) * out.size(1), out.size(2))\n",
    "\n",
    "        return out  # if we never send h its never detached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 20, 1) (10, 20, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAItCAYAAAB2LXNJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAeiElEQVR4nO3de5CdZ30f8O/PkizJlu+2MNjmkkBJCUkhdUko0w63EkMykLZpC21SmqbjptO0kKGTQjtTOsk/7bTNpTOZtCpQciEkKYGWyRDATcLQzAQnMjjcTIJLuBg7NsZ3C9vS7q9/7CpVhPacY/m8R/usP5+ZM9LqPPvT8+579uyz3+d5n7e6OwAAq3DWme4AAPD4YeABAKyMgQcAsDIGHgDAyhh4AAArs/tMdwAAeHS+80Xn9lfvWlvJ/3XDxx/+QHdfs6x6Bh4AMJiv3rWW3/vAk1fyf+164mcvXWY9Uy0AwMpIPABgMJ1kPetnuhunReIBAKyMxAMAhtNZa4kHAMBMEg8AGMzGGo8xb/Iq8QAAVkbiAQADclULAMAcEg8AGEyns9bWeAAAzCTxAIABuaoFAGAOAw8AYGVMtQDAYDrJmqkWAIDZJB4AMCCLSwEA5pB4AMBgOrGBGADAPBIPABjQmLeIk3gAACsk8QCAwXTaPh4AAPNIPABgNJ2sjRl4SDwAgNWReADAYDquagEAmEviAQDDqaylznQnTovEAwBYGQMPAGBlTLUAwGA6ybrLaQEAZpN4AMCALC4FAJhD4gEAg+lIPAAA5pJ4AMCA1lviAQAwk8QDAAZjjQcAwAIkHgAwmE5lbdDsYMxeAwBDkngAwIBc1QIAMIfEAwAG46oWAIAFGHgAACtjqgUAhlNZ6zGzgzF7DQAMSeIBAIPpJOuDZgdj9hoAGJLEAwAG5HJaAIA5JB4AMJhuV7UAAMwl8QCAAa1b4wEAMJvEAwAGs3GTuDGzgzF7DQAMSeIBAMNxVQsAwFwSDwAYjHu1AAAswMADAFgZUy0AMKC1toEYAMBMEg8AGEynbCAGADCPxAMABrRuAzEAgNkkHgAwGDeJAwBYgMQDAAbTKft4AADMI/EAgAG5SRwAwBwSDwAYTHeyZh8PAIDZJB4AMJzKerbPVS1V9fkk9ydZS3Ksu6/eqq2BBwCwDC/q7jvnNTLVAgCsjMQDAAbTWeni0kur6vAJHx/q7kOn6NIHq6qT/NdTPP+nDDwAgFnunLVmY9MLuvvWqjqY5Lqq+kx3f/hUDQ08AGBA2+kmcd196+afd1TVe5I8L8kpBx7bp9cAwHCq6tyqOu/435O8LMknt2ov8QCAwXQq69vnJnFPSPKeqko2xhW/1N3v36qxgQcAcNq6+3NJ/sKi7Q08AGBA22mNx6MxZq8BgCFJPABgMJ1k3U3iAABmk3gAwHAqa9voJnGPhsQDAFgZiQcADMYaDwCABUg8AGBA1ngAAMwh8QCAwXSXNR4AAPMYeAAAK2OqBQAGtGaqBQBgNokHAAymk6y7nBYAYDaJBwAMp6zxAACYR+IBAIPZuEmcNR4AADNJPABgQGuDZgdj9hoAGJLEAwAG0ylrPAAA5pF4AMCA1gfNDsbsNQAwJIkHAAymO1mzxgMAYDYDDwBgZUy1AMCAXE4LADCHxAMABrOxgdiY2cGYvQYAhiTxAIABrcUaDwCAmSQeADCYjqtaAADmkngAwHBc1QIAMJfEAwAGtO6qFgCA2SQeADCY7mTNVS0AALNJPABgQK5qAQCYw8ADAFgZUy0AMJhO2TIdAGAeiQcADMgGYgAAc0g8AGAwnVjjAQAwj8QDAAZkAzEAgDkkHgAwmraPBwDAXBIPABhMxz4eAABzSTwAYEDWeAAAzCHxAIDB2LkUAGABBh4AwMqYagGAAZlqAQCYQ+IBAIPp2DIdAGAuiQcADMiW6QAAc0ySeJy9+5zev+eC5RdeX19+zeN27Zqm7rG1aerWhCPdXdOMR/uhhyepW7unC+766NFpCp+zb5Ky9cixSeomSc6a6DU31ff1ek9TN0n3RLWnqjuh2j3Re+eePdPUTdIPPbT0mg/1g3mkH15dBNHjXtUyyTv2/j0X5Pnf8ANLr1tHlv9iOW79wgOT1D3rznsnqZuzp/umXL/g3Enq9qdunqTuroOXTlI3SY59+dZJ6tY3P3uSumd94fZJ6iZJ7ds7Sd0+cmSauhMNdJOkH5lmQNpHH5mkbpLJflnZdeFFk9Ttqy6fpG6S9Cf/aOk1P3LsA0uvuVNZ4wEAg7FlOgDAAiQeADAgiQcAwBwSDwAYjJ1LAQAWIPEAgAH1Tk48quqaqvrDqrq5qt44dacAgJ1p7sCjqnYl+ZkkL0/yrCSvqapnTd0xAGDnWWSq5XlJbu7uzyVJVf1yklcl+fSUHQMAtraTbxJ3RZIvnfDxLZv/9mdU1bVVdbiqDj+yNs0WyADA2BZJPE41pPq6Oxl196Ekh5Lkgv1PHO9ORwAwiB74JnGLJB63JLnqhI+vTDLNnbMAgOFU1a6q+lhV/fq8toskHr+f5BlV9bQkX07y6iR/9zH2EQB4DLbZ5bSvS3JTkvPnNZybeHT3sSQ/nOQDm0V/tbs/9Vh7CACMr6quTPJdSd6ySPuFNhDr7vcled9j6BcAsDQr3TL90qo6fMLHhzbXdR73U0l+NMl5ixSzcykAMMud3X31qZ6oqu9Ockd331BVL1ykmIEHAAxom6zxeEGSV1bVK5LsS3J+Vf1id3/fVp/gJnEAwGnp7jd195Xd/dRsXHzyW7MGHYnEAwCG0xl3H49JBh7Hztmdu59zydLrnvfFh5Ze87gjT9w7Sd2zLztnkrr7vnD3JHWTZG3/nknq7nnSEyapm7X1aeom2X3Fkyape8e3LLQG61E7eOd9k9SdUu3fP0ndY8+8an6j07TrD26epO5Z3/DkSeomSR2Z5v3z4acfnKTu7vsenqRukpx1zvLfl+sBEwjd/aEkH5rXTuIBAKPpjd1LR2SIBgCsjMQDAAa0k+9OCwCwFAYeAMDKmGoBgMF0ts0GYo+axAMAWBmJBwAMZ6U3iVsqiQcAsDISDwAYkA3EAADmkHgAwIBc1QIAMIfEAwAG0y3xAACYS+IBAAOyjwcAwBwSDwAYkH08AADmkHgAwIBc1QIAMIeBBwCwMpNMtaztTe75c8sf0xy4ZbpY6eHzphmDPXTB2ZPUXdt36SR1k2TXI+uT1H3ouU+cpO55H7ttkrpTuv9p09S95MbzpimcpI6uTVL3waefP0ndKZ171jTvF0cvn+787fnyNOfv3qfunaTuJR9/eJK6SZKz9yy/Zq122qNTploAAOaxuBQABjTo1bQSDwBgdSQeADAaN4kDAJhP4gEAIxp0kYfEAwBYmbmJR1VdleTnk1yeZD3Joe7+6ak7BgBsbdQ1HotMtRxL8obu/mhVnZfkhqq6rrs/PXHfAIAdZu7Ao7tvS3Lb5t/vr6qbklyRxMADAM6Qfjys8aiqpyZ5bpLrT/HctVV1uKoOrx15cDm9AwB2lIWvaqmqA0l+Lcnru/u+k5/v7kNJDiXJviddNeg4DAC2v864azwWSjyqak82Bh3v6O53T9slAGCnWuSqlkry1iQ3dfdPTN8lAGCmTrKDE48XJPn+JC+uqhs3H6+YuF8AwA60yFUtv5NkzGEVALCt2DIdAAb0uLicFgDgsZB4AMCIJB4AALNJPABgODXsBmLTDDzOStb2Lz8DOnL52UuvedzXDk5zAtf3TFI2Z63tmqZwknNvW5+k7t3PmOblds4Xzp+kbpL03mm+zmv7Jimbu79luq/FRZ/4ug2Ll+KhC6f5Gh95wnRvygd+d/8kde/6poleGEn2HTw4Sd2HLp3m63zXs6d7LV92+z3LL1omEBYl8QCAEVnjAQAwm8QDAEbTO/wmcQAAyyDxAIARWeMBADCbxAMAhmSNBwDATBIPABiRNR4AALMZeAAAK2OqBQBGZKoFAGA2iQcAjKaT2DIdAGA2iQcADKit8QAAmE3iAQAjkngAAMwm8QCAEbmqBQBgNokHAAyoBl3jMcnAozPNmpeHLp4uoHn4kmnO4LHz1iapu7Z/1yR1k2T316YZj/ZEXV47d880hZOkpokyp0pI7336NHWT5Jw7zpmk7kMXT/PFePii6d6Va880r7l7vmm6Pl92wzRf5/WJvv3W9k5TN0n6/HOXX/QrJhAWJfEAgNFM9Rv+ChiiAQArI/EAgOGUq1oAAOYx8AAAVsZUCwCMyOJSAIDZFh54VNWuqvpYVf36lB0CABbQK3os2aNJPF6X5KbldwEAeLxYaOBRVVcm+a4kb5m2OwDAQnZ44vFTSX40yfpWDarq2qo6XFWH1x98cCmdAwB2lrkDj6r67iR3dPcNs9p196Huvrq7rz7r3An2wQcANnQ2NhBbxWPJFkk8XpDklVX1+SS/nOTFVfWLS+8JALDjzR14dPebuvvK7n5qklcn+a3u/r7JewYAbKl6NY9ls48HALAyj2rn0u7+UJIPTdITAGBx22Tn0qral+TDSfZmY1zxru5+81btbZkOADwWDyd5cXc/UFV7kvxOVf1Gd3/kVI0NPACA09bdneSBzQ/3bD62zGOs8QAAZrn0+D5dm49rT26weVuVG5PckeS67r5+q2ISDwAY0BRXnGzhzu6+elaD7l5L8pyqujDJe6rq2d39yVO1lXgAAEvR3fdk4yKUa7ZqM03iUUnvXv5Q7MEnTTdO6iuPTFP4a9N8iR+5ZJKySZIHnzRNnx++aJrh+b3fuH+Sukmytm+iuuetTVK3d033PXLv06Z5XTx41ZZ3YnhMapovcZLkkW88OEndfU++f5K6SXLsU+dPUvdrl0/zha61XZPUTZJjFy7/PWPK772t/9Pl7yp6OqrqsiRHu/ueqtqf5KVJ/v1W7U21AACPxROT/FxV7crGTMqvdvevb9XYwAMAOG3d/fEkz120vYEHAIxmolvWr4LFpQDAykg8AGBEEg8AgNkkHgAwoBVuILZUEg8AYGUkHgAwIokHAMBsEg8AGJHEAwBgNokHAAym2lUtAABzSTwAYERdZ7oHp0XiAQCsjMQDAEZkjQcAwGwGHgDAyphqAYABuZwWAGCOaRKPs5L1/csfij184NjSax73/Kd8YZK6H7n5aZPU7fXpLqOa6gqtesqDk9R98J4Dk9RNkqMXTPMrxeVP+eokdXvCy+sevPXgJHV71/o0dS8+OkndJLnrm/ZNUveSA/dMUjdJbn/a+ZPU7XPXJqn70GXTvZbvf8r+pddcu+kM/B4v8QAAmM0aDwAYjS3TAQDmk3gAwIgkHgAAs0k8AGBEEg8AgNkWGnhU1YVV9a6q+kxV3VRVz5+6YwDA1qpX81i2RadafjrJ+7v7e6vq7CTnLL8rAMBON3fgUVXnJ/mrSf5BknT3I0kembZbAMBOtMhUyzck+UqS/15VH6uqt1TVuSc3qqprq+pwVR1ee+CBpXcUABjfIgOP3Um+LcnPdvdzkzyY5I0nN+ruQ919dXdfvevAdPfOAACycVXLKh5LtsjA45Ykt3T39ZsfvysbAxEAgEdl7sCju/8kyZeq6pmb//SSJJ+etFcAwI606FUt/yzJOzavaPlckh+YrksAwEwD3yRuoYFHd9+Y5OqJ+wIA7HC2TAeAEQ2aeNgyHQBYGYkHAIxI4gEAMJvEAwAGUxn3qhaJBwCwMpMkHnv2Hs2VT79j6XXvPrJ/6TWPu+aST0xS97N3XzZJ3b27j01SN0m+8idPmKTunj1rk9Q98uTpvhZ1tCap+9qnfGSSurcdvXCSuknyzouneS33nml+bbv84L2T1E2Sh/ZM8z1y8Jz7J6mbJLc+dZrXxv69Ryepu++yaeomyd1HLl56zbUPLb3kfBIPAIDZrPEAgNEMvHOpxAMAWBmJBwCMSOIBADCbxAMARiTxAACYzcADAFgZUy0AMCCX0wIAzCHxAIARSTwAAGaTeADAaDoSDwCAeSQeADAgV7UAAMwh8QCAEUk8AABmk3gAwICs8QAAmEPiAQAjGjTxmGTgsees9Rw85/6l171435Gl1zzuOftumaTuVeffPUndfbuOTVI3SW4/+wmT1H3Rk2+epO7v7XvyJHWTZM+utUnqXrZ7+d8fSXLXsQOT1E2SPc+8b5K6lx+Y5vv60v0PTFI3Sf7o3Gm+Rw7um67Pz3nylyap+8DRvZPUXevpAvm7rzhv6TV7z6CjgDNA4gEAo7FzKQDAfAYeAMDKmGoBgMHU5mNEEg8AYGUkHgAwIotLAQBmW2jgUVU/UlWfqqpPVtU7q2rf1B0DALZWvZrHss0deFTVFUn+eZKru/vZSXYlefXyuwIA7HSLrvHYnWR/VR1Nck6SW6frEgAw105d49HdX07yH5N8McltSe7t7g+e3K6qrq2qw1V1+OF7vrb8ngIAw1tkquWiJK9K8rQkT0pyblV938ntuvtQd1/d3VfvvXD/8nsKAPx/vaLHki2yuPSlSf64u7/S3UeTvDvJX15+VwCAnW6RNR5fTPIdVXVOkq8leUmSw5P2CgDY2kRXnKzCIms8rk/yriQfTfKJzc85NHG/AIAdaKGrWrr7zUnePHFfAIBF7dTEAwBgWdyrBQAGtGPXeAAALIuBBwBw2qrqqqr67aq6afO+bq+b1d5UCwCMaPtMtRxL8obu/mhVnZfkhqq6rrs/farGEg8A4LR1923d/dHNv9+f5KYkV2zVfpLE49zdD+fbL/rjpdd95t7bll7zuH21Nknd7zn4sUnq/u+7nzVJ3STJVdPca+fyvfdOUvfpF945Sd0keWR91yR1/+aB+yap+zPHLpikbpJ8y+XTfP8dW5/m959vPDDd6+Izu58xSd2zzzo2Sd0k+UsXfmGSuk/fe/skdb909OJJ6ibJLxxZ/m09vrJnmp8hs6xwcemlVXXixqGHuvuU+3lV1VOTPDfJ9VsVM9UCAMxyZ3dfPa9RVR1I8mtJXt/dW/52ZeABAKOZ6AZup6uq9mRj0PGO7n73rLbWeAAAp62qKslbk9zU3T8xr72BBwCMaFm3vZ/3mO8FSb4/yYur6sbNxyu2amyqBQA4bd39O0lq0fYGHgAwmIot0wEA5pJ4AMCIJB4AALNJPABgQNVjRh4SDwBgZSQeADCabbZz6aMh8QAAVsbAAwBYGVMtADAgG4gBAMwh8QCAEUk8AABmk3gAwICs8QAAmEPiAQAjGjTxqJ5gr/eq+kqSLyzY/NIkdy69E9vHTj6+nXxsieMbneMb14jH9pTuvmxV/9m5l17V3/xdP7KS/+v3f/4NN3T31cuqN0ni8Wi++FV1eJkHtN3s5OPbyceWOL7ROb5x7eRjW5q2xgMAYC5rPABgRBKP03boTHdgYjv5+HbysSWOb3SOb1w7+dge9yZZXAoATOfAJVf1s1++msWl179juYtLt0PiAQA8TljjAQAjGnTGYiWJR1VdU1V/WFU3V9UbT/H83qr6lc3nr6+qp66iX8tQVVdV1W9X1U1V9amqet0p2rywqu6tqhs3H//mTPT1dFXV56vqE5t9P3yK56uq/vPm+ft4VX3bmejn6aiqZ55wXm6sqvuq6vUntRnq/FXV26rqjqr65An/dnFVXVdVn93886ItPve1m20+W1WvXV2vF7fF8f2HqvrM5uvvPVV14RafO/O1vB1scXz/tqq+fMJr8BVbfO7M99ozbYtj+5UTjuvzVXXjFp+77c8di5l84FFVu5L8TJKXJ3lWktdU1bNOavaDSe7u7qcn+ckk/37qfi3RsSRv6O4/n+Q7kvzTUxxfkvyf7n7O5uPHVtvFpXjRZt9PNc/38iTP2Hxcm+RnV9qzx6C7//D4eUnyF5McSfKeUzQd6fy9Pck1J/3bG5P8Znc/I8lvbn78Z1TVxUnenOTbkzwvyZu3GqCcYW/P1x/fdUme3d3fmuSPkrxpxufPei1vB2/P1x9fkvzkCa/B95385ILvtWfa23PSsXX33znhe/DXkrx7xudv93PHAlaReDwvyc3d/bnufiTJLyd51UltXpXk5zb//q4kL6mqWkHfHrPuvq27P7r59/uT3JTkijPbq5V7VZKf7w0fSXJhVT3xTHfqNLwkyf/t7kV33d2WuvvDSe466Z9P/B77uSTfc4pP/c4k13X3Xd19dzZ+mJ/qB+AZdarj6+4PdvexzQ8/kuTKlXdsSbY4f4tY5L32jJp1bJvv+X87yTtX2qmBVa/msWyrGHhckeRLJ3x8S77+B/Ofttl887g3ySUr6NtSbU4RPTfJ9ad4+vlV9QdV9RtV9c0r7dhj10k+WFU3VNW1p3h+kXM8gldn6ze9kc9fkjyhu29LNgbLSQ6eos1OOY//MMlvbPHcvNfydvbDm1NJb9siiRr9/P2VJLd392e3eH7kc8cJVrG49FTJxcljqEXabGtVdSAbMeHru/u+k57+aDb28X9gc272f2ZjWmIUL+juW6vqYJLrquozm7+5HLcTzt/ZSV6ZU0f0o5+/Re2E8/ivszH9+Y4tmsx7LW9XP5vkx7NxPn48yX/KxgDrRKOfv9dkdtox6rmbRmess3uCVSQetyS56oSPr0xy61Ztqmp3kgtyelHjGVFVe7Ix6HhHd3/d/GR339fdD2z+/X1J9lTVpSvu5mnr7ls3/7wjG+sfnndSk0XO8Xb38iQf7e7bT35i9PO36fbj01+bf95xijZDn8fNxbDfneTv9RYbFC3wWt6Wuvv27l7r7vUk/y2n7vew52/zff9vJPmVrdqMeu74eqsYePx+kmdU1dM2f6t8dZL3ntTmvUmOr6D/3iS/tdUbx3azOS/51iQ3dfdPbNHm8uNrVqrqedn4un91db08fVV1blWdd/zvSV6W5JMnNXtvkr+/eXXLdyS593isP5Atf9sa+fyd4MTvsdcm+V+naPOBJC+rqos2o/yXbf7btldV1yT5l0le2d1HtmizyGt5WzppzdRfz6n7vch77Xb10iSf6e5bTvXkyOduSrW+mseyTT7V0t3HquqHs/EGtivJ27r7U1X1Y0kOd/d7s/GD+xeq6uZsJB2vnrpfS/SCJN+f5BMnXAb2r5I8OUm6+79kYzD1T6rqWJKvJXn1KAOrJE9I8p7Nn7u7k/xSd7+/qn4o+dPje1+SVyS5ORtXhfzAGerraamqc5L8tST/+IR/O/H4hjp/VfXOJC9McmlV3ZKNK1X+XZJfraofTPLFJH9rs+3VSX6ou/9Rd99VVT+ejR9gSfJj3b3tksctju9NSfZmI4JPko909w9V1ZOSvKW7X5EtXstn4BBm2uL4XlhVz8lGuP75bL5WTzy+rd5rz8AhbOlUx9bdb80p1leNeO5YjC3TAWAwBy6+qr/1pa+f33AJfvd//AtbpgMAY7JlOgAMaIo9NlZB4gEArIzEAwBG03GTOACAeSQeADAgazwAAOaQeADAiCQeAACzGXgAACtjqgUABlOxuBQAYC6JBwCMptsGYgAA80g8AGBA1ngAAMwh8QCAEUk8AABmk3gAwICs8QAAmEPiAQCj6STrY0YeEg8AYGUkHgAwojEDD4kHALA6Eg8AGJCrWgAA5jDwAABWxlQLAIyox5xrkXgAACsj8QCAAVlcCgAwh8QDAEbTsYEYAMA8Eg8AGEwlKVe1AADMJvEAgBGtn+kOnB6JBwCwMhIPABiQNR4AAHNIPABgNPbxAACYT+IBAMNpd6cFAJhH4gEAA3J3WgCAOQw8AICVMdUCACOyuBQAYDaJBwCMppNykzgAgNkkHgAwIms8AIDHm6p6W1XdUVWfXKS9gQcAjKhX9Jjv7UmuWbTbBh4AwGnr7g8nuWvR9tZ4AMCAanVrPC6tqsMnfHyouw+dbjEDDwBglju7++plFTPwAIARuaoFAGA2Aw8AGE0nWV/RY46qemeS303yzKq6pap+cFZ7Uy0AwGnr7tc8mvYGHgAwmEqv8qqWpTLVAgCsjIEHALAyploAYESmWgAAZpN4AMCIJB4AALNJPABgNMc3EBuQxAMAWBmJBwAMyAZiAABzSDwAYEQSDwCA2SQeADCclngAAMwj8QCA0XQkHgAA80g8AGBEdi4FAJjNwAMAWBlTLQAwIFumAwDMIfEAgBFJPAAAZpN4AMBoOsm6xAMAYCaJBwAMx03iAADmkngAwIgkHgAAs0k8AGBEEg8AgNkkHgAwGvt4AADMJ/EAgOF00utnuhOnReIBAKyMgQcAsDKmWgBgRC6nBQCYTeIBAKNxOS0AwHwSDwAYkTUeAACzSTwAYEQSDwCA2SQeADCclngAAMwj8QCA0XSSdTeJAwCYSeIBACOyxgMAYDaJBwCMSOIBADCbgQcAsDKmWgBgOJ2sm2oBAJhJ4gEAo+mk2wZiAAAzSTwAYETWeAAAzCbxAIAR2UAMAGA2iQcAjKY7WXdVCwDATBIPABiRNR4AALNJPABgQG2NBwDAbBIPABhOW+MBADCPgQcAsDKmWgBgNB03iQMAmEfiAQAjapfTAgDMJPEAgMF0krbGAwBgNokHAIym2xoPAIB5JB4AMCBrPAAA5pB4AMCIrPEAAJitetDb6gLA41VVvT/JpSv67+7s7muWVczAAwBYGVMtAMDKGHgAACtj4AEArIyBBwCwMgYeAMDK/D/1fz7mw+uiYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, t = mock_rnn_data(seq_len=10, batch_size=20)\n",
    "print(X.shape, t.shape)\n",
    "y = np.cumsum(X, axis=0)\n",
    "im(t[...,0])\n",
    "im(y[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, False, True)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(torch.ones(10)).requires_grad, torch.Tensor(10).requires_grad,y_pred.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-23T21:49:15 | root | INFO | Data shapes of files in generated_data.npz\n",
      "2019-10-23T21:49:15 | root | INFO | \tcrime_feature_indices shape (11,)\n",
      "2019-10-23T21:49:15 | root | INFO | \tcrime_types_grids shape (365, 11, 47, 33)\n",
      "2019-10-23T21:49:15 | root | INFO | \tcrime_grids shape (365, 1, 47, 33)\n",
      "2019-10-23T21:49:15 | root | INFO | \tdemog_grid shape (1, 37, 47, 33)\n",
      "2019-10-23T21:49:15 | root | INFO | \tstreet_grid shape (1, 512, 47, 33)\n",
      "2019-10-23T21:49:15 | root | INFO | \ttime_vectors shape (366, 52)\n",
      "2019-10-23T21:49:15 | root | INFO | \tweather_vectors shape (365, 11)\n",
      "2019-10-23T21:49:15 | root | INFO | \tx_range shape (33,)\n",
      "2019-10-23T21:49:15 | root | INFO | \ty_range shape (47,)\n",
      "2019-10-23T21:49:15 | root | INFO | \tt_range shape (366,)\n",
      "(64, 3)\n",
      "(1, 64, 37)\n",
      "(10, 64, 65)\n",
      "(1, 64, 512)\n",
      "(10, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# CRIME DATA    \n",
    "conf.seq_len = 10\n",
    "data_group = FlatDataGroup(data_path=data_path, conf=conf)\n",
    "loaders = FlatDataLoaders(data_group=data_group, conf=conf)\n",
    "for indices, spc_feats, tmp_feats, env_feats, targets in loaders.train_loader:\n",
    "    for i in [indices, spc_feats, tmp_feats, env_feats, targets]:\n",
    "        print(np.shape(i))\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape, y.shape (23, 100, 2) (23, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "losses = []\n",
    "\n",
    "n_epochs = 80\n",
    "seq_len=23\n",
    "batch_size=100\n",
    "\n",
    "\n",
    "CLASSIFICATION = False\n",
    "if CLASSIFICATION:\n",
    "    input_size, hidden_size, num_layers = 1, 2, 2\n",
    "    loss_fn = nn.CrossEntropyLoss()  # todo add class weights\n",
    "    X, y = mock_rnn_data(seq_len=seq_len, batch_size=batch_size)\n",
    "else:    \n",
    "    input_size, hidden_size, num_layers = 2, 1, 2\n",
    "    loss_fn = nn.MSELoss()\n",
    "    X, y = mock_adding_problem_data(seq_len=seq_len, batch_size=batch_size)\n",
    "    \n",
    "print(\"X.shape, y.shape\", X.shape, y.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.GRU(input_size, hidden_size, num_layers)\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-2)\n",
    "state_dict = model.state_dict()\n",
    "optimiser_dict = optimiser.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loop - use conf to get device?\n",
    "def train_epoch_for_gru(model, optimiser, batch_loader, loss_fn, total_losses, conf):\n",
    "    \"\"\"\n",
    "    Training the model for a single epoch\n",
    "    \"\"\"\n",
    "    epoch_losses = []\n",
    "    num_batches = batch_loader.num_batches\n",
    "    for indices, data, targets in batch_loader:\n",
    "        \n",
    "        data = torch.Tensor(data).to(conf.device)\n",
    "        y_true = torch.LongTensor(targets).to(conf.device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        y_pred, h = model.forward(data)\n",
    "\n",
    "        if not conf.use_seq_loss: # seq_loss means we look at entire sequence when calculating the loss.\n",
    "            y_pred, y_true = y_pred[-1], y_true[-1]\n",
    "        y_pred, y_true = y_pred.view(-1,2), y_true.view(-1)\n",
    "\n",
    "\n",
    "        loss = loss_fn(input=y_pred, target=y_true)\n",
    "        \n",
    "        current_batch = batch_loader.current_batch\n",
    "\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "        loss = loss_fn(input=out, target=targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "        total_losses.append(epoch_losses[-1])\n",
    "\n",
    "        if model.training:  # not used in validation loops\n",
    "            optimiser.zero_grad()\n",
    "            clip_grad_norm_(model.parameters(), 0.5)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            log.debug(f\"Batch: {current_batch:04d}/{num_batches:04d} \\t Loss: {epoch_losses[-1]:.4f}\")\n",
    "    mean_epoch_loss = np.mean(epoch_losses)\n",
    "    return mean_epoch_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn.GRU(input_size, hidden_size, num_layers)\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-2)    \n",
    "optimiser.load_state_dict(optimiser_dict)\n",
    "model.load_state_dict(state_dict)\n",
    "losses = []\n",
    "losses_seq = []\n",
    "for i in range(n_epochs*2):\n",
    "    \n",
    "    X = torch.Tensor(X).to(conf.device)\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    y_pred, h = model.forward(X)\n",
    "\n",
    "    if CLASSIFICATION:\n",
    "        y_true = torch.LongTensor(y).to(conf.device)\n",
    "        y_pred, y_true = y_pred[-1], y_true[-1]\n",
    "        y_pred, y_true = y_pred.view(-1,2), y_true.view(-1)\n",
    "    else:\n",
    "        y_true = torch.Tensor(y).to(conf.device)\n",
    "        y_pred, y_true = y_pred[-1], y_true[-1]\n",
    "        y_pred, y_true = y_pred.view(-1), y_true.view(-1)\n",
    "     \n",
    "    loss = loss_fn(input=y_pred, target=y_true)\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimiser.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    ######################################################################################\n",
    "    y_pred, h = model.forward(X)\n",
    "    if CLASSIFICATION:\n",
    "        y_true = torch.LongTensor(y).to(conf.device)\n",
    "        y_pred, y_true = y_pred.view(-1,2), y_true.view(-1)\n",
    "    else:\n",
    "        y_true = torch.Tensor(y).to(conf.device)\n",
    "        y_pred, y_true = y_pred.view(-1), y_true.view(-1)\n",
    "    loss_seq = loss_fn(input=y_pred, target=y_true)\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    loss_seq.backward()\n",
    "    clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimiser.step()\n",
    "    \n",
    "    losses_seq.append(loss_seq.item())\n",
    "    \n",
    "plt.plot(losses, label=\"only last\") \n",
    "plt.plot(losses_seq, label=\"entire sequence\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mock_data import mock_rnn_data\n",
    "from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,t = mock_rnn_data(seq_len=30,batch_size=100) #(seq_len, n_features, batch_size)\n",
    "X,t  = np.swapaxes(X,1,2),np.swapaxes(t,1,2) # (seq_len, batch, input_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-19T20:23:20 | root | INFO | test\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RNN-CRIME-MODEL'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.info(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 1, 763)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_group.crimes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dataloaders.flat_loader.FlatDataLoaders at 0x1a203e6d68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.early_stopping = True\n",
    "conf.max_epochs = 20\n",
    "conf.dropout = 0.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "trn_loss = []  # epoch losses\n",
    "val_loss = []\n",
    "val_loss_best = float(\"inf\")\n",
    "\n",
    "all_trn_loss = []  # batch losses\n",
    "all_val_loss = []\n",
    "\n",
    "# SET MODEL PARAMS\n",
    "train_set = loaders.train_loader.dataset\n",
    "indices, spc_feats, tmp_feats, env_feats, target = train_set[train_set.min_index]\n",
    "\n",
    "# input_size = hidden_size = np.sum([spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]])\n",
    "# model = SimpleFNN(input_size, hidden_size)\n",
    "\n",
    "spc_size, tmp_size, env_size = spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]\n",
    "# model = KangFeedForwardNetwork(spc_size=spc_size, tmp_size=tmp_size, env_size=env_size, dropout_p=conf.dropout)\n",
    "model = SimpleKangFNN(spc_size=spc_size, tmp_size=tmp_size, env_size=env_size, dropout_p=conf.dropout)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "\n",
    "parameters = [\n",
    "                {'params': model.spcNet.parameters(), 'lr': 1e-3},    \n",
    "                {'params': model.tmpNet.parameters(), 'lr': 1e-3},    \n",
    "                {'params': model.envNet.parameters(), 'lr': 1e-3},\n",
    "                {'params': model.finalNet.parameters(), 'lr': 1e-3}\n",
    "            ]\n",
    "\n",
    "\n",
    "# important note: using weight decay (l2 penalty) can prohibit long term memory in LSTM networks\n",
    "# - use gradient clipping instead\n",
    "optimiser = optim.Adam(params=parameters, lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "for epoch in range(conf.max_epochs):\n",
    "    log.info(f\"Epoch: {(1+epoch):04d}/{conf.max_epochs:04d}\")\n",
    "    conf.timer.reset()\n",
    "    # Training loop\n",
    "    tmp_trn_loss = []\n",
    "    num_batches = loaders.train_loader.num_batches\n",
    "    for indices, spc_feats, tmp_feats, env_feats, targets in loaders.train_loader:\n",
    "        current_batch = loaders.train_loader.current_batch\n",
    "        \n",
    "        # Transfer to PyTorch Tensor and GPU\n",
    "        spc_feats = torch.Tensor(spc_feats[0]).to(device) # only taking [0] for fnn\n",
    "        tmp_feats = torch.Tensor(tmp_feats[0]).to(device) # only taking [0] for fnn\n",
    "        env_feats = torch.Tensor(env_feats[0]).to(device) # only taking [0] for fnn\n",
    "        targets = torch.LongTensor(targets[0,:,0]).to(device) # only taking [0] for fnn\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "        loss = loss_function(input=out, target=targets)\n",
    "        tmp_trn_loss.append(loss.item())\n",
    "        all_trn_loss.append(tmp_trn_loss[-1])\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        log.debug(f\"Batch: {current_batch:04d}/{num_batches:04d} \\t Loss: {tmp_trn_loss[-1]:.4f}\")\n",
    "\n",
    "    trn_loss.append(np.mean(tmp_trn_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Training Loop Duration: {conf.timer.check()}\")\n",
    "    conf.timer.reset()\n",
    "\n",
    "    # Validation loop\n",
    "    tmp_val_loss = []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # Transfer to GPU - todo put into a loop - then add wrapper\n",
    "        for indices, spc_feats, tmp_feats, env_feats, targets in loaders.validation_loader:\n",
    "            # Transfer to GPU\n",
    "            spc_feats = torch.Tensor(spc_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            tmp_feats = torch.Tensor(tmp_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            env_feats = torch.Tensor(env_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            targets = torch.LongTensor(targets[0,:,0]).to(device)  # only taking [0] for fnn\n",
    "            out = model(spc_feats, tmp_feats, env_feats)\n",
    "\n",
    "            loss = loss_function(input=out, target=targets)\n",
    "            tmp_val_loss.append(loss.item())\n",
    "            all_val_loss.append(tmp_val_loss[-1])\n",
    "\n",
    "    val_loss.append(np.mean(tmp_val_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Validation Loop Duration: {conf.timer.check()}\")\n",
    "    \n",
    "    log.info(f\"\\tlearning rate: \\t{optimiser.param_groups[0]['lr']:.5f}\")\n",
    "    log.info(f\"\\tLoss (Trn): \\t{trn_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Val): \\t{val_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Dif): \\t{np.abs(val_loss[-1]-trn_loss[-1]):.5f}\\n\")        \n",
    "    \n",
    "    # save best model\n",
    "    if min(val_loss) < val_loss_best:\n",
    "        val_loss_best = min(val_loss)\n",
    "        torch.save(model.state_dict(), model_path + \"model_best.pth\")\n",
    "        torch.save(optimiser.state_dict(), model_path + \"optimiser_best.pth\")\n",
    "\n",
    "#     # model has been over-fitting stop maybe? # average of val_loss has increase - starting to over-fit\n",
    "#     if conf.early_stopping and epoch > 5 and np.sum(np.diff(val_loss[-5:])) > 0:  # increasing moving average\n",
    "#         log.warning(\"Early stopping: Over-fitting has taken place\")\n",
    "#         break\n",
    "        \n",
    "#     if conf.early_stopping and epoch > 1 and np.abs(val_loss[-1]-val_loss[-2]) < conf.tolerance:\n",
    "#         log.warning(\"Converged: Difference between the past two validation losses is within tolerance\")\n",
    "#         break\n",
    "\n",
    "    # checkpoint - save models and loss values\n",
    "    torch.save(model.state_dict(), model_path + \"model.pth\")\n",
    "    torch.save(optimiser.state_dict(), model_path + \"optimiser.pth\")\n",
    "    np.savez_compressed(model_path + \"losses.npz\",\n",
    "                        all_val_loss=all_val_loss,\n",
    "                        val_loss=val_loss,\n",
    "                        trn_loss=trn_loss,\n",
    "                        all_trn_loss=all_trn_loss,\n",
    "                        val_loss_best=val_loss_best)\n",
    "    \n",
    "# Save training and validation plots\n",
    "skip = 0\n",
    "loss_plotter = LossPlotter(title=\"Cross Entropy Loss of Linear Regression Model\")\n",
    "loss_plotter.plot_losses(trn_loss, all_trn_loss[skip:], val_loss, all_val_loss[skip:])\n",
    "loss_plotter.savefig(model_path + \"plot_train_val_loss.png\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of RNN training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.early_stopping = True\n",
    "conf.max_epochs = 20\n",
    "conf.dropout = 0.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "trn_loss = []\n",
    "val_loss = []\n",
    "val_loss_best = float(\"inf\")\n",
    "\n",
    "all_trn_loss = []\n",
    "all_val_loss = []\n",
    "\n",
    "# SET MODEL PARAMS\n",
    "train_set = loaders.train_loader.dataset\n",
    "indices, spc_feats, tmp_feats, env_feats, target = train_set[train_set.min_index]\n",
    "\n",
    "# input_size = hidden_size = np.sum([spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]])\n",
    "# model = SimpleFNN(input_size, hidden_size)\n",
    "\n",
    "spc_size, tmp_size, env_size = spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]\n",
    "# model = KangFeedForwardNetwork(spc_size=spc_size, tmp_size=tmp_size, env_size=env_size, dropout_p=conf.dropout)\n",
    "model = SimpleKangFNN(spc_size=spc_size, tmp_size=tmp_size, env_size=env_size, dropout_p=conf.dropout)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "\n",
    "parameters = [\n",
    "                {'params': model.spcNet.parameters(), 'lr': 1e-3},    \n",
    "                {'params': model.tmpNet.parameters(), 'lr': 1e-3},    \n",
    "                {'params': model.envNet.parameters(), 'lr': 1e-3},\n",
    "                {'params': model.finalNet.parameters(), 'lr': 1e-3}\n",
    "            ]\n",
    "\n",
    "\n",
    "# important note: using weight decay (l2 penalty) can prohibit long term memory in LSTM networks\n",
    "# - use gradient clipping instead\n",
    "optimiser = optim.Adam(params=parameters, lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "for epoch in range(conf.max_epochs):\n",
    "    log.info(f\"Epoch: {(1+epoch):04d}/{conf.max_epochs:04d}\")\n",
    "    conf.timer.reset()\n",
    "    # Training loop\n",
    "    tmp_trn_loss = []\n",
    "    num_batches = loaders.train_loader.num_batches\n",
    "    for indices, spc_feats, tmp_feats, env_feats, targets in loaders.train_loader:\n",
    "        current_batch = loaders.train_loader.current_batch\n",
    "        \n",
    "        # Transfer to PyTorch Tensor and GPU\n",
    "        spc_feats = torch.Tensor(spc_feats[0]).to(device) # only taking [0] for fnn\n",
    "        tmp_feats = torch.Tensor(tmp_feats[0]).to(device) # only taking [0] for fnn\n",
    "        env_feats = torch.Tensor(env_feats[0]).to(device) # only taking [0] for fnn\n",
    "        targets = torch.LongTensor(targets[0,:,0]).to(device) # only taking [0] for fnn\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "        loss = loss_function(input=out, target=targets)\n",
    "        tmp_trn_loss.append(loss.item())\n",
    "        all_trn_loss.append(tmp_trn_loss[-1])\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        log.debug(f\"Batch: {current_batch:04d}/{num_batches:04d} \\t Loss: {tmp_trn_loss[-1]:.4f}\")\n",
    "\n",
    "    trn_loss.append(np.mean(tmp_trn_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Training Loop Duration: {conf.timer.check()}\")\n",
    "    conf.timer.reset()\n",
    "\n",
    "    # Validation loop\n",
    "    tmp_val_loss = []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # Transfer to GPU\n",
    "        for indices, spc_feats, tmp_feats, env_feats, targets in loaders.validation_loader:\n",
    "            # Transfer to GPU\n",
    "            spc_feats = torch.Tensor(spc_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            tmp_feats = torch.Tensor(tmp_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            env_feats = torch.Tensor(env_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            targets = torch.LongTensor(targets[0,:,0]).to(device)  # only taking [0] for fnn\n",
    "            out = model(spc_feats, tmp_feats, env_feats)\n",
    "\n",
    "            loss = loss_function(input=out, target=targets)\n",
    "            tmp_val_loss.append(loss.item())\n",
    "            all_val_loss.append(tmp_val_loss[-1])\n",
    "\n",
    "    val_loss.append(np.mean(tmp_val_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Validation Loop Duration: {conf.timer.check()}\")\n",
    "\n",
    "    \n",
    "    log.info(f\"\\tlearning rate: \\t{optimiser.param_groups[0]['lr']:.5f}\")\n",
    "    log.info(f\"\\tLoss (Trn): \\t{trn_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Val): \\t{val_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Dif): \\t{np.abs(val_loss[-1]-trn_loss[-1]):.5f}\\n\")        \n",
    "    \n",
    "\n",
    "    # save best model\n",
    "    if min(val_loss) < val_loss_best:\n",
    "        val_loss_best = min(val_loss)\n",
    "        torch.save(model.state_dict(), model_path + \"model_best.pth\")\n",
    "        torch.save(optimiser.state_dict(), model_path + \"optimiser_best.pth\")\n",
    "\n",
    "#     # model has been over-fitting stop maybe? # average of val_loss has increase - starting to over-fit\n",
    "#     if conf.early_stopping and epoch > 5 and np.sum(np.diff(val_loss[-5:])) > 0:  # increasing moving average\n",
    "#         log.warning(\"Early stopping: Over-fitting has taken place\")\n",
    "#         break\n",
    "        \n",
    "#     if conf.early_stopping and epoch > 1 and np.abs(val_loss[-1]-val_loss[-2]) < conf.tolerance:\n",
    "#         log.warning(\"Converged: Difference between the past two validation losses is within tolerance\")\n",
    "#         break\n",
    "    \n",
    "    if epoch > 3:\n",
    "        model.dropout.p = ((conf.max_epochs - epoch)/conf.max_epochs)**2\n",
    "    \n",
    "    \n",
    "    # checkpoint - save models and loss values\n",
    "    torch.save(model.state_dict(), model_path + \"model.pth\")\n",
    "    torch.save(optimiser.state_dict(), model_path + \"optimiser.pth\")\n",
    "    np.savez_compressed(model_path + \"losses.npz\",\n",
    "                        all_val_loss=all_val_loss,\n",
    "                        val_loss=val_loss,\n",
    "                        trn_loss=trn_loss,\n",
    "                        all_trn_loss=all_trn_loss,\n",
    "                        val_loss_best=val_loss_best)\n",
    "    \n",
    "# Save training and validation plots\n",
    "skip = 0\n",
    "loss_plotter = LossPlotter(title=\"Cross Entropy Loss of Linear Regression Model\")\n",
    "loss_plotter.plot_losses(trn_loss, all_trn_loss[skip:], val_loss, all_val_loss[skip:])\n",
    "loss_plotter.savefig(model_path + \"plot_train_val_loss.png\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some part of the code was referenced from below.\n",
    "# https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model\n",
    "# which was in part referenced from below.\n",
    "# https://github.com/pytorch/examples/tree/master/word_language_model \n",
    "# TODO: Add parsing capabilities\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "# Hyper Parameters\n",
    "embed_size = 100\n",
    "hidden_size = 1000\n",
    "num_layers = 1\n",
    "num_epochs = 1\n",
    "batch_size = 20\n",
    "seq_length = 20 # month history\n",
    "learning_rate = 0.002\n",
    "is_weighted_loss = False\n",
    "\n",
    "# Load Time Series\n",
    "data = np.load(\"./data/time_series.npy\")[:1000] # just select the centre square\n",
    "data = threshold(data) # treshold data to 0 and 1 values\n",
    "\n",
    "# Calculate the CrossEntropy weights for skew data (Rosenberg 2012)\n",
    "values, counts = np.unique(data,return_counts=True)\n",
    "weights = counts/np.sum(counts)\n",
    "# weights = Variable(torch.FloatTensor([weights[1], weights[0]])).cuda()\n",
    "weights = Variable(torch.FloatTensor([weights[1], weights[0]]))\n",
    "\n",
    "vocab_size = int(data.max() + 1) # only two types - crime or no crime\n",
    "\n",
    "n_trn_val_samps = int(len(data)*0.8)# 80:20 split data to train and test set\n",
    "n_trn_samps = int(n_trn_val_samps*0.8)# 80:20 split train to train and validation set\n",
    "train_and_validation = data[:n_trn_val_samps]\n",
    "train = train_and_validation[:n_trn_samps]\n",
    "validation = train_and_validation[n_trn_samps:]\n",
    "test = data[n_trn_val_samps:]\n",
    "\n",
    "# Setup train data\n",
    "train = batchify(train,batch_size) # segment the data into batches\n",
    "train = torch.LongTensor(train) #expects long tensor for the embedding\n",
    "\n",
    "# Setup validation data and starting hidden state\n",
    "validation = torch.LongTensor(validation)\n",
    "validation = validation.view(1,-1) # reshape validation => 1 batch , many samples\n",
    "# val_states = (Variable(torch.zeros(num_layers, 1, hidden_size)).cuda(), \n",
    "#                 Variable(torch.zeros(num_layers, 1, hidden_size)).cuda())\n",
    "val_states = (Variable(torch.zeros(num_layers, 1, hidden_size)), \n",
    "                Variable(torch.zeros(num_layers, 1, hidden_size)))\n",
    "# val_inputs = Variable(validation[:,:-1]).cuda()\n",
    "# val_targets = Variable(validation[:,1:]).cuda()\n",
    "val_inputs = Variable(validation[:,:-1])\n",
    "val_targets = Variable(validation[:,1:])\n",
    "min_val_los = 10000 #used to get early stopping parameters\n",
    "ideal_model_state = {}\n",
    "\n",
    "# Setup test data and starting hidden state\n",
    "test = torch.LongTensor(test)\n",
    "test = test.view(1,-1) # reshape test => 1 batch , many samples\n",
    "# tst_states = (Variable(torch.zeros(num_layers, 1, hidden_size)).cuda(), \n",
    "#                 Variable(torch.zeros(num_layers, 1, hidden_size)).cuda()) #batch_size is one in testing case\n",
    "tst_states = (Variable(torch.zeros(num_layers, 1, hidden_size)), \n",
    "                Variable(torch.zeros(num_layers, 1, hidden_size))) #batch_s\n",
    "# tst_inputs = Variable(test[:,:-1]).cuda()\n",
    "# tst_targets = Variable(test[:,1:]).cuda()\n",
    "tst_inputs = Variable(test[:,:-1])\n",
    "tst_targets = Variable(test[:,1:])\n",
    "\n",
    "num_batches = train.shape[1] // seq_length\n",
    "\n",
    "print(\"Hyper Parameters:\\n================\")\n",
    "print(\"embed_size:\\t\",embed_size)\n",
    "print(\"hidden_size:\\t\",hidden_size)\n",
    "print(\"num_layers:\\t\",num_layers)\n",
    "print(\"num_epochs:\\t\",num_epochs)\n",
    "print(\"batch_size:\\t\",batch_size)\n",
    "print(\"seq_length:\\t\",seq_length)\n",
    "print(\"learning_rate:\\t\",learning_rate)\n",
    "print(\"num_batches:\\t\",num_batches)\n",
    "print(\"is_weighted_loss:\\t\",is_weighted_loss)\n",
    "\n",
    "# RNN Based Crime Model\n",
    "class RNN_crime(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNN_crime, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) # note batch first\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, x, h): # give seq_jump to know which hidden state to use for next sequence\n",
    "        # Embed word crimes to vectors\n",
    "        x = self.embed(x) \n",
    "\n",
    "        # Forward propagate RNN  \n",
    "        out, h = self.lstm(x, h) # Not saving hidden state because seq_jump # init lstm weight to 1 to make easier\n",
    "\n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.contiguous().view(out.size(0)*out.size(1), out.size(2))\n",
    "\n",
    "        # Decode hidden states of all time step\n",
    "        out = self.linear(out)  \n",
    "        return out, h # if we never send h its never detached\n",
    "    \n",
    "    \n",
    "model = RNN_crime(vocab_size, embed_size, hidden_size, num_layers)\n",
    "# model.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "if is_weighted_loss:\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Truncated Backpropagation \n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] \n",
    "\n",
    "print(\"Model Training Starting...\")\n",
    "\n",
    "# Training and validation\n",
    "for epoch in range(num_epochs): # TODO: Change to epoch length \n",
    "    for start in range(1): #TODO: change 1 to seq length # this is done to ensure more available training data\n",
    "    \n",
    "        # Initial hidden and memory states\n",
    "        states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)),\n",
    "                  Variable(torch.zeros(num_layers, batch_size, hidden_size)))\n",
    "    \n",
    "        for i in range(start, train.size(1) - seq_length, seq_length):\n",
    "            #Train model\n",
    "            model.train()\n",
    "            \n",
    "            # Get batch inputs and targets\n",
    "            # inputs = Variable(train[:, i:i+seq_length]).cuda()\n",
    "            # targets = Variable(train[:, (i+1):(i+1)+seq_length].contiguous()).cuda()\n",
    "            inputs = Variable(train[:, i:i+seq_length])\n",
    "            targets = Variable(train[:, (i+1):(i+1)+seq_length].contiguous())\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            model.zero_grad()\n",
    "\n",
    "            # hidden states not reinitialised - same h_state is propogated forward\n",
    "            # hidden states must be detached from history because the weights have been updated\n",
    "            states = detach(states) \n",
    "\n",
    "            outputs, states = model(inputs, states) \n",
    "            loss = criterion(outputs, targets.view(-1))\n",
    "            loss.backward() #no retain graph - work with set sequence lenght\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            print('{{\"metric\": \"train loss\", \"value\": {}}}'.format(loss.data[0]))\n",
    "\n",
    "            step = (i // seq_length) + 1\n",
    "            if step % 1 == 0:\n",
    "                print ('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n",
    "                       (epoch+1, num_epochs, step, num_batches, loss.data[0], np.exp(loss.data[0])))\n",
    "\n",
    "            # Validate model\n",
    "            # Calculate the evaluation metrics\n",
    "            model.eval()\n",
    "\n",
    "            val_outputs, _ = model(val_inputs, val_states)\n",
    "            val_loss = criterion(val_outputs, val_targets.view(-1))\n",
    "\n",
    "            # Save Ideal Model on Validation Set (Early Stopping)\n",
    "            if (val_loss.data[0] < min_val_los):\n",
    "                min_val_los = val_loss.data[0] \n",
    "                ideal_model_state = model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_collection = []\n",
    "val_collection = []\n",
    "best_model_val_collection = []\n",
    "best_model_trn_collection = []\n",
    "\n",
    "itrs = [5]\n",
    "\n",
    "for itr in itrs:\n",
    "    torch.manual_seed(itr)  # manual seed to test other params of the model and keep init weights constant\n",
    "\n",
    "    print('Start Time:', pd.datetime.now(), '\\n')\n",
    "\n",
    "    # data loader setup\n",
    "    n_epochs = 10\n",
    "    seq_len = 24 * 3\n",
    "    batch_size = 32\n",
    "    lr = 1e-3\n",
    "\n",
    "    S = Variable(torch.FloatTensor(subX)).view(-1, subX.shape[-1] ** 2)\n",
    "    #   S = Variable(torch.FloatTensor(grids)).view(-1,grid_size**2)\n",
    "    #   S = Variable(torch.FloatTensor(f[-500:]))\n",
    "    S[S > cap] = cap\n",
    "    S = S.cuda()\n",
    "    #   S = S.mm(trans_mat_d2s)\n",
    "\n",
    "    loader = DataLoader(S, batch_size, seq_len, shuffle=True)\n",
    "    trnT, trnX, trnTimes = loader.getTrainBatch()\n",
    "    loader.reset_current_t()\n",
    "    trnT = trnT[:, :, 12:12 + 1]\n",
    "    #   trnX = trnX.view(batch_size,seq_len,-1)\n",
    "\n",
    "    # model setup\n",
    "    input_size = trnX.shape[-1]\n",
    "    output_size = trnT.shape[-1]\n",
    "    hidden_size = 10 * trnX.shape[-1]\n",
    "    num_layers = 1\n",
    "\n",
    "    #   model = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    model = GRU_MLPRegressor(input_size, hidden_size, output_size, num_layers=num_layers)\n",
    "    model.cuda()\n",
    "    print('\\nMODEL\\n', model)\n",
    "\n",
    "    params = list(model.parameters())  # + list(h0) + list(c0)\n",
    "    optimizer = optim.Adam(params=params, lr=lr, weight_decay=0)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    n_steps = (loader.getTrainDataLength() // batch_size) + 1\n",
    "    print(\"\\nSteps per epoch\", n_steps)\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    min_val_loss = np.inf  # used for early stopping TODO: IMPLEMENT MOVING AVERAGE AND STOP IF VAL LOSS IS MOVING UP, IE OVERFITTING\n",
    "\n",
    "    valT, valX, valTimes = loader.getValidationSet()\n",
    "    valT = valT[:, :, 12:12 + 1]\n",
    "\n",
    "    for epoch in range(n_epochs):  # epoch is one iteration over the entire input data\n",
    "        loader.reset_current_t()\n",
    "        print(\"\\nEpoch:\", epoch)\n",
    "        for step in range(n_steps - 1):\n",
    "            # Train Evaluation and Optimization Step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            trnT, trnX, trnTimes = loader.getTrainBatch()\n",
    "            trnT = trnT[:, :, 12:12 + 1]  # choosing only center square\n",
    "            #       trnY, (hn, cn) = model.forward(trnX, (h0, c0))\n",
    "            trnY = model.forward(trnX)\n",
    "\n",
    "            trnLoss = torch.sqrt(criterion(trnY[-1], trnT[-1]))  # SQRT MSE not same as MAE\n",
    "            trnLoss.backward()\n",
    "            #       if itr == 0:\n",
    "            #         torch.nn.utils.clip_grad_norm(model.parameters(), 0.5) # Figure out fitting clip value\n",
    "            optimizer.step()\n",
    "            train_losses.append(trnLoss.cpu().item())\n",
    "\n",
    "            # Validation Evaluation\n",
    "            model.eval()\n",
    "            valY = model.forward(valX)\n",
    "\n",
    "            valLoss = torch.sqrt(criterion(valY[-1], valT[-1]))\n",
    "            validation_losses.append(valLoss.cpu().item())\n",
    "\n",
    "            if validation_losses[-1] < min_val_loss:\n",
    "                min_val_loss = validation_losses[-1]\n",
    "                best_model_val = model.state_dict()\n",
    "\n",
    "            if len(validation_losses) % 50 == 0:\n",
    "                print(\"Iteration:\", len(validation_losses))\n",
    "\n",
    "        print(\"Last Train Loss of Epoch:\", train_losses[-1])\n",
    "        print(\"Last Validation Loss of Epoch:\", validation_losses[-1])\n",
    "        print(\"Current Best Validation Loss:\", min_val_loss)\n",
    "    best_model_trn = model.state_dict()\n",
    "    trn_collection.append(train_losses)\n",
    "    val_collection.append(validation_losses)\n",
    "    best_model_trn_collection.append(best_model_trn)\n",
    "    best_model_val_collection.append(best_model_val)\n",
    "    print('Stop Time:', pd.datetime.now(), '\\n')\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"RMSE Loss of Model over Itterations\")\n",
    "for i, _ in enumerate(trn_collection):\n",
    "    plt.plot(trn_collection[i], label='Training: ' + str(itrs[i]))\n",
    "    plt.plot(val_collection[i], label='Validation: ' + str(itrs[i]))\n",
    "    plt.scatter(np.argmin(val_collection[i]), np.min(val_collection[i]), marker='X', c='r')\n",
    "plt.xlabel(\"Itteraion\")\n",
    "plt.ylabel(\"RMSE Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
