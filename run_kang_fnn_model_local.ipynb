{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38501,
     "status": "ok",
     "timestamp": 1575549782699,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "EIP3ykajpZPk",
    "outputId": "c0c6db47-f28c-4a45-bb92-fa513801db07"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# %cd /content/gdrive/My\\ Drive/masters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2872,
     "status": "error",
     "timestamp": 1573546837354,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "JNulAac9o8gl",
    "outputId": "68f90833-6270-4c5d-937b-f229f5822671"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "from time import strftime\n",
    "from copy import deepcopy\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from utils.data_processing import *\n",
    "from logger.logger import setup_logging\n",
    "from utils.configs import BaseConf\n",
    "from utils.utils import write_json, Timer\n",
    "from models.kangkang_fnn_models import KangFeedForwardNetwork, SimpleKangFNN, evaluate_fnn\n",
    "from dataloaders.flat_loader import FlatDataLoaders, MockLoader, MockLoaders\n",
    "from datasets.flat_dataset import FlatDataGroup\n",
    "from utils.metrics import PRCurvePlotter, ROCCurvePlotter, LossPlotter, PerTimeStepPlotter\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "from models.model_result import ModelResult, ModelMetrics, save_metrics, compare_all_models, get_models_metrics\n",
    "from utils.mock_data import mock_fnn_data_classification\n",
    "from utils.plots import im\n",
    "from utils.utils import pshape, get_data_sub_paths, by_ref\n",
    "from trainers.generic_trainer import train_model\n",
    "from models.kangkang_fnn_models import train_epoch_for_fnn\n",
    "\n",
    "from utils.metrics import best_threshold, get_y_pred, get_y_pred_by_thresholds, best_thresholds\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub_paths = get_data_sub_paths()\n",
    "print(np.sort(data_sub_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub_paths = by_ref(\"c97\")\n",
    "data_sub_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "conf = BaseConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TBN_1gYo8go"
   },
   "outputs": [],
   "source": [
    "# T24H-X850M-Y880M_2012-01-01_2019-01-01\n",
    "\n",
    "data_sub_path = data_sub_paths[0]\n",
    "\n",
    "# manually set\n",
    "conf = BaseConf()\n",
    "conf.seed = int(time()) # 3\n",
    "conf.model_name = \"Kang FNN\"  # needs to be created\n",
    "conf.data_path = f\"./data/processed/{data_sub_path}/\"\n",
    "\n",
    "compare_all_models(data_path=conf.data_path)\n",
    "\n",
    "if not os.path.exists(conf.data_path):\n",
    "    raise Exception(f\"Directory ({conf.data_path}) needs to exist.\")\n",
    "\n",
    "conf.model_path = f\"{conf.data_path}models/{conf.model_name}/\"\n",
    "os.makedirs(conf.data_path, exist_ok=True)\n",
    "os.makedirs(conf.model_path, exist_ok=True)\n",
    "\n",
    "# logging config is set globally thus we only need to call this in this file\n",
    "# imported function logs will follow the configuration\n",
    "setup_logging(save_dir=conf.model_path, log_config='./logger/standard_logger_config.json', default_level=log.INFO)\n",
    "log.info(\"=====================================BEGIN=====================================\")\n",
    "\n",
    "\n",
    "info = deepcopy(conf.__dict__)\n",
    "info[\"start_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# DATA LOADER SETUP\n",
    "np.random.seed(conf.seed)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(conf.seed)\n",
    "else:\n",
    "    torch.manual_seed(conf.seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "log.info(f\"Device: {device}\")\n",
    "info[\"device\"] = device.type\n",
    "conf.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlGDYBWmQz_1"
   },
   "outputs": [],
   "source": [
    "log.getLogger().setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i15tiMhOQz_3"
   },
   "outputs": [],
   "source": [
    "# SET THE HYPER PARAMETERS\n",
    "conf.resume = False\n",
    "conf.early_stopping = True\n",
    "conf.max_epochs = 300\n",
    "conf.dropout = 0.0005\n",
    "conf.weight_decay = 1e-8\n",
    "conf.checkpoint = \"best\" # [\"best\"|\"latest\"]\n",
    "conf.lr = 1e-2\n",
    "conf.shaper_top_k = -1\n",
    "conf.batch_size = 256\n",
    "\n",
    "MOCK = False\n",
    "\n",
    "if MOCK:\n",
    "    # MOCK THE DATA\n",
    "    vector_size= [5,5,5]#[37,65,512]    \n",
    "    class_split=0.5\n",
    "    train_loader = MockLoader(vector_size, conf.batch_size, n_samples=1000, class_split=class_split)\n",
    "    validation_loader = MockLoader(vector_size, conf.batch_size, n_samples=200, class_split=class_split)\n",
    "    test_loader = MockLoader(vector_size, conf.batch_size, n_samples=300, class_split=class_split)\n",
    "    loaders = MockLoaders(train_loader,validation_loader,test_loader)\n",
    "else:\n",
    "    # CRIME DATA\n",
    "    conf.sub_sample_test_set = 0\n",
    "    conf.sub_sample_train_set = 1\n",
    "    conf.sub_sample_validation_set = 1\n",
    "\n",
    "    data_group = FlatDataGroup(data_path=conf.data_path, conf=conf)\n",
    "    loaders = FlatDataLoaders(data_group=data_group, conf=conf)\n",
    "\n",
    "conf.freqstr = data_group.t_range.freqstr    \n",
    "\n",
    "# SET LOSS FUNCTION\n",
    "# size averaged - so more epochs or larger lr for smaller batches\n",
    "loss_function = nn.CrossEntropyLoss()  \n",
    "\n",
    "# SETUP MODEL\n",
    "train_set = loaders.train_loader.dataset\n",
    "indices, spc_feats, tmp_feats, env_feats, target = train_set[train_set.min_index]\n",
    "spc_size, tmp_size, env_size = spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]\n",
    "\n",
    "\n",
    "model_arch = {\n",
    "    \"h_size0\": 10,\n",
    "    \"h_size1\": 10,\n",
    "    \"h_size2\": 10,\n",
    "}\n",
    "model = SimpleKangFNN(spc_size=spc_size,\n",
    "                     tmp_size=tmp_size,\n",
    "                     env_size=env_size,\n",
    "                     dropout_p=conf.dropout,\n",
    "                     model_arch=model_arch)\n",
    "\n",
    "# model_arch = {\n",
    "#     \"scp_net_h0\": 64,\n",
    "#     \"scp_net_h1\": 32,\n",
    "#     \"tmp_net_h0\": 64,\n",
    "#     \"tmp_net_h1\": 32,\n",
    "#     \"env_net_h0\": 64,\n",
    "#     \"env_net_h1\": 32,\n",
    "#     \"final_net_h1\": 64,\n",
    "# }\n",
    "# model = KangFeedForwardNetwork(spc_size=spc_size,\n",
    "#                                  tmp_size=tmp_size,\n",
    "#                                  env_size=env_size,\n",
    "#                                  dropout_p=conf.dropout,\n",
    "#                                  model_arch=model_arch)\n",
    "\n",
    "model.to(conf.device)\n",
    "\n",
    "# SETUP OPTIMISER\n",
    "parameters = model.parameters()\n",
    "\n",
    "# important note: using weight decay (l2 penalty) can prohibit long term memory in LSTM networks\n",
    "# - use gradient clipping instead\n",
    "optimiser = optim.Adam(params=parameters, lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "##### RESUME LOGIC\n",
    "if conf.resume:  # todo check if the files actually exist\n",
    "    try:\n",
    "        # resume from previous check point or resume from best validaton score checkpoint\n",
    "        # load model state\n",
    "        model_state_dict = torch.load(f\"{conf.model_path}model_{conf.checkpoint}.pth\",\n",
    "                                      map_location=conf.device.type)\n",
    "        model.load_state_dict(model_state_dict)\n",
    "        \n",
    "        # load optimiser state\n",
    "        optimiser_state_dict = torch.load(f\"{conf.model_path}optimiser_{conf.checkpoint}.pth\",\n",
    "                                          map_location=conf.device.type)\n",
    "        optimiser.load_state_dict(optimiser_state_dict) \n",
    "\n",
    "        # new optimiser hyper-parameters\n",
    "        optimiser.param_groups[0]['lr'] = conf.lr\n",
    "        optimiser.param_groups[0]['weight_decay'] = conf.weight_decay\n",
    "\n",
    "        # new model hyper-parameters\n",
    "        model.dropout.p = conf.dropout # note that drop out is not part of the saved state dict\n",
    "\n",
    "    except Exception as e:\n",
    "        log.error(f\"Nothing to resume from, training from scratch \\n\\t-> {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEP8Ro-7ktLt",
    "outputId": "078ab7a1-6bba-40ac-e3cc-df92c7ea0a2a"
   },
   "outputs": [],
   "source": [
    "trn_epoch_losses, val_epoch_losses, stopped_early = train_model(\n",
    "    model=model,\n",
    "    optimiser=optimiser,\n",
    "    loaders=loaders,\n",
    "    train_epoch_fn=train_epoch_for_fnn,\n",
    "    loss_fn=loss_function,\n",
    "    conf=conf,\n",
    "    patience=conf.max_epochs, #50,\n",
    ")    \n",
    "\n",
    "print(f\"stopped_early: {stopped_early}\") # use the current epoch instead\n",
    "# if stopped_early -> continue with best_model - new hyper-parameters -> no n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(pd.DataFrame(dict(\n",
    "    trn_epoch_losses=trn_epoch_losses,\n",
    "    val_epoch_losses=val_epoch_losses,\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpAiVGybQz_6"
   },
   "outputs": [],
   "source": [
    "log.getLogger().setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6n5nE-CKQz_8"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1853,
     "status": "ok",
     "timestamp": 1572971602911,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "G_tVBsxynfax",
    "outputId": "816baf10-7b93-42e8-9f86-af432e77144b"
   },
   "outputs": [],
   "source": [
    "# Load latest or best validation model\n",
    "# conf.checkpoint = \"latest\"\n",
    "conf.checkpoint = \"best\"\n",
    "\n",
    "log.info(f\"Loading model from checkpoint ({conf.checkpoint}) for evaluation\")\n",
    "\n",
    "# resume from previous check point or resume from best validaton score checkpoint\n",
    "# load model state\n",
    "model_state_dict = torch.load(f\"{conf.model_path}model_{conf.checkpoint}.pth\",\n",
    "                                map_location=conf.device.type)\n",
    "model.load_state_dict(model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_M5lhF-iBkV"
   },
   "outputs": [],
   "source": [
    "conf.sub_sample_test_set = 0\n",
    "conf.sub_sample_train_set = 0\n",
    "conf.sub_sample_validation_set = 0\n",
    "\n",
    "loaders = FlatDataLoaders(data_group=data_group, conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11966,
     "status": "ok",
     "timestamp": 1572971617035,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "SwjL3yAxQz_8",
    "outputId": "bcd4f634-3a71-4ff5-8ce9-7c2b3dcb4b6f"
   },
   "outputs": [],
   "source": [
    "# todo set the train_loader to eval so that it does not subsample\n",
    "trn_y_true, trn_probas_pred, trn_t_range = evaluate_fnn(model=model,\n",
    "                                                        batch_loader=loaders.train_loader,\n",
    "                                                        conf=conf)\n",
    "\n",
    "thresh = best_threshold(y_true=trn_y_true,\n",
    "                       probas_pred=trn_probas_pred)\n",
    "\n",
    "tst_y_true, tst_probas_pred, tst_t_range = evaluate_fnn(model=model,\n",
    "                                                        batch_loader=loaders.test_loader,\n",
    "                                                        conf=conf)    \n",
    "        \n",
    "tst_y_pred = get_y_pred(thresh, tst_probas_pred)    \n",
    "        \n",
    "save_metrics(y_true=tst_y_true,\n",
    "             y_pred=tst_y_pred,\n",
    "             probas_pred=tst_probas_pred,\n",
    "             t_range=tst_t_range,\n",
    "             shaper=data_group.shaper,                \n",
    "             conf=conf)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ionwwS_IAfUi",
    "outputId": "b59a98eb-6425-4c09-8cae-21df7dfea865",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_all_models(data_path=conf.data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TMrB2MJAfSX"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZOnJiXDxAf1O"
   },
   "source": [
    "## All in one code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVMREIpVBXxi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "from time import strftime\n",
    "from copy import deepcopy\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from utils.data_processing import *\n",
    "from logger.logger import setup_logging\n",
    "from utils.configs import BaseConf\n",
    "from utils.utils import write_json, Timer\n",
    "from models.kangkang_fnn_models import KangFeedForwardNetwork, SimpleKangFNN\n",
    "\n",
    "from dataloaders.flat_loader import FlatDataLoaders, MockLoader, MockLoaders\n",
    "from datasets.flat_dataset import FlatDataGroup\n",
    "from utils.metrics import PRCurvePlotter, ROCCurvePlotter, LossPlotter, PerTimeStepPlotter\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "from models.model_result import ModelResult, ModelMetrics, save_metrics, compare_all_models, get_models_metrics\n",
    "from utils.mock_data import mock_fnn_data_classification\n",
    "from utils.plots import im\n",
    "from utils.utils import pshape\n",
    "from trainers.generic_trainer import train_model\n",
    "\n",
    "from models.kangkang_fnn_models import train_epoch_for_fnn\n",
    "\n",
    "from utils.metrics import best_threshold, get_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17129,
     "status": "ok",
     "timestamp": 1575549809167,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "jLNw3nBaCRDf",
    "outputId": "55f76bed-5008-46ed-cf44-20a46d5f09e8"
   },
   "outputs": [],
   "source": [
    "data_sub_paths = os.listdir(\"./data/processed/\")\n",
    "data_sub_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub_paths = ['T24H-X255M-Y220M_2012-01-01_2019-01-01_#c97']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10742,
     "status": "error",
     "timestamp": 1575552789748,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "Rj4hCcmrAfRa",
    "outputId": "4ecc386d-9b1c-4200-f899-0b0c154ea32c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# data_dim_str = \"T24H-X255M-Y220M\"#\"T24H-X850M-Y880M\" # \"T24H-X85M-Y110M\" #\"T1H-X1700M-Y1760M\"  # needs to exist  \n",
    "for data_sub_path in data_sub_paths:\n",
    "    model_name = \"Kang FNN\"  # needs to be created    \n",
    "    data_path = f\"./data/processed/{data_sub_path}/\"\n",
    "    compare_all_models(data_path=data_path)\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        raise Exception(f\"Directory ({data_path}) needs to exist.\")\n",
    "\n",
    "    model_path = data_path + f\"models/{model_name}/\"\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    # logging config is set globally thus we only need to call this in this file\n",
    "    # imported function logs will follow the configuration\n",
    "    setup_logging(save_dir=model_path, log_config='./logger/standard_logger_config.json', default_level=log.INFO)\n",
    "    log.info(\"=====================================BEGIN=====================================\")\n",
    "\n",
    "    timer = Timer()\n",
    "    # manually set the config\n",
    "    conf_dict = {\n",
    "        \"seed\": 3,\n",
    "        \"use_cuda\": True,\n",
    "        \n",
    "        \"use_crime_types\": False,\n",
    "        \n",
    "        # data group/data set related\n",
    "        \"val_ratio\": 0.1,  # ratio of the total dataset\n",
    "        \"tst_ratio\": 0.3,# ratio of the total dataset\n",
    "        \"seq_len\": 1,\n",
    "        \"flatten_grid\": True,  # if the shaper should be used to squeeze the data\n",
    "        \n",
    "        # shaper related \n",
    "        \"shaper_top_k\": -1,  # if less then 0, top_k will not be applied\n",
    "        \"shaper_threshold\": 0,\n",
    "\n",
    "        \n",
    "        # data loader related\n",
    "        \"sub_sample_train_set\": 1,\n",
    "        \"sub_sample_validation_set\": 1,\n",
    "        \"sub_sample_validation_set\": 0,\n",
    "        \n",
    "        # training parameters\n",
    "        \"resume\": False,\n",
    "        \"early_stopping\": False,\n",
    "        \"tolerance\": 1e-8,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 1e-8,\n",
    "        \"max_epochs\": 1,\n",
    "        \"batch_size\": 64,\n",
    "        \"dropout\": 0.2,\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 6,\n",
    "        \n",
    "        # attached global variables - bad practice -find alternative\n",
    "        \"device\": None,  # pytorch device object [CPU|GPU]\n",
    "        \"timer\": Timer(),\n",
    "        \"model_name\": model_name,\n",
    "        \"model_path\": model_path,\n",
    "        \"checkpoint\": \"best\",\n",
    "\n",
    "        \"pad_width\": 0,\n",
    "        \n",
    "        \"use_seq_loss\": True,\n",
    "        \"use_classification\": True,\n",
    "        \"use_historic_average\": True,\n",
    "    }\n",
    "    conf = BaseConf(conf_dict=conf_dict)\n",
    "\n",
    "    info = deepcopy(conf.__dict__)\n",
    "    info[\"start_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "    # DATA LOADER SETUP\n",
    "    np.random.seed(conf.seed)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed( conf.seed)\n",
    "    else:\n",
    "        torch.manual_seed(conf.seed)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    log.info(f\"Device: {device}\")\n",
    "    info[\"device\"] = device.type\n",
    "    conf.device = device\n",
    "\n",
    "    log.getLogger().setLevel(\"INFO\")\n",
    "\n",
    "    # SET THE HYPER PARAMETERS\n",
    "    conf.resume = False\n",
    "    conf.early_stopping = False\n",
    "    conf.max_epochs = 1\n",
    "    conf.dropout = 0#0.2\n",
    "    conf.weight_decay = 1e-8\n",
    "    conf.checkpoint = \"best\" # [\"best\"|\"latest\"]\n",
    "    conf.lr = 5e-4\n",
    "    conf.shaper_top_k = -1\n",
    "    conf.batch_size =  256\n",
    "\n",
    "    MOCK = False\n",
    "\n",
    "    if MOCK:\n",
    "        # MOCK THE DATA\n",
    "        vector_size= [5,5,5]#[37,65,512]    \n",
    "        class_split=0.5\n",
    "        train_loader = MockLoader(vector_size, conf.batch_size, n_samples=1000, class_split=class_split)\n",
    "        validation_loader = MockLoader(vector_size, conf.batch_size, n_samples=200, class_split=class_split)\n",
    "        test_loader = MockLoader(vector_size, conf.batch_size, n_samples=300, class_split=class_split)\n",
    "        loaders = MockLoaders(train_loader,validation_loader,test_loader)\n",
    "    else:\n",
    "        # CRIME DATA\n",
    "        conf.sub_sample_test_set = False\n",
    "        conf.sub_sample_train_set = True\n",
    "        conf.sub_sample_validation_set = True\n",
    "\n",
    "        data_group = FlatDataGroup(data_path=data_path, conf=conf)\n",
    "        loaders = FlatDataLoaders(data_group=data_group, conf=conf)\n",
    "\n",
    "    conf.freqstr = data_group.t_range.freqstr    \n",
    "\n",
    "    # SET LOSS FUNCTION\n",
    "    # size averaged - so more epochs or larger lr for smaller batches\n",
    "    loss_function = nn.CrossEntropyLoss()  \n",
    "\n",
    "    # SETUP MODEL\n",
    "    train_set = loaders.train_loader.dataset\n",
    "    indices, spc_feats, tmp_feats, env_feats, target = train_set[train_set.min_index]\n",
    "    spc_size, tmp_size, env_size = spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]\n",
    "\n",
    "\n",
    "    model_arch = {\n",
    "        \"h_size0\": 100,\n",
    "        \"h_size1\": 100,\n",
    "        \"h_size2\": 100,\n",
    "    }\n",
    "    model = SimpleKangFNN(spc_size=spc_size,\n",
    "                        tmp_size=tmp_size,\n",
    "                        env_size=env_size,\n",
    "                        dropout_p=conf.dropout,\n",
    "                        model_arch=model_arch)\n",
    "\n",
    "    # model_arch = {\n",
    "    #     \"scp_net_h0\": 64,\n",
    "    #     \"scp_net_h1\": 32,\n",
    "    #     \"tmp_net_h0\": 64,\n",
    "    #     \"tmp_net_h1\": 32,\n",
    "    #     \"env_net_h0\": 64,\n",
    "    #     \"env_net_h1\": 32,\n",
    "    #     \"final_net_h1\": 64,\n",
    "    # }\n",
    "    # model = KangFeedForwardNetwork(spc_size=spc_size,\n",
    "    #                                  tmp_size=tmp_size,\n",
    "    #                                  env_size=env_size,\n",
    "    #                                  dropout_p=conf.dropout,\n",
    "    #                                  model_arch=model_arch)\n",
    "\n",
    "    model.to(conf.device)\n",
    "\n",
    "    # SETUP OPTIMISER\n",
    "    parameters = model.parameters()\n",
    "\n",
    "    # important note: using weight decay (l2 penalty) can prohibit long term memory in LSTM networks\n",
    "    # - use gradient clipping instead\n",
    "    optimiser = optim.Adam(params=parameters, lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "    ##### RESUME LOGIC\n",
    "    if conf.resume:  # todo check if the files actually exist\n",
    "        try:\n",
    "            # resume from previous check point or resume from best validaton score checkpoint\n",
    "            # load model state\n",
    "            model_state_dict = torch.load(f\"{conf.model_path}model_{conf.checkpoint}.pth\",\n",
    "                                        map_location=conf.device.type)\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            \n",
    "            # load optimiser state\n",
    "            optimiser_state_dict = torch.load(f\"{conf.model_path}optimiser_{conf.checkpoint}.pth\",\n",
    "                                            map_location=conf.device.type)\n",
    "            optimiser.load_state_dict(optimiser_state_dict) \n",
    "\n",
    "            # new optimiser hyper-parameters\n",
    "            optimiser.param_groups[0]['lr'] = conf.lr\n",
    "            optimiser.param_groups[0]['weight_decay'] = conf.weight_decay\n",
    "\n",
    "            # new model hyper-parameters\n",
    "            model.dropout.p = conf.dropout # note that drop out is not part of the saved state dict\n",
    "\n",
    "        except Exception as e:\n",
    "            log.error(f\"Nothing to resume from, training from scratch \\n\\t-> {e}\")\n",
    "\n",
    "    trn_epoch_losses, val_epoch_losses, stopped_early = train_model(model=model,\n",
    "                                                                        optimiser=optimiser,\n",
    "                                                                        loaders=loaders,\n",
    "                                                                        train_epoch_fn=train_epoch_for_fnn,\n",
    "                                                                        loss_fn=loss_function,\n",
    "                                                                        conf=conf)    \n",
    "\n",
    "    print(f\"stopped_early: {stopped_early}\") # use the current epoch instead\n",
    "    # if stopped_early -> continue with best_model - new hyper-parameters -> no n \n",
    "\n",
    "    # Load latest or best validation model\n",
    "    # conf.checkpoint = \"best\"\n",
    "    conf.checkpoint = \"best\"\n",
    "\n",
    "    log.info(f\"Loading model from checkpoint ({conf.checkpoint}) for evaluation\")\n",
    "\n",
    "    # resume from previous check point or resume from best validaton score checkpoint\n",
    "    # load model state\n",
    "    model_state_dict = torch.load(f\"{conf.model_path}model_{conf.checkpoint}.pth\",\n",
    "                                    map_location=conf.device.type)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "\n",
    "    conf.sub_sample_test_set = False\n",
    "    conf.sub_sample_train_set = False\n",
    "    conf.sub_sample_validation_set = False\n",
    "\n",
    "    loaders = FlatDataLoaders(data_group=data_group, conf=conf)\n",
    "\n",
    "    def evaluate_fnn(model, batch_loader, conf):    \n",
    "        \"\"\"\n",
    "        Only used to get probas in a time and location based format. The hard predictions should be done outside \n",
    "        this function where the threshold is determined using only the training data\n",
    "        \"\"\"\n",
    "        probas_pred = np.zeros(batch_loader.dataset.target_shape,dtype=np.float)\n",
    "        y_true = batch_loader.dataset.targets[-len(probas_pred):]\n",
    "        t_range = batch_loader.dataset.t_range[-len(probas_pred):]\n",
    "        \n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            model.eval()\n",
    "                \n",
    "            num_batches = batch_loader.num_batches\n",
    "            for indices, spc_feats, tmp_feats, env_feats, targets in batch_loader:\n",
    "                current_batch = batch_loader.current_batch\n",
    "\n",
    "                # Transfer to PyTorch Tensor and GPU\n",
    "                spc_feats = torch.Tensor(spc_feats[0]).to(conf.device) # only taking [0] for fnn\n",
    "                tmp_feats = torch.Tensor(tmp_feats[0]).to(conf.device) # only taking [0] for fnn\n",
    "                env_feats = torch.Tensor(env_feats[0]).to(conf.device) # only taking [0] for fnn\n",
    "                targets = torch.LongTensor(targets[0,:,0]).to(conf.device) # only taking [0] for fnn\n",
    "                out = model(spc_feats, tmp_feats, env_feats)\n",
    "\n",
    "                batch_probas_pred = F.softmax(out,dim=-1)[:,1].cpu().numpy() # select class1 prediction\n",
    "                \n",
    "                for i, p in zip(indices,batch_probas_pred):\n",
    "                    n,c,l = i   \n",
    "                    probas_pred[n,c,l] = p\n",
    "\n",
    "        \n",
    "        return y_true, probas_pred, t_range\n",
    "\n",
    "\n",
    "    # todo set the train_loader to eval so that it does not subsample\n",
    "    trn_y_true, trn_probas_pred, trn_t_range = evaluate_fnn(model=model,\n",
    "                                                            batch_loader=loaders.train_loader,\n",
    "                                                            conf=conf)\n",
    "\n",
    "    thresh = best_threshold(y_true=trn_y_true,\n",
    "                        probas_pred=trn_probas_pred)\n",
    "\n",
    "    tst_y_true, tst_probas_pred, tst_t_range = evaluate_fnn(model=model,\n",
    "                                                            batch_loader=loaders.test_loader,\n",
    "                                                            conf=conf)    \n",
    "        \n",
    "        \n",
    "\n",
    "    tst_y_pred = get_y_pred(thresh, tst_probas_pred)    \n",
    "        \n",
    "        \n",
    "    save_metrics(y_true=tst_y_true,\n",
    "                y_pred=tst_y_pred,\n",
    "                probas_pred=tst_probas_pred,\n",
    "                t_range=tst_t_range,\n",
    "                shaper=data_group.shaper,                \n",
    "                conf=conf)   \n",
    "\n",
    "    compare_all_models(data_path=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wuHJzAl9AfOW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nF7EKOLIAfJS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E-ENJTuuAfGF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6604,
     "status": "ok",
     "timestamp": 1572962084556,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "VQURtJxoP4Dd",
    "outputId": "4dc9be4f-dfe3-41a2-e40a-997656debec9"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "timer = Timer()\n",
    "timer.reset()\n",
    "i = 0\n",
    "for v in loaders.train_loader:\n",
    "    i += 1\n",
    "print(f\"i: {i} and train loader time {timer.check()}\")    \n",
    "\n",
    "timer.reset()\n",
    "i = 0\n",
    "for v in loaders.validation_loader:\n",
    "    i += 1\n",
    "print(f\"i: {i} and val loader time {timer.check()}\")    \n",
    "\n",
    "timer.reset()\n",
    "i = 0\n",
    "for v in loaders.test_loader:\n",
    "    i += 1\n",
    "print(f\"i: {i} and test loader time {timer.check()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_frDTLUb87D"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "\n",
    "def safe_f1_score(pr):\n",
    "    p, r = pr\n",
    "    if p + r == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 2 * (p * r) / (p + r)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(trn_y_true.flatten(), trn_probas_pred.flatten())\n",
    "precision, recall, thresholds = precision_recall_curve(tst_y_true.flatten(), tst_probas_pred.flatten())\n",
    "scores = np.array(list(map(safe_f1_score, zip(precision, recall))))\n",
    "index_array = np.argmax(scores)  # almost always a singular int, and not an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5672,
     "status": "ok",
     "timestamp": 1572536246817,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "rXdC-wrgcly3",
    "outputId": "bd77ee21-1c55-417d-8358-19212846eab8"
   },
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.scatter(index_array,scores[index_array])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(recall, precision)\n",
    "plt.scatter(recall, precision, s=scores*5,alpha=.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uj6S4o_bQz_-"
   },
   "source": [
    "**Metrics per cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 112385,
     "status": "ok",
     "timestamp": 1572534296792,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "JII1fubnQz__",
    "outputId": "358f1c22-11b8-4f75-f3d6-523cf08649a6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"ap_per_cell\\n\")\n",
    "ap_per_cell = average_precision_score_per_cell(tst_y_true, tst_probas_pred)\n",
    "ap_per_cell = np.nan_to_num(ap_per_cell)\n",
    "ap_per_cell = data_group.shaper.unsqueeze(ap_per_cell)[0,0]\n",
    "im(ap_per_cell)\n",
    "\n",
    "print(\"roc_per_cell\\n\")\n",
    "roc_per_cell = roc_auc_score_per_cell(tst_y_true, tst_probas_pred)\n",
    "roc_per_cell = np.nan_to_num(roc_per_cell)\n",
    "roc_per_cell = data_group.shaper.unsqueeze(roc_per_cell)[0,0]\n",
    "im(roc_per_cell)\n",
    "\n",
    "print(\"p_per_cell\\n\")\n",
    "p_per_cell = precision_score_per_cell(tst_y_true, tst_y_pred)\n",
    "p_per_cell = data_group.shaper.unsqueeze(p_per_cell)[0,0]\n",
    "im(p_per_cell)\n",
    "\n",
    "print(\"r_per_cell\\n\")\n",
    "r_per_cell = precision_score_per_cell(tst_y_true, tst_y_pred)\n",
    "r_per_cell = data_group.shaper.unsqueeze(r_per_cell)[0,0]\n",
    "im(r_per_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D5R9BEaXQ0AB"
   },
   "source": [
    "**Metrics per time slot**: should be one plot per metric - with multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jZ8SPDB2Q0AC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7226,
     "status": "ok",
     "timestamp": 1572533895011,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "VeXhlKKMQ0AD",
    "outputId": "232bd95a-c48f-4223-aee2-9f2e246c6e3a"
   },
   "outputs": [],
   "source": [
    "freqstr = data_group.t_range.freqstr\n",
    "\n",
    "N = tst_y_true.shape[0]\n",
    "\n",
    "ap_per_time = average_precision_score_per_time_slot(y_true=tst_y_true,\n",
    "                                                    probas_pred=tst_probas_pred)\n",
    "\n",
    "ap_per_time = np.nan_to_num(ap_per_time)\n",
    "\n",
    "roc_per_time = roc_auc_score_per_time_slot(y_true=tst_y_true,\n",
    "                                           probas_pred=tst_probas_pred)\n",
    "roc_per_time = np.nan_to_num(roc_per_time)\n",
    "\n",
    "acc_per_time = accuracy_score_per_time_slot(y_true=tst_y_true,\n",
    "                                           y_pred=tst_y_pred)\n",
    "acc_per_time = np.nan_to_num(acc_per_time)\n",
    "\n",
    "p_per_time = precision_score_per_time_slot(y_true=tst_y_true,\n",
    "                                           y_pred=tst_y_pred)\n",
    "p_per_time = np.nan_to_num(p_per_time)\n",
    "\n",
    "r_per_time = recall_score_per_time_slot(y_true=tst_y_true,\n",
    "                                           y_pred=tst_y_pred)\n",
    "r_per_time = np.nan_to_num(r_per_time)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(acc_per_time[:,0,0],label=\"Accuracy\")\n",
    "plt.plot(roc_per_time[:,0,0],label=\"ROC-AUC\")\n",
    "plt.plot(ap_per_time[:,0,0],label=\"Average Precision\")\n",
    "plt.plot(p_per_time[:,0,0],label=\"Precision\")\n",
    "plt.plot(r_per_time[:,0,0],label=\"Recall\")\n",
    "plt.title(\"Metrics per time step\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xlabel(f\"Time (step={freqstr})\")\n",
    "plt.ylim([0,1])\n",
    "# plt.xlim([0,N-1])\n",
    "plt.yticks(np.arange(11)/10)\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True,alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gw0syNo6Q0AF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZVozhnCQ0AH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HCvX66iUQ0AI"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score, recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_XlPeFHQ0AK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3395,
     "status": "ok",
     "timestamp": 1572533985727,
     "user": {
      "displayName": "Bernard Swart",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mA7uSA62RDxMrL_HdFxD2SxWswIX97G3OjMRiFvmMU=s64",
      "userId": "00293673988406756138"
     },
     "user_tz": -120
    },
    "id": "L5jdMvmlQ0AM",
    "outputId": "d2a3a30c-b71c-4d88-d0fa-e95abbdf71e6"
   },
   "outputs": [],
   "source": [
    "im(data_group.shaper.unsqueeze(tst_probas_pred)[40][0])\n",
    "im(data_group.shaper.unsqueeze(tst_y_pred)[40][0])\n",
    "im(data_group.shaper.unsqueeze(tst_y_true)[40][0])\n",
    "# im(data_group.shaper.unsqueeze(loaders.test_loader.dataset.targets)[40][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7pQF1xKQ0AN"
   },
   "outputs": [],
   "source": [
    "loaders.test_loader.dataset.targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Damf22UiQ0AQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NNwSMXK_Q0AS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lv45JgU3Q0AU"
   },
   "outputs": [],
   "source": [
    "targets = loaders.data_group.testing_set.targets\n",
    "\n",
    "# todo get better name\n",
    "probas_grid = np.zeros_like(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZwkB1D_QQ0AX"
   },
   "outputs": [],
   "source": [
    "\n",
    "###### testing ###### torch outputs to see if we're actually getting valuable things out\n",
    "out = torch.Tensor([[-3,2],[2,-1],[0.5,0.4]])\n",
    "print(out.shape)\n",
    "print(out)\n",
    "out_soft = F.softmax(out, dim=-1)\n",
    "print(out_soft)\n",
    "out_label = torch.argmax(out_soft, dim=-1)\n",
    "print(out_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfZh9USWo8g2"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path + \"model_best.pth\"))\n",
    "conf = BaseConf(conf_dict=conf_dict)\n",
    "conf.batch_size = 4\n",
    "loaders = FlatDataLoaders(data_path=data_path, conf=conf)\n",
    "\n",
    "# EVALUATE MODEL\n",
    "with torch.set_grad_enabled(False):\n",
    "    # Transfer to GPU\n",
    "    testing_losses = []\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    probas_pred = []\n",
    "    \n",
    "    \n",
    "    # loop through is set does not fit in batch\n",
    "    for indices, spc_feats, tmp_feats, env_feats, targets in loaders.test_loader: \n",
    "        \"\"\"\n",
    "        IMPORTNANT NOTE: WHEN DOING LSTM - ONLY FEED THE TEMPORAL VECTORS IN THE LSTM\n",
    "        FEED THE REST INTO THE NORMAL LINEAR NETWORKS\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Transfer to GPU\n",
    "        spc_feats = torch.Tensor(spc_feats).to(conf.device)\n",
    "        tmp_feats = torch.Tensor(tmp_feats).to(conf.device)\n",
    "        env_feats = torch.Tensor(env_feats).to(conf.device)\n",
    "        targets = torch.LongTensor(targets).to(conf.device)\n",
    "        \n",
    "        y_true.extend(targets.tolist())\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "                        \n",
    "        out = F.softmax(out, dim=-1)\n",
    "        \n",
    "        log.info(f\"out: {out}\")\n",
    "        log.info(f\"indices: {indices}\")\n",
    "        log.info(f\"indices: {targets}\")\n",
    "    \n",
    "        out_label = torch.argmax(out, dim=-1)\n",
    "        log.info(f\"out_label: {out_label}\")\n",
    "        y_pred.extend(out_label.tolist())\n",
    "        out_proba = out[:, 1]  # likelihood of crime is more general form - when comparing to moving averages\n",
    "        probas_pred.extend(out_proba.tolist())\n",
    "        \n",
    "        break  # !! remove\n",
    "\n",
    "\n",
    "# todo change to be the shape (N,L) of the original prediction.        \n",
    "model_result = ModelResult(model_name=\"FNN (Kang and Kang)\",\n",
    "                            y_true=y_true,\n",
    "                            y_pred=y_pred,\n",
    "                            probas_pred=probas_pred,\n",
    "                            t_range=loaders.test_loader.dataset.t_range,\n",
    "                            shaper=loaders.data_group.shaper)\n",
    "                        \n",
    "# log.info(model_result)\n",
    "\n",
    "np.savez_compressed(model_path + \"evaluation_results.npz\", model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DRCri5po8g3"
   },
   "outputs": [],
   "source": [
    "pr_plotter = PRCurvePlotter()\n",
    "pr_plotter.add_curve(y_true, probas_pred, label_name=\"FNN (Kang and Kang)\")\n",
    "pr_plotter.savefig(model_path + \"plot_pr_curve.png\")\n",
    "\n",
    "roc_plotter = ROCCurvePlotter()\n",
    "roc_plotter.add_curve(y_true, probas_pred, label_name=\"FNN (Kang and Kang)\")\n",
    "roc_plotter.savefig(model_path + \"plot_roc_curve.png\")\n",
    "\n",
    "info[\"stop_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "write_json(info, model_path + \"info.json\")\n",
    "\n",
    "log.info(\"=====================================END=====================================\")targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K17LWEnyo8g5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0hpqNyfVo8g7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "run_kang_fnn_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
