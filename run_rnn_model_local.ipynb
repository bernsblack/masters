{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# major problem what we have is that the models try to generalise to all cells and do not focus on local temporal features\n",
    "\n",
    "in this notebook we are going to simplify our problem by only looking at the individual cells. This allows us to ocmpare different techniques like the moving averages and actual hawkes processes (point processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# %cd /content/gdrive/My\\ Drive/masters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mT12H-X850M-Y880M_2013-01-01_2017-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT1H-X1700M-Y1760M_2013-01-01_2017-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT24H-X1275M-Y1320M_2012-01-01_2019-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT24H-X1700M-Y1760M_2012-01-01_2019-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT24H-X255M-Y220M_2012-01-01_2019-01-01_#c97\u001b[0m/\r\n",
      "\u001b[01;34mT24H-X255M-Y220M_2013-01-01_2017-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT24H-X425M-Y440M_2012-01-01_2019-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT24H-X425M-Y440M_2013-01-01_2017-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT24H-X850M-Y880M_2012-01-01_2019-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT24H-X850M-Y880M_2013-01-01_2017-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT24H-X85M-Y110M_2013-01-01_2017-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT3H-X850M-Y880M_2013-01-01_2017-01-01\u001b[0m/\r\n",
      "\u001b[01;34mT6H-X850M-Y880M_2013-01-01_2017-01-01\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls ./data/processed\n",
    "\n",
    "# T1H-X1700M-Y1760M/  T24H-X850M-Y880M/  T3H-X850M-Y880M/\n",
    "# T12H-X850M-Y880M/  T24H-X425M-Y440M/   T24H-X85M-Y110M/   T6H-X850M-Y880M/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging as log\n",
    "from time import strftime\n",
    "from copy import deepcopy\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from utils.data_processing import *\n",
    "from logger.logger import setup_logging\n",
    "from utils.configs import BaseConf\n",
    "from utils.utils import write_json, Timer, get_data_sub_paths, pshape, by_ref\n",
    "from models.kangkang_fnn_models import KangFeedForwardNetwork\n",
    "from dataloaders.flat_loader import FlatDataLoaders\n",
    "from datasets.flat_dataset import FlatDataGroup\n",
    "\n",
    "from dataloaders.cell_loader import CellDataLoaders\n",
    "from datasets.cell_dataset import CellDataGroup\n",
    "\n",
    "from dataloaders.grid_loader import GridDataLoaders\n",
    "from datasets.grid_dataset import GridDataGroup\n",
    "\n",
    "from utils.metrics import PRCurvePlotter, ROCCurvePlotter, LossPlotter\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "from models.model_result import ModelResult\n",
    "from utils.mock_data import mock_rnn_data_classification, mock_rnn_data_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.plots import im\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUFNN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRUFNN, self).__init__()\n",
    "\n",
    "        self.name = \"GRUFNN\"\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers)  # (seq_len, batch_size, n_features) format\n",
    "        self.lin1 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        # Forward propagate RNN\n",
    "        if h0 is not None:\n",
    "            out, hn = self.gru(x, h0)\n",
    "        else:\n",
    "            out, hn = self.gru(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T12H-X850M-Y880M_2013-01-01_2017-01-01',\n",
      " 'T1H-X1700M-Y1760M_2013-01-01_2017-01-01',\n",
      " 'T1H-X3400M-Y3520M_2014-01-01_2016-01-01_#2a4',\n",
      " 'T24H-X1275M-Y1320M_2012-01-01_2019-01-01',\n",
      " 'T24H-X1700M-Y1760M_2012-01-01_2019-01-01',\n",
      " 'T24H-X255M-Y220M_2012-01-01_2019-01-01_#c97',\n",
      " 'T24H-X255M-Y220M_2012-01-01_2019-01-01_#d97',\n",
      " 'T24H-X255M-Y220M_2013-01-01_2017-01-01',\n",
      " 'T24H-X255M-Y220M_2014-01-01_2019-01-01_#b29',\n",
      " 'T24H-X425M-Y440M_2012-01-01_2019-01-01',\n",
      " 'T24H-X425M-Y440M_2013-01-01_2017-01-01',\n",
      " 'T24H-X850M-Y880M_2012-01-01_2019-01-01',\n",
      " 'T24H-X850M-Y880M_2013-01-01_2017-01-01',\n",
      " 'T24H-X850M-Y880M_2014-01-01_2019-01-01_#1a0',\n",
      " 'T24H-X85M-Y110M_2013-01-01_2017-01-01',\n",
      " 'T3H-X850M-Y880M_2013-01-01_2017-01-01',\n",
      " 'T6H-X850M-Y880M_2013-01-01_2017-01-01']\n"
     ]
    }
   ],
   "source": [
    "data_sub_paths = get_data_sub_paths()\n",
    "pprint(sorted(data_sub_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: T24H-X850M-Y880M_2014-01-01_2019-01-01_#1a0\n"
     ]
    }
   ],
   "source": [
    "data_sub_path = by_ref(\"1a0\")[0]\n",
    "print(f\"using: {data_sub_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted historic average: step (1) and max_steps (-1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D object at 0x7f16826becd0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conf = BaseConf()\n",
    "conf.data_path = f\"./data/processed/{data_sub_path}/\"    \n",
    "\n",
    "group = CellDataGroup(data_path=conf.data_path, conf=conf)\n",
    "loaders = CellDataLoaders(data_group=group, conf=conf)\n",
    "\n",
    "crimes = group.shaper.unsqueeze(group.crimes)[:,0:1]\n",
    "\n",
    "from utils.data_processing import pad2d\n",
    "crimes_max = crimes.max(0)[0]\n",
    "args = np.argwhere(crimes_max > 0)\n",
    "batch_size = 64\n",
    "i = 0\n",
    "\n",
    "seq_len = 10\n",
    "N,C,H,W = crimes.shape\n",
    "\n",
    "\n",
    "model = nn.GRU(input_size=1, hidden_size=1)\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "optimiser = optim.Adam(model.parameters(),lr=0.001,weight_decay=0)\n",
    "\n",
    "max_epochs = 1\n",
    "\n",
    "x = np.sin(np.arange(100)*2*np.pi/20).reshape(-1,1,1)\n",
    "y = np.copy(x[1:])\n",
    "x = x[:-1]\n",
    "\n",
    "\n",
    "losses = []\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    while i < len(args):\n",
    "        j = i + batch_size\n",
    "        if j > len(args):\n",
    "            j = len(args)\n",
    "\n",
    "        y_args, x_args = args[i:j,0], args[i:j,1] # h, w shape - thus y is first\n",
    "        sub_crimes = crimes[:,:,y_args,x_args]\n",
    "        # time loop\n",
    "        for t in range(seq_len, N-1):\n",
    "            # detach states for truncate backprop\n",
    "            optimiser.zero_grad()\n",
    "    #         print(sub_crimes[t-seq_len:t].shape)\n",
    "            inputs = torch.Tensor(sub_crimes[t-seq_len:t]).transpose(1,2)\n",
    "            targets = torch.Tensor(sub_crimes[t+1-seq_len:t+1]).transpose(1,2)\n",
    "\n",
    "            outputs, h = model(inputs)\n",
    "\n",
    "            loss = torch.sqrt(mse(outputs, targets))\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            losses.append(loss.item())        \n",
    "        # (seq_len, batch_size, n_feats)\n",
    "\n",
    "        i = i + batch_size\n",
    "        \n",
    "plt.plot(losses)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = seq_len, batch_size, n_feats = 10,64,1\n",
    "inputs = torch.rand(shape)\n",
    "\n",
    "y,h = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimes_mean = crimes.mean(0)[0]\n",
    "zeros = np.zeros_like(crimes_mean)\n",
    "mean_shape = crimes_mean.shape\n",
    "sorted_indices = np.argsort(crimes_mean.flatten())[::-1]\n",
    "y_args, x_args = np.unravel_index(indices=sorted_indices, shape=mean_shape)\n",
    "\n",
    "\n",
    "top_k = 10\n",
    "zeros[y_args[:top_k], x_args[:top_k]] = 1\n",
    "im(zeros)\n",
    "im(crimes_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 3]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3e50d70498d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 3]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "model = nn.GRU(input_size=1, hidden_size=1)\n",
    "\n",
    "mse = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001,weight_decay=0)\n",
    "\n",
    "max_epochs = 1\n",
    "\n",
    "N = 120\n",
    "seq_len = 10\n",
    "\n",
    "x = np.sin(np.arange(N)*2*np.pi/20).reshape(-1,1,1)\n",
    "y = np.copy(x[1:])\n",
    "x = x[:-1]\n",
    "\n",
    "h_state = torch.zeros((1,1,1))\n",
    "losses = []\n",
    "for t in range(seq_len, N-1):\n",
    "    h_state.detach()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    inputs = torch.Tensor(x[t-seq_len:t])\n",
    "    targets = torch.Tensor(y[t-seq_len:t])\n",
    "\n",
    "    outputs, h = model(inputs,h_state)\n",
    "    h_state = outputs[0:1]\n",
    "    \n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    \n",
    "    loss = torch.sqrt(mse(outputs, targets))\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())        \n",
    "    \n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-28T09:10:27 | root | INFO | =====================================BEGIN=====================================\n",
      "2020-09-28T09:10:27 | root | INFO | Device: cpu\n"
     ]
    }
   ],
   "source": [
    "conf = BaseConf()\n",
    "\n",
    "conf.model_name = \"GRU\"  # needs to be created\n",
    "data_path = f\"./data/processed/{data_sub_path}/\"\n",
    "conf.model_path = f\"{data_path}models/{conf.model_name}/\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "os.makedirs(conf.model_path, exist_ok=True)\n",
    "\n",
    "# logging config is set globally thus we only need to call this in this file\n",
    "# imported function logs will follow the configuration\n",
    "setup_logging(save_dir=conf.model_path, log_config='./logger/standard_logger_config.json', default_level=log.INFO)\n",
    "log.info(\"=====================================BEGIN=====================================\")\n",
    "\n",
    "\n",
    "info = deepcopy(conf.__dict__)\n",
    "info[\"start_time\"] = strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# DATA LOADER SETUP\n",
    "np.random.seed(conf.seed)\n",
    "use_cuda = False # torch.cuda.is_available()\n",
    "torch.manual_seed(conf.seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(conf.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "log.info(f\"Device: {device}\")\n",
    "info[\"device\"] = device.type\n",
    "conf.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-28T09:10:28 | root | INFO | \tt_range shape (1827,)\n",
      "fitted historic average: step (1) and max_steps (-1)\n"
     ]
    }
   ],
   "source": [
    "# GET DATA\n",
    "data_group = FlatDataGroup(data_path=data_path, conf=conf)\n",
    "loaders = FlatDataLoaders(data_group=data_group, conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNN_crime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-83ad97c45bb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN_crime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# model.cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# # Loss and Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# if is_weighted_loss:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RNN_crime' is not defined"
     ]
    }
   ],
   "source": [
    "model = RNN_crime(vocab_size, embed_size, hidden_size, num_layers)\n",
    "# model.cuda()\n",
    "\n",
    "# # Loss and Optimizer\n",
    "# if is_weighted_loss:\n",
    "#     criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# else:\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Truncated Backpropagation \n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] \n",
    "\n",
    "print(\"Model Training Starting...\")\n",
    "\n",
    "# Training and validation\n",
    "for epoch in range(num_epochs): # TODO: Change to epoch length \n",
    "    for start in range(1): #TODO: change 1 to seq length # this is done to ensure more available training data\n",
    "\n",
    "        for i in range(start, train.size(1) - seq_length, seq_length):\n",
    "            #Train model\n",
    "            model.train()\n",
    "            \n",
    "            # Get batch inputs and targets\n",
    "            # inputs = Variable(train[:, i:i+seq_length]).cuda()\n",
    "            # targets = Variable(train[:, (i+1):(i+1)+seq_length].contiguous()).cuda()\n",
    "            inputs = Variable(train[:, i:i+seq_length])\n",
    "            targets = Variable(train[:, (i+1):(i+1)+seq_length].contiguous())\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            model.zero_grad()\n",
    "\n",
    "            # hidden states not reinitialised - same h_state is propogated forward\n",
    "            # hidden states must be detached from history because the weights have been updated\n",
    "            states = detach(states) \n",
    "\n",
    "            outputs, states = model(inputs, states) \n",
    "            loss = criterion(outputs, targets.view(-1))\n",
    "            loss.backward() #no retain graph - work with set sequence lenght\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            print('{{\"metric\": \"train loss\", \"value\": {}}}'.format(loss.data[0]))\n",
    "\n",
    "            step = (i // seq_length) + 1\n",
    "            if step % 1 == 0:\n",
    "                print ('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n",
    "                       (epoch+1, num_epochs, step, num_batches, loss.data[0], np.exp(loss.data[0])))\n",
    "\n",
    "            # Validate model\n",
    "            # Calculate the evaluation metrics\n",
    "            model.eval()\n",
    "\n",
    "            val_outputs, _ = model(val_inputs, val_states)\n",
    "            val_loss = criterion(val_outputs, val_targets.view(-1))\n",
    "\n",
    "            # Save Ideal Model on Validation Set (Early Stopping)\n",
    "            if (val_loss.data[0] < min_val_los):\n",
    "                min_val_los = val_loss.data[0] \n",
    "                ideal_model_state = model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating mock data for the models\n",
    "### Cumsum example from tutorial [5 Examples of Simple Sequence Prediction Problems for LSTMs](https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockLoaders:\n",
    "    def __init__(self, train_loader=None, validation_loader=None, test_loader=None):\n",
    "        self.train_loader = train_loader\n",
    "        self.validation_loader = validation_loader\n",
    "        self.test_loader = test_loader\n",
    "    \n",
    "\n",
    "class MockRNNLoader:\n",
    "    def __init__(self, seq_len, batch_size, n_samples):\n",
    "        \n",
    "        X,t = mock_rnn_data(seq_len=seq_len, batch_size=n_samples) #(seq_len, batch_size, n_features)                \n",
    "        \n",
    "        indices = np.expand_dims(np.arange(n_samples), axis=0)\n",
    "        indices = np.expand_dims(indices, axis=2)\n",
    "        \n",
    "        vectors = [indices]\n",
    "        i = 0\n",
    "        for j in vector_size:\n",
    "            vectors.append(X[:,:,i:i+j])\n",
    "            i += j\n",
    "        vectors.append(y)    \n",
    "        \n",
    "        self.n_samples = n_samples\n",
    "        self.n_feats = n_feats\n",
    "        self.vectors = vectors\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = int(np.ceil(self.n_samples / self.batch_size))\n",
    "        self.current_batch = 0\n",
    "        \n",
    "        \n",
    "        self.max_index = n_samples\n",
    "        self.min_index = 0\n",
    "        self.dataset = self # just to act as an interface\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_batch = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch >= self.num_batches:\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            self.current_batch += 1\n",
    "            start_index = (self.current_batch - 1) * self.batch_size\n",
    "            stop_index = self.current_batch * self.batch_size\n",
    "            if stop_index > len(self):\n",
    "                stop_index = len(self)\n",
    "            return self[start_index:stop_index]\n",
    "        \n",
    "            \n",
    "    def __getitem__(self, index):        \n",
    "        return tuple(map(lambda x: x[:,index], self.vectors))\n",
    "    \n",
    "vector_size= [3,6,13]#[37,65,512]\n",
    "batch_size = 100    \n",
    "class_split=0.5\n",
    "train_loader = MockLoader(vector_size, batch_size, n_samples=7000, class_split=class_split)\n",
    "validation_loader = MockLoader(vector_size, batch_size, n_samples=1000, class_split=class_split)\n",
    "test_loader = MockLoader(vector_size, batch_size, n_samples=2000, class_split=class_split)\n",
    "loaders = MockLoaders(train_loader,validation_loader,test_loader)\n",
    "\n",
    "X,t = mock_rnn_data() #(seq_len, n_features, batch_size)\n",
    "X,t  = np.swapaxes(X,1,2),np.swapaxes(t,1,2) # (seq_len, batch, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUMLPClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    GRU then a MLP (flattens data output to easier use in NLLLoss)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(GRUMLPClassifier, self).__init__()\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)  # note batch first\n",
    "        self.lin1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.lin2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, h0=None):\n",
    "        if h0 is not None:\n",
    "            out, hn = self.gru(x, h0)\n",
    "        else:\n",
    "            out, hn = self.gru(x)  # hidden state start is zero\n",
    "        out = self.lin1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin2(out)\n",
    "\n",
    "        # Reshape output to (batch_size*seq_len, hidden_size)\n",
    "#         out = out.contiguous().view(out.size(0) * out.size(1), out.size(2))\n",
    "\n",
    "        return out  # if we never send h its never detached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, t = mock_rnn_data(seq_len=10, batch_size=20)\n",
    "print(X.shape, t.shape)\n",
    "y = np.cumsum(X, axis=0)\n",
    "im(t[...,0])\n",
    "im(y[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable(torch.ones(10)).requires_grad, torch.Tensor(10).requires_grad,y_pred.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIME DATA    \n",
    "conf.seq_len = 10\n",
    "data_group = FlatDataGroup(data_path=data_path, conf=conf)\n",
    "loaders = FlatDataLoaders(data_group=data_group, conf=conf)\n",
    "for indices, spc_feats, tmp_feats, env_feats, targets in loaders.train_loader:\n",
    "    for i in [indices, spc_feats, tmp_feats, env_feats, targets]:\n",
    "        print(np.shape(i))\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "losses = []\n",
    "\n",
    "n_epochs = 80\n",
    "seq_len=23\n",
    "batch_size=100\n",
    "\n",
    "\n",
    "CLASSIFICATION = False\n",
    "if CLASSIFICATION:\n",
    "    input_size, hidden_size, num_layers = 1, 2, 2\n",
    "    loss_fn = nn.CrossEntropyLoss()  # todo add class weights\n",
    "    X, y = mock_rnn_data(seq_len=seq_len, batch_size=batch_size)\n",
    "else:    \n",
    "    input_size, hidden_size, num_layers = 2, 1, 2\n",
    "    loss_fn = nn.MSELoss()\n",
    "    X, y = mock_adding_problem_data(seq_len=seq_len, batch_size=batch_size)\n",
    "    \n",
    "print(\"X.shape, y.shape\", X.shape, y.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.GRU(input_size, hidden_size, num_layers)\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-2)\n",
    "state_dict = model.state_dict()\n",
    "optimiser_dict = optimiser.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loop - use conf to get device?\n",
    "def train_epoch_for_gru(model, optimiser, batch_loader, loss_fn, total_losses, conf):\n",
    "    \"\"\"\n",
    "    Training the model for a single epoch\n",
    "    \"\"\"\n",
    "    epoch_losses = []\n",
    "    num_batches = batch_loader.num_batches\n",
    "    for indices, data, targets in batch_loader:\n",
    "        \n",
    "        data = torch.Tensor(data).to(conf.device)\n",
    "        y_true = torch.LongTensor(targets).to(conf.device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        y_pred, h = model.forward(data)\n",
    "\n",
    "        if not conf.use_seq_loss: # seq_loss means we look at entire sequence when calculating the loss.\n",
    "            y_pred, y_true = y_pred[-1], y_true[-1]\n",
    "        y_pred, y_true = y_pred.view(-1,2), y_true.view(-1)\n",
    "\n",
    "\n",
    "        loss = loss_fn(input=y_pred, target=y_true)\n",
    "        \n",
    "        current_batch = batch_loader.current_batch\n",
    "\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "        loss = loss_fn(input=out, target=targets)\n",
    "        epoch_losses.append(loss.item())\n",
    "        total_losses.append(epoch_losses[-1])\n",
    "\n",
    "        if model.training:  # not used in validation loops\n",
    "            optimiser.zero_grad()\n",
    "            clip_grad_norm_(model.parameters(), 0.5)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            log.debug(f\"Batch: {current_batch:04d}/{num_batches:04d} \\t Loss: {epoch_losses[-1]:.4f}\")\n",
    "    mean_epoch_loss = np.mean(epoch_losses)\n",
    "    return mean_epoch_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.GRU(input_size, hidden_size, num_layers)\n",
    "optimiser = optim.Adam(model.parameters(), lr=1e-2)    \n",
    "optimiser.load_state_dict(optimiser_dict)\n",
    "model.load_state_dict(state_dict)\n",
    "losses = []\n",
    "losses_seq = []\n",
    "for i in range(n_epochs*2):\n",
    "    \n",
    "    X = torch.Tensor(X).to(conf.device)\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    y_pred, h = model.forward(X)\n",
    "\n",
    "    if CLASSIFICATION:\n",
    "        y_true = torch.LongTensor(y).to(conf.device)\n",
    "        y_pred, y_true = y_pred[-1], y_true[-1]\n",
    "        y_pred, y_true = y_pred.view(-1,2), y_true.view(-1)\n",
    "    else:\n",
    "        y_true = torch.Tensor(y).to(conf.device)\n",
    "        y_pred, y_true = y_pred[-1], y_true[-1]\n",
    "        y_pred, y_true = y_pred.view(-1), y_true.view(-1)\n",
    "     \n",
    "    loss = loss_fn(input=y_pred, target=y_true)\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimiser.step()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    ######################################################################################\n",
    "    y_pred, h = model.forward(X)\n",
    "    if CLASSIFICATION:\n",
    "        y_true = torch.LongTensor(y).to(conf.device)\n",
    "        y_pred, y_true = y_pred.view(-1,2), y_true.view(-1)\n",
    "    else:\n",
    "        y_true = torch.Tensor(y).to(conf.device)\n",
    "        y_pred, y_true = y_pred.view(-1), y_true.view(-1)\n",
    "    loss_seq = loss_fn(input=y_pred, target=y_true)\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    loss_seq.backward()\n",
    "    clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optimiser.step()\n",
    "    \n",
    "    losses_seq.append(loss_seq.item())\n",
    "    \n",
    "plt.plot(losses, label=\"only last\") \n",
    "plt.plot(losses_seq, label=\"entire sequence\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mock_data import mock_rnn_data\n",
    "from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,t = mock_rnn_data(seq_len=30,batch_size=100) #(seq_len, n_features, batch_size)\n",
    "X,t  = np.swapaxes(X,1,2),np.swapaxes(t,1,2) # (seq_len, batch, input_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group.crimes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.early_stopping = True\n",
    "conf.max_epochs = 20\n",
    "conf.dropout = 0.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "trn_loss = []  # epoch losses\n",
    "val_loss = []\n",
    "val_loss_best = float(\"inf\")\n",
    "\n",
    "all_trn_loss = []  # batch losses\n",
    "all_val_loss = []\n",
    "\n",
    "# SET MODEL PARAMS\n",
    "train_set = loaders.train_loader.dataset\n",
    "indices, spc_feats, tmp_feats, env_feats, target = train_set[train_set.min_index]\n",
    "\n",
    "# input_size = hidden_size = np.sum([spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]])\n",
    "# model = SimpleFNN(input_size, hidden_size)\n",
    "\n",
    "spc_size, tmp_size, env_size = spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]\n",
    "# model = KangFeedForwardNetwork(spc_size=spc_size, tmp_size=tmp_size, env_size=env_size, dropout_p=conf.dropout)\n",
    "model = SimpleKangFNN(spc_size=spc_size, tmp_size=tmp_size, env_size=env_size, dropout_p=conf.dropout)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "\n",
    "parameters = [\n",
    "                {'params': model.spcNet.parameters(), 'lr': 1e-3},    \n",
    "                {'params': model.tmpNet.parameters(), 'lr': 1e-3},    \n",
    "                {'params': model.envNet.parameters(), 'lr': 1e-3},\n",
    "                {'params': model.finalNet.parameters(), 'lr': 1e-3}\n",
    "            ]\n",
    "\n",
    "\n",
    "# important note: using weight decay (l2 penalty) can prohibit long term memory in LSTM networks\n",
    "# - use gradient clipping instead\n",
    "optimiser = optim.Adam(params=parameters, lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "for epoch in range(conf.max_epochs):\n",
    "    log.info(f\"Epoch: {(1+epoch):04d}/{conf.max_epochs:04d}\")\n",
    "    conf.timer.reset()\n",
    "    # Training loop\n",
    "    tmp_trn_loss = []\n",
    "    num_batches = loaders.train_loader.num_batches\n",
    "    for indices, spc_feats, tmp_feats, env_feats, targets in loaders.train_loader:\n",
    "        current_batch = loaders.train_loader.current_batch\n",
    "        \n",
    "        # Transfer to PyTorch Tensor and GPU\n",
    "        spc_feats = torch.Tensor(spc_feats[0]).to(device) # only taking [0] for fnn\n",
    "        tmp_feats = torch.Tensor(tmp_feats[0]).to(device) # only taking [0] for fnn\n",
    "        env_feats = torch.Tensor(env_feats[0]).to(device) # only taking [0] for fnn\n",
    "        targets = torch.LongTensor(targets[0,:,0]).to(device) # only taking [0] for fnn\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "        loss = loss_function(input=out, target=targets)\n",
    "        tmp_trn_loss.append(loss.item())\n",
    "        all_trn_loss.append(tmp_trn_loss[-1])\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        log.debug(f\"Batch: {current_batch:04d}/{num_batches:04d} \\t Loss: {tmp_trn_loss[-1]:.4f}\")\n",
    "\n",
    "    trn_loss.append(np.mean(tmp_trn_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Training Loop Duration: {conf.timer.check()}\")\n",
    "    conf.timer.reset()\n",
    "\n",
    "    # Validation loop\n",
    "    tmp_val_loss = []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # Transfer to GPU - todo put into a loop - then add wrapper\n",
    "        for indices, spc_feats, tmp_feats, env_feats, targets in loaders.validation_loader:\n",
    "            # Transfer to GPU\n",
    "            spc_feats = torch.Tensor(spc_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            tmp_feats = torch.Tensor(tmp_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            env_feats = torch.Tensor(env_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            targets = torch.LongTensor(targets[0,:,0]).to(device)  # only taking [0] for fnn\n",
    "            out = model(spc_feats, tmp_feats, env_feats)\n",
    "\n",
    "            loss = loss_function(input=out, target=targets)\n",
    "            tmp_val_loss.append(loss.item())\n",
    "            all_val_loss.append(tmp_val_loss[-1])\n",
    "\n",
    "    val_loss.append(np.mean(tmp_val_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Validation Loop Duration: {conf.timer.check()}\")\n",
    "    \n",
    "    log.info(f\"\\tlearning rate: \\t{optimiser.param_groups[0]['lr']:.5f}\")\n",
    "    log.info(f\"\\tLoss (Trn): \\t{trn_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Val): \\t{val_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Dif): \\t{np.abs(val_loss[-1]-trn_loss[-1]):.5f}\\n\")        \n",
    "    \n",
    "    # save best model\n",
    "    if min(val_loss) < val_loss_best:\n",
    "        val_loss_best = min(val_loss)\n",
    "        torch.save(model.state_dict(), model_path + \"model_best.pth\")\n",
    "        torch.save(optimiser.state_dict(), model_path + \"optimiser_best.pth\")\n",
    "\n",
    "#     # model has been over-fitting stop maybe? # average of val_loss has increase - starting to over-fit\n",
    "#     if conf.early_stopping and epoch > 5 and np.sum(np.diff(val_loss[-5:])) > 0:  # increasing moving average\n",
    "#         log.warning(\"Early stopping: Over-fitting has taken place\")\n",
    "#         break\n",
    "        \n",
    "#     if conf.early_stopping and epoch > 1 and np.abs(val_loss[-1]-val_loss[-2]) < conf.tolerance:\n",
    "#         log.warning(\"Converged: Difference between the past two validation losses is within tolerance\")\n",
    "#         break\n",
    "\n",
    "    # checkpoint - save models and loss values\n",
    "    torch.save(model.state_dict(), model_path + \"model.pth\")\n",
    "    torch.save(optimiser.state_dict(), model_path + \"optimiser.pth\")\n",
    "    np.savez_compressed(model_path + \"losses.npz\",\n",
    "                        all_val_loss=all_val_loss,\n",
    "                        val_loss=val_loss,\n",
    "                        trn_loss=trn_loss,\n",
    "                        all_trn_loss=all_trn_loss,\n",
    "                        val_loss_best=val_loss_best)\n",
    "    \n",
    "# Save training and validation plots\n",
    "skip = 0\n",
    "loss_plotter = LossPlotter(title=\"Cross Entropy Loss of Linear Regression Model\")\n",
    "loss_plotter.plot_losses(trn_loss, all_trn_loss[skip:], val_loss, all_val_loss[skip:])\n",
    "loss_plotter.savefig(model_path + \"plot_train_val_loss.png\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of RNN training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.early_stopping = True\n",
    "conf.max_epochs = 20\n",
    "conf.dropout = 0.\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "trn_loss = []\n",
    "val_loss = []\n",
    "val_loss_best = float(\"inf\")\n",
    "\n",
    "all_trn_loss = []\n",
    "all_val_loss = []\n",
    "\n",
    "# SET MODEL PARAMS\n",
    "train_set = loaders.train_loader.dataset\n",
    "indices, spc_feats, tmp_feats, env_feats, target = train_set[train_set.min_index]\n",
    "\n",
    "# input_size = hidden_size = np.sum([spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]])\n",
    "# model = SimpleFNN(input_size, hidden_size)\n",
    "\n",
    "spc_size, tmp_size, env_size = spc_feats.shape[-1], tmp_feats.shape[-1], env_feats.shape[-1]\n",
    "# model = KangFeedForwardNetwork(spc_size=spc_size, tmp_size=tmp_size, env_size=env_size, dropout_p=conf.dropout)\n",
    "model = SimpleKangFNN(spc_size=spc_size, tmp_size=tmp_size, env_size=env_size, dropout_p=conf.dropout)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# parameters = model.parameters()\n",
    "\n",
    "parameters = [\n",
    "                {'params': model.spcNet.parameters(), 'lr': 1e-3},    \n",
    "                {'params': model.tmpNet.parameters(), 'lr': 1e-3},    \n",
    "                {'params': model.envNet.parameters(), 'lr': 1e-3},\n",
    "                {'params': model.finalNet.parameters(), 'lr': 1e-3}\n",
    "            ]\n",
    "\n",
    "\n",
    "# important note: using weight decay (l2 penalty) can prohibit long term memory in LSTM networks\n",
    "# - use gradient clipping instead\n",
    "optimiser = optim.Adam(params=parameters, lr=conf.lr, weight_decay=conf.weight_decay)\n",
    "\n",
    "for epoch in range(conf.max_epochs):\n",
    "    log.info(f\"Epoch: {(1+epoch):04d}/{conf.max_epochs:04d}\")\n",
    "    conf.timer.reset()\n",
    "    # Training loop\n",
    "    tmp_trn_loss = []\n",
    "    num_batches = loaders.train_loader.num_batches\n",
    "    for indices, spc_feats, tmp_feats, env_feats, targets in loaders.train_loader:\n",
    "        current_batch = loaders.train_loader.current_batch\n",
    "        \n",
    "        # Transfer to PyTorch Tensor and GPU\n",
    "        spc_feats = torch.Tensor(spc_feats[0]).to(device) # only taking [0] for fnn\n",
    "        tmp_feats = torch.Tensor(tmp_feats[0]).to(device) # only taking [0] for fnn\n",
    "        env_feats = torch.Tensor(env_feats[0]).to(device) # only taking [0] for fnn\n",
    "        targets = torch.LongTensor(targets[0,:,0]).to(device) # only taking [0] for fnn\n",
    "        out = model(spc_feats, tmp_feats, env_feats)\n",
    "        loss = loss_function(input=out, target=targets)\n",
    "        tmp_trn_loss.append(loss.item())\n",
    "        all_trn_loss.append(tmp_trn_loss[-1])\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        log.debug(f\"Batch: {current_batch:04d}/{num_batches:04d} \\t Loss: {tmp_trn_loss[-1]:.4f}\")\n",
    "\n",
    "    trn_loss.append(np.mean(tmp_trn_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Training Loop Duration: {conf.timer.check()}\")\n",
    "    conf.timer.reset()\n",
    "\n",
    "    # Validation loop\n",
    "    tmp_val_loss = []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        # Transfer to GPU\n",
    "        for indices, spc_feats, tmp_feats, env_feats, targets in loaders.validation_loader:\n",
    "            # Transfer to GPU\n",
    "            spc_feats = torch.Tensor(spc_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            tmp_feats = torch.Tensor(tmp_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            env_feats = torch.Tensor(env_feats[0]).to(device)  # only taking [0] for fnn\n",
    "            targets = torch.LongTensor(targets[0,:,0]).to(device)  # only taking [0] for fnn\n",
    "            out = model(spc_feats, tmp_feats, env_feats)\n",
    "\n",
    "            loss = loss_function(input=out, target=targets)\n",
    "            tmp_val_loss.append(loss.item())\n",
    "            all_val_loss.append(tmp_val_loss[-1])\n",
    "\n",
    "    val_loss.append(np.mean(tmp_val_loss))\n",
    "    log.debug(f\"Epoch {epoch} -> Validation Loop Duration: {conf.timer.check()}\")\n",
    "\n",
    "    \n",
    "    log.info(f\"\\tlearning rate: \\t{optimiser.param_groups[0]['lr']:.5f}\")\n",
    "    log.info(f\"\\tLoss (Trn): \\t{trn_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Val): \\t{val_loss[-1]:.5f}\")\n",
    "    log.info(f\"\\tLoss (Dif): \\t{np.abs(val_loss[-1]-trn_loss[-1]):.5f}\\n\")        \n",
    "    \n",
    "\n",
    "    # save best model\n",
    "    if min(val_loss) < val_loss_best:\n",
    "        val_loss_best = min(val_loss)\n",
    "        torch.save(model.state_dict(), model_path + \"model_best.pth\")\n",
    "        torch.save(optimiser.state_dict(), model_path + \"optimiser_best.pth\")\n",
    "\n",
    "#     # model has been over-fitting stop maybe? # average of val_loss has increase - starting to over-fit\n",
    "#     if conf.early_stopping and epoch > 5 and np.sum(np.diff(val_loss[-5:])) > 0:  # increasing moving average\n",
    "#         log.warning(\"Early stopping: Over-fitting has taken place\")\n",
    "#         break\n",
    "        \n",
    "#     if conf.early_stopping and epoch > 1 and np.abs(val_loss[-1]-val_loss[-2]) < conf.tolerance:\n",
    "#         log.warning(\"Converged: Difference between the past two validation losses is within tolerance\")\n",
    "#         break\n",
    "    \n",
    "    if epoch > 3:\n",
    "        model.dropout.p = ((conf.max_epochs - epoch)/conf.max_epochs)**2\n",
    "    \n",
    "    \n",
    "    # checkpoint - save models and loss values\n",
    "    torch.save(model.state_dict(), model_path + \"model.pth\")\n",
    "    torch.save(optimiser.state_dict(), model_path + \"optimiser.pth\")\n",
    "    np.savez_compressed(model_path + \"losses.npz\",\n",
    "                        all_val_loss=all_val_loss,\n",
    "                        val_loss=val_loss,\n",
    "                        trn_loss=trn_loss,\n",
    "                        all_trn_loss=all_trn_loss,\n",
    "                        val_loss_best=val_loss_best)\n",
    "    \n",
    "# Save training and validation plots\n",
    "skip = 0\n",
    "loss_plotter = LossPlotter(title=\"Cross Entropy Loss of Linear Regression Model\")\n",
    "loss_plotter.plot_losses(trn_loss, all_trn_loss[skip:], val_loss, all_val_loss[skip:])\n",
    "loss_plotter.savefig(model_path + \"plot_train_val_loss.png\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/time_series.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2d3f8532936d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Load Time Series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/time_series.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# just select the centre square\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# treshold data to 0 and 1 values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/time_series.npy'"
     ]
    }
   ],
   "source": [
    "# Some part of the code was referenced from below.\n",
    "# https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model\n",
    "# which was in part referenced from below.\n",
    "# https://github.com/pytorch/examples/tree/master/word_language_model \n",
    "# TODO: Add parsing capabilities\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "\n",
    "# Hyper Parameters\n",
    "embed_size = 100\n",
    "hidden_size = 1000\n",
    "num_layers = 1\n",
    "num_epochs = 1\n",
    "batch_size = 20\n",
    "seq_length = 20 # month history\n",
    "learning_rate = 0.002\n",
    "is_weighted_loss = False\n",
    "\n",
    "# Load Time Series\n",
    "data = np.load(\"./data/time_series.npy\")[:1000] # just select the centre square\n",
    "data = threshold(data) # treshold data to 0 and 1 values\n",
    "\n",
    "# Calculate the CrossEntropy weights for skew data (Rosenberg 2012)\n",
    "values, counts = np.unique(data,return_counts=True)\n",
    "weights = counts/np.sum(counts)\n",
    "# weights = Variable(torch.FloatTensor([weights[1], weights[0]])).cuda()\n",
    "weights = Variable(torch.FloatTensor([weights[1], weights[0]]))\n",
    "\n",
    "vocab_size = int(data.max() + 1) # only two types - crime or no crime\n",
    "\n",
    "n_trn_val_samps = int(len(data)*0.8)# 80:20 split data to train and test set\n",
    "n_trn_samps = int(n_trn_val_samps*0.8)# 80:20 split train to train and validation set\n",
    "train_and_validation = data[:n_trn_val_samps]\n",
    "train = train_and_validation[:n_trn_samps]\n",
    "validation = train_and_validation[n_trn_samps:]\n",
    "test = data[n_trn_val_samps:]\n",
    "\n",
    "# Setup train data\n",
    "train = batchify(train,batch_size) # segment the data into batches\n",
    "train = torch.LongTensor(train) #expects long tensor for the embedding\n",
    "\n",
    "# Setup validation data and starting hidden state\n",
    "validation = torch.LongTensor(validation)\n",
    "validation = validation.view(1,-1) # reshape validation => 1 batch , many samples\n",
    "# val_states = (Variable(torch.zeros(num_layers, 1, hidden_size)).cuda(), \n",
    "#                 Variable(torch.zeros(num_layers, 1, hidden_size)).cuda())\n",
    "val_states = (Variable(torch.zeros(num_layers, 1, hidden_size)), \n",
    "                Variable(torch.zeros(num_layers, 1, hidden_size)))\n",
    "# val_inputs = Variable(validation[:,:-1]).cuda()\n",
    "# val_targets = Variable(validation[:,1:]).cuda()\n",
    "val_inputs = Variable(validation[:,:-1])\n",
    "val_targets = Variable(validation[:,1:])\n",
    "min_val_los = 10000 #used to get early stopping parameters\n",
    "ideal_model_state = {}\n",
    "\n",
    "# Setup test data and starting hidden state\n",
    "test = torch.LongTensor(test)\n",
    "test = test.view(1,-1) # reshape test => 1 batch , many samples\n",
    "# tst_states = (Variable(torch.zeros(num_layers, 1, hidden_size)).cuda(), \n",
    "#                 Variable(torch.zeros(num_layers, 1, hidden_size)).cuda()) #batch_size is one in testing case\n",
    "tst_states = (Variable(torch.zeros(num_layers, 1, hidden_size)), \n",
    "                Variable(torch.zeros(num_layers, 1, hidden_size))) #batch_s\n",
    "# tst_inputs = Variable(test[:,:-1]).cuda()\n",
    "# tst_targets = Variable(test[:,1:]).cuda()\n",
    "tst_inputs = Variable(test[:,:-1])\n",
    "tst_targets = Variable(test[:,1:])\n",
    "\n",
    "num_batches = train.shape[1] // seq_length\n",
    "\n",
    "print(\"Hyper Parameters:\\n================\")\n",
    "print(\"embed_size:\\t\",embed_size)\n",
    "print(\"hidden_size:\\t\",hidden_size)\n",
    "print(\"num_layers:\\t\",num_layers)\n",
    "print(\"num_epochs:\\t\",num_epochs)\n",
    "print(\"batch_size:\\t\",batch_size)\n",
    "print(\"seq_length:\\t\",seq_length)\n",
    "print(\"learning_rate:\\t\",learning_rate)\n",
    "print(\"num_batches:\\t\",num_batches)\n",
    "print(\"is_weighted_loss:\\t\",is_weighted_loss)\n",
    "\n",
    "# RNN Based Crime Model\n",
    "class RNN_crime(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNN_crime, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) # note batch first\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "    def forward(self, x, h): # give seq_jump to know which hidden state to use for next sequence\n",
    "        # Embed word crimes to vectors\n",
    "        x = self.embed(x) \n",
    "\n",
    "        # Forward propagate RNN  \n",
    "        out, h = self.lstm(x, h) # Not saving hidden state because seq_jump # init lstm weight to 1 to make easier\n",
    "\n",
    "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
    "        out = out.contiguous().view(out.size(0)*out.size(1), out.size(2))\n",
    "\n",
    "        # Decode hidden states of all time step\n",
    "        out = self.linear(out)  \n",
    "        return out, h # if we never send h its never detached\n",
    "    \n",
    "    \n",
    "model = RNN_crime(vocab_size, embed_size, hidden_size, num_layers)\n",
    "# model.cuda()\n",
    "\n",
    "# Loss and Optimizer\n",
    "if is_weighted_loss:\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "else:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Truncated Backpropagation \n",
    "def detach(states):\n",
    "    return [state.detach() for state in states] \n",
    "\n",
    "print(\"Model Training Starting...\")\n",
    "\n",
    "# Training and validation\n",
    "for epoch in range(num_epochs): # TODO: Change to epoch length \n",
    "    for start in range(1): #TODO: change 1 to seq length # this is done to ensure more available training data\n",
    "    \n",
    "        # Initial hidden and memory states\n",
    "        states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)),\n",
    "                  Variable(torch.zeros(num_layers, batch_size, hidden_size)))\n",
    "    \n",
    "        for i in range(start, train.size(1) - seq_length, seq_length):\n",
    "            #Train model\n",
    "            model.train()\n",
    "            \n",
    "            # Get batch inputs and targets\n",
    "            # inputs = Variable(train[:, i:i+seq_length]).cuda()\n",
    "            # targets = Variable(train[:, (i+1):(i+1)+seq_length].contiguous()).cuda()\n",
    "            inputs = Variable(train[:, i:i+seq_length])\n",
    "            targets = Variable(train[:, (i+1):(i+1)+seq_length].contiguous())\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            model.zero_grad()\n",
    "\n",
    "            # hidden states not reinitialised - same h_state is propogated forward\n",
    "            # hidden states must be detached from history because the weights have been updated\n",
    "            states = detach(states) \n",
    "\n",
    "            outputs, states = model(inputs, states) \n",
    "            loss = criterion(outputs, targets.view(-1))\n",
    "            loss.backward() #no retain graph - work with set sequence lenght\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            print('{{\"metric\": \"train loss\", \"value\": {}}}'.format(loss.data[0]))\n",
    "\n",
    "            step = (i // seq_length) + 1\n",
    "            if step % 1 == 0:\n",
    "                print ('Epoch [%d/%d], Step[%d/%d], Loss: %.3f, Perplexity: %5.2f' %\n",
    "                       (epoch+1, num_epochs, step, num_batches, loss.data[0], np.exp(loss.data[0])))\n",
    "\n",
    "            # Validate model\n",
    "            # Calculate the evaluation metrics\n",
    "            model.eval()\n",
    "\n",
    "            val_outputs, _ = model(val_inputs, val_states)\n",
    "            val_loss = criterion(val_outputs, val_targets.view(-1))\n",
    "\n",
    "            # Save Ideal Model on Validation Set (Early Stopping)\n",
    "            if (val_loss.data[0] < min_val_los):\n",
    "                min_val_los = val_loss.data[0] \n",
    "                ideal_model_state = model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_collection = []\n",
    "val_collection = []\n",
    "best_model_val_collection = []\n",
    "best_model_trn_collection = []\n",
    "\n",
    "itrs = [5]\n",
    "\n",
    "for itr in itrs:\n",
    "    torch.manual_seed(itr)  # manual seed to test other params of the model and keep init weights constant\n",
    "\n",
    "    print('Start Time:', pd.datetime.now(), '\\n')\n",
    "\n",
    "    # data loader setup\n",
    "    n_epochs = 10\n",
    "    seq_len = 24 * 3\n",
    "    batch_size = 32\n",
    "    lr = 1e-3\n",
    "\n",
    "    S = Variable(torch.FloatTensor(subX)).view(-1, subX.shape[-1] ** 2)\n",
    "    #   S = Variable(torch.FloatTensor(grids)).view(-1,grid_size**2)\n",
    "    #   S = Variable(torch.FloatTensor(f[-500:]))\n",
    "    S[S > cap] = cap\n",
    "    S = S.cuda()\n",
    "    #   S = S.mm(trans_mat_d2s)\n",
    "\n",
    "    loader = DataLoader(S, batch_size, seq_len, shuffle=True)\n",
    "    trnT, trnX, trnTimes = loader.getTrainBatch()\n",
    "    loader.reset_current_t()\n",
    "    trnT = trnT[:, :, 12:12 + 1]\n",
    "    #   trnX = trnX.view(batch_size,seq_len,-1)\n",
    "\n",
    "    # model setup\n",
    "    input_size = trnX.shape[-1]\n",
    "    output_size = trnT.shape[-1]\n",
    "    hidden_size = 10 * trnX.shape[-1]\n",
    "    num_layers = 1\n",
    "\n",
    "    #   model = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    model = GRU_MLPRegressor(input_size, hidden_size, output_size, num_layers=num_layers)\n",
    "    model.cuda()\n",
    "    print('\\nMODEL\\n', model)\n",
    "\n",
    "    params = list(model.parameters())  # + list(h0) + list(c0)\n",
    "    optimizer = optim.Adam(params=params, lr=lr, weight_decay=0)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    n_steps = (loader.getTrainDataLength() // batch_size) + 1\n",
    "    print(\"\\nSteps per epoch\", n_steps)\n",
    "\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    min_val_loss = np.inf  # used for early stopping TODO: IMPLEMENT MOVING AVERAGE AND STOP IF VAL LOSS IS MOVING UP, IE OVERFITTING\n",
    "\n",
    "    valT, valX, valTimes = loader.getValidationSet()\n",
    "    valT = valT[:, :, 12:12 + 1]\n",
    "\n",
    "    for epoch in range(n_epochs):  # epoch is one iteration over the entire input data\n",
    "        loader.reset_current_t()\n",
    "        print(\"\\nEpoch:\", epoch)\n",
    "        for step in range(n_steps - 1):\n",
    "            # Train Evaluation and Optimization Step\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            trnT, trnX, trnTimes = loader.getTrainBatch()\n",
    "            trnT = trnT[:, :, 12:12 + 1]  # choosing only center square\n",
    "            #       trnY, (hn, cn) = model.forward(trnX, (h0, c0))\n",
    "            trnY = model.forward(trnX)\n",
    "\n",
    "            trnLoss = torch.sqrt(criterion(trnY[-1], trnT[-1]))  # SQRT MSE not same as MAE\n",
    "            trnLoss.backward()\n",
    "            #       if itr == 0:\n",
    "            #         torch.nn.utils.clip_grad_norm(model.parameters(), 0.5) # Figure out fitting clip value\n",
    "            optimizer.step()\n",
    "            train_losses.append(trnLoss.cpu().item())\n",
    "\n",
    "            # Validation Evaluation\n",
    "            model.eval()\n",
    "            valY = model.forward(valX)\n",
    "\n",
    "            valLoss = torch.sqrt(criterion(valY[-1], valT[-1]))\n",
    "            validation_losses.append(valLoss.cpu().item())\n",
    "\n",
    "            if validation_losses[-1] < min_val_loss:\n",
    "                min_val_loss = validation_losses[-1]\n",
    "                best_model_val = model.state_dict()\n",
    "\n",
    "            if len(validation_losses) % 50 == 0:\n",
    "                print(\"Iteration:\", len(validation_losses))\n",
    "\n",
    "        print(\"Last Train Loss of Epoch:\", train_losses[-1])\n",
    "        print(\"Last Validation Loss of Epoch:\", validation_losses[-1])\n",
    "        print(\"Current Best Validation Loss:\", min_val_loss)\n",
    "    best_model_trn = model.state_dict()\n",
    "    trn_collection.append(train_losses)\n",
    "    val_collection.append(validation_losses)\n",
    "    best_model_trn_collection.append(best_model_trn)\n",
    "    best_model_val_collection.append(best_model_val)\n",
    "    print('Stop Time:', pd.datetime.now(), '\\n')\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"RMSE Loss of Model over Itterations\")\n",
    "for i, _ in enumerate(trn_collection):\n",
    "    plt.plot(trn_collection[i], label='Training: ' + str(itrs[i]))\n",
    "    plt.plot(val_collection[i], label='Validation: ' + str(itrs[i]))\n",
    "    plt.scatter(np.argmin(val_collection[i]), np.min(val_collection[i]), marker='X', c='r')\n",
    "plt.xlabel(\"Itteraion\")\n",
    "plt.ylabel(\"RMSE Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
